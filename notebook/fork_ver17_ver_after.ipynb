{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":8151133,"sourceType":"datasetVersion","datasetId":4820822},{"sourceId":8154375,"sourceType":"datasetVersion","datasetId":4823277},{"sourceId":8156628,"sourceType":"datasetVersion","datasetId":4825049},{"sourceId":8160583,"sourceType":"datasetVersion","datasetId":4827991},{"sourceId":8375716,"sourceType":"datasetVersion","datasetId":4980053},{"sourceId":8389716,"sourceType":"datasetVersion","datasetId":4990168}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 標準ライブラリ\nimport gc\nimport os\nimport pickle\nimport random\nimport sys\nimport warnings\nfrom itertools import combinations, permutations\nfrom pathlib import Path\nimport pytz\n\n# サードパーティのライブラリ\nimport category_encoders as ce\nimport joblib\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport scipy as sp\nimport seaborn as sns\nimport torch\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\nfrom sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n                                     StratifiedGroupKFold, TimeSeriesSplit,\n                                     train_test_split)\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\nimport glob\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:40.998109Z","iopub.execute_input":"2024-05-12T08:20:40.998672Z","iopub.status.idle":"2024-05-12T08:20:49.925670Z","shell.execute_reply.started":"2024-05-12T08:20:40.998634Z","shell.execute_reply":"2024-05-12T08:20:49.924631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n    \n    train_data_path = os.path.join(home_directory, 'train/')\n    test_data_path = os.path.join(home_directory, 'test/')\n    \n    OOF_DATA_PATH = Path(home_directory) / 'oof'\n    MODEL_DATA_PATH = Path(home_directory) / 'models'\n    SUB_DATA_PATH = Path(home_directory) / 'submission'\n\n    def __init__(self):\n        self.create_directories()\n    \n    def create_directories(self):\n        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n            path.mkdir(parents=True, exist_ok=True)\n    \n    \n    VER = 'after'\n    AUTHOR = 'Mira'\n    COMPETITION = 'HomeCredit'\n\n    METHOD_LIST = ['lightgbm']\n    seed = 28\n    n_folds = 5\n    target_col = 'target'\n    metric = 'auc'\n    \n    metric_maximize_flag = True\n    num_boost_round = 2000\n    early_stopping_round = 200\n    verbose = 25\n    classification_lgb_params = {\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"max_depth\": 10,  \n        \"learning_rate\": 0.05,\n        \"n_estimators\": num_boost_round,  \n        \"colsample_bytree\": 0.8,\n        \"colsample_bynode\": 0.8,\n        \"verbose\": 1,\n        \"reg_alpha\": 0.1,\n        \"reg_lambda\": 10,\n        \"extra_trees\":True,\n        'num_leaves':64,\n        'seed': seed,\n    }\n    classification_xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'learning_rate': 0.05,\n        'random_state': seed,\n        \"tree_method\": \"gpu_hist\",\n    }\n\n    classification_cat_params = {\n        'iterations':num_boost_round,                \n        'learning_rate':0.05,\n        'depth':10,                     \n        'l2_leaf_reg':10,                  \n        'loss_function':'Logloss',        \n        'eval_metric':'AUC',              \n        'bootstrap_type':'Bernoulli',    \n        'subsample':0.8,                  \n        'colsample_bylevel':0.8,         \n        'verbose':False,                  \n        'leaf_estimation_iterations':10,       \n        'random_seed':seed,\n        #\"task_type\": \"GPU\",\n    }\n    #model_weight_dict = {'lightgbm': 0.5,'catboost':0.5}\n    model_weight_dict = {'lightgbm': 1}\n    \n\nclass is_kaggle:\n    def __init__(self, Kaggle):\n        if Kaggle == \"Yes\":\n            self.path = Path(CFG.kaggle_directory)\n            CFG.MODEL_DATA_PATH = Path('/kaggle/input/05121717/models')\n        else:\n            self.path = Path(CFG.home_directory)\n            CFG.MODEL_DATA_PATH = Path(CFG.home_directory) / 'models'\n\ndef create_timestamped_file():\n    tz_tokyo = pytz.timezone('Asia/Tokyo')\n    now = datetime.datetime.now(tz=tz_tokyo)\n    filename = now.strftime('%m%d-%H%M') + '.txt'\n    full_path = CFG.MODEL_DATA_PATH / filename\n    full_path.touch()\n\n#create_timestamped_file()\ncfg_instance = CFG()      \nselector = is_kaggle(\"Yes\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:49.927794Z","iopub.execute_input":"2024-05-12T08:20:49.929303Z","iopub.status.idle":"2024-05-12T08:20:49.951498Z","shell.execute_reply.started":"2024-05-12T08:20:49.929244Z","shell.execute_reply":"2024-05-12T08:20:49.950052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:49.954191Z","iopub.execute_input":"2024-05-12T08:20:49.954731Z","iopub.status.idle":"2024-05-12T08:20:49.984447Z","shell.execute_reply.started":"2024-05-12T08:20:49.954693Z","shell.execute_reply":"2024-05-12T08:20:49.981925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]\n\n        return expr_max + expr_last + expr_mean \n\n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n\n        return expr_max + expr_last + expr_mean \n\n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return expr_max + expr_last  # +expr_count\n\n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return expr_max + expr_last\n\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return expr_max + expr_last\n\n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:49.986953Z","iopub.execute_input":"2024-05-12T08:20:49.987433Z","iopub.status.idle":"2024-05-12T08:20:50.009679Z","shell.execute_reply.started":"2024-05-12T08:20:49.987391Z","shell.execute_reply":"2024-05-12T08:20:50.007982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    \n    if depth in [1, 2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n    \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    for path in glob.glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        \n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        \n        chunks.append(df)\n        \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.013610Z","iopub.execute_input":"2024-05-12T08:20:50.014050Z","iopub.status.idle":"2024-05-12T08:20:50.030741Z","shell.execute_reply.started":"2024-05-12T08:20:50.014017Z","shell.execute_reply":"2024-05-12T08:20:50.029413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n        \n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n        \n    df_base = df_base.pipe(Pipeline.handle_dates)\n    \n    return df_base","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.032307Z","iopub.execute_input":"2024-05-12T08:20:50.032773Z","iopub.status.idle":"2024-05-12T08:20:50.047569Z","shell.execute_reply.started":"2024-05-12T08:20:50.032738Z","shell.execute_reply":"2024-05-12T08:20:50.046309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n\n    # bool型の列をint型に変換\n    bool_cols = df_data.select_dtypes('bool').columns\n    df_data[bool_cols] = df_data[bool_cols].astype(int)\n\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.048849Z","iopub.execute_input":"2024-05-12T08:20:50.050085Z","iopub.status.idle":"2024-05-12T08:20:50.061912Z","shell.execute_reply.started":"2024-05-12T08:20:50.050051Z","shell.execute_reply":"2024-05-12T08:20:50.060311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.064205Z","iopub.execute_input":"2024-05-12T08:20:50.064761Z","iopub.status.idle":"2024-05-12T08:20:50.084829Z","shell.execute_reply.started":"2024-05-12T08:20:50.064728Z","shell.execute_reply":"2024-05-12T08:20:50.082992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = selector.path\nTRAIN_DIR       = ROOT / \"parquet_files/train\"\nTEST_DIR        = ROOT / \"parquet_files/test\"\nSAMPLE_SUB = ROOT / \"sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.086594Z","iopub.execute_input":"2024-05-12T08:20:50.087378Z","iopub.status.idle":"2024-05-12T08:20:50.104996Z","shell.execute_reply.started":"2024-05-12T08:20:50.087343Z","shell.execute_reply":"2024-05-12T08:20:50.103073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:20:50.107326Z","iopub.execute_input":"2024-05-12T08:20:50.107893Z","iopub.status.idle":"2024-05-12T08:24:31.427231Z","shell.execute_reply.started":"2024-05-12T08:20:50.107847Z","shell.execute_reply":"2024-05-12T08:24:31.424754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.431254Z","iopub.execute_input":"2024-05-12T08:24:31.431953Z","iopub.status.idle":"2024-05-12T08:24:31.869469Z","shell.execute_reply.started":"2024-05-12T08:24:31.431899Z","shell.execute_reply":"2024-05-12T08:24:31.867825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    cat_features = x_train.select_dtypes('category').columns.tolist()\n    lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=cat_features)\n    lgb_valid = lgb.Dataset(x_valid, y_valid,categorical_feature=cat_features)\n    model = lgb.train(\n                params = CFG.classification_lgb_params,\n                train_set = lgb_train,\n                num_boost_round = CFG.num_boost_round,\n                valid_sets = [lgb_train, lgb_valid],\n                #feval = CFG.metric,\n                callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n                                              verbose=CFG.verbose)]\n            )\n    valid_pred = model.predict(x_valid)\n    \n    importance_df = pd.DataFrame({\n        'feature': features,\n        'importance': model.feature_importance(importance_type='gain')\n    })\n    importance_df['importance'] = importance_df['importance'] / np.sum(importance_df['importance'])\n    importance_df = importance_df.sort_values(by='importance', ascending=False)\n    print(importance_df)\n    return model, valid_pred\ndef xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n    model = xgb.train(\n                CFG.classification_xgb_params,\n                dtrain = xgb_train,\n                num_boost_round = CFG.num_boost_round,\n                evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n                early_stopping_rounds = CFG.early_stopping_round,\n                verbose_eval = CFG.verbose,\n                #feval = CFG.metric,\n                maximize = CFG.metric_maximize_flag,\n        )\n    valid_pred = model.predict(xgb.DMatrix(x_valid))\n    return model, valid_pred\ndef catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    cat_features = x_train.select_dtypes('category').columns.tolist()\n    cat_train = Pool(data=x_train, label=y_train,cat_features=cat_features)\n    cat_valid = Pool(data=x_valid, label=y_valid,cat_features=cat_features)\n    model = CatBoostClassifier(**CFG.classification_cat_params)\n    model.fit(cat_train,\n              eval_set = [cat_valid],\n              early_stopping_rounds = CFG.early_stopping_round,\n              verbose = CFG.verbose,\n              use_best_model = True)\n    valid_pred = model.predict_proba(x_valid)[:, 1]\n    return model, valid_pred","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.871389Z","iopub.execute_input":"2024-05-12T08:24:31.871780Z","iopub.status.idle":"2024-05-12T08:24:31.896339Z","shell.execute_reply.started":"2024-05-12T08:24:31.871750Z","shell.execute_reply":"2024-05-12T08:24:31.894225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list):\n    weeks = train_df[\"WEEK_NUM\"]\n    X = train_df.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n    y = train_df[\"target\"]\n    \n    oof_predictions = np.zeros(len(train_df))\n    oof_fold = np.zeros(len(train_df))\n    cv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n    for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups=weeks)):\n        print('-'*50)\n        print(f'{method} training fold {fold+1}')\n\n        x_train = train_df[features].iloc[train_index]\n        y_train = train_df[CFG.target_col].iloc[train_index]\n        x_valid = train_df[features].iloc[valid_index]\n        y_valid = train_df[CFG.target_col].iloc[valid_index]\n        if method == 'lightgbm':\n            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'xgboost':\n            model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'catboost':\n            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features)\n\n        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n\n        oof_predictions[valid_index] = valid_pred\n        oof_fold[valid_index] = fold + 1\n        del x_train, x_valid, y_train, y_valid, model, valid_pred\n        gc.collect()\n\n    score = roc_auc_score(train_df[CFG.target_col], oof_predictions)\n    print(f'{method} our out of folds CV f1score is {score}')\n\n    oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n    oof_df.to_csv(CFG.OOF_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n\ndef Learning(input_df: pd.DataFrame, features: list):\n    for method in CFG.METHOD_LIST:\n        gradient_boosting_model_cv_training(method, input_df, features)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.898554Z","iopub.execute_input":"2024-05-12T08:24:31.899503Z","iopub.status.idle":"2024-05-12T08:24:31.917890Z","shell.execute_reply.started":"2024-05-12T08:24:31.899451Z","shell.execute_reply":"2024-05-12T08:24:31.916517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unified_inference(method: str, x_test: pd.DataFrame):\n    test_pred = np.zeros(len(x_test))\n    for fold in range(CFG.n_folds):\n        model_path = CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl'\n        model = pickle.load(open(model_path, 'rb'))\n\n        if method == 'lightgbm':\n            pred = model.predict(x_test)\n        elif method == 'xgboost':\n            pred = model.predict(xgb.DMatrix(x_test))\n        elif method == 'catboost':\n            pred = model.predict_proba(x_test)[:, 1]\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        test_pred += pred\n    \n    return test_pred / CFG.n_folds\n\ndef gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list):\n    x_test = test_df[features]\n    test_pred = unified_inference(method, x_test)\n    return test_pred\n\ndef Predicting(input_df: pd.DataFrame, features: list):\n    output_df = input_df.copy()\n    output_df['pred_prob'] = 0\n    for method in CFG.METHOD_LIST:\n        output_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, input_df, features)\n        output_df['pred_prob'] += CFG.model_weight_dict[method] * output_df[f'{method}_pred_prob']\n    return output_df","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.922762Z","iopub.execute_input":"2024-05-12T08:24:31.924051Z","iopub.status.idle":"2024-05-12T08:24:31.940445Z","shell.execute_reply.started":"2024-05-12T08:24:31.923997Z","shell.execute_reply":"2024-05-12T08:24:31.938620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def target_enc(df,test,col):\n    features = [col for col in df.columns if col != CFG.target_col]\n    kf = KFold(n_splits=CFG.n_folds,shuffle=True,random_state = CFG.seed)\n    encoded_features = []\n\n    for train_idx, val_idx in kf.split(df):\n        X_train, X_valid = df[features].iloc[train_idx], df[features].iloc[val_idx]\n        y_train = df[CFG.target_col].iloc[train_idx]\n\n        target_encoder = ce.TargetEncoder()\n        target_encoder.fit(X_train[col], y_train)\n\n        X_valid[f'{col}_target_Encoded'] = target_encoder.transform(X_valid[col])\n        encoded_features.append(X_valid)\n\n\n    encoded_df = pd.concat(encoded_features).sort_index()\n    df[f'{col}_target_Encoded'] = encoded_df[f'{col}_target_Encoded']\n    \n    target_encoder = ce.TargetEncoder()\n    target_encoder.fit(df[[col]], df[CFG.target_col])\n\n    test[f'{col}_target_Encoded'] = target_encoder.transform(test[[col]])\n    \n    return df, test\n\ndef encoder(df, test):\n    object_columns = [col for col in df.columns if df[col].dtype.name == 'object' or df[col].dtype.name == 'category']\n    \n    bool_columns = [col for col in df.columns if df[col].dtype == 'bool']\n\n    for col in object_columns:\n        df, test = target_enc(df, test, col)\n        \n    for col in bool_columns:\n        df[col] = df[col].astype(int)\n        test[col] = test[col].astype(int)\n\n    df.drop(object_columns, axis=1, inplace=True)\n    test.drop(object_columns, axis=1, inplace=True)\n        \n    return df, test\n\n\ndef preprocess(df,test):\n    df,test = encoder(df,test)\n    features = [col for col in df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n    \n    return df,test,features","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.942267Z","iopub.execute_input":"2024-05-12T08:24:31.943396Z","iopub.status.idle":"2024-05-12T08:24:31.963017Z","shell.execute_reply.started":"2024-05-12T08:24:31.943355Z","shell.execute_reply":"2024-05-12T08:24:31.961504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataPreprocessor:\n    def __init__(self, df):\n        self.df = df\n\n    def select_numerical_columns(self):\n        return self.df.select_dtypes(exclude='category').columns\n\n    def calculate_nan_groups(self, nums):\n        nans_df = self.df[nums].isna()\n        nans_groups = {}\n        for col in nums:\n            cur_group = nans_df[col].sum()\n            nans_groups.setdefault(cur_group, []).append(col)\n        del nans_df\n        gc.collect()\n        return nans_groups\n\n    def reduce_group(self, groups):\n        use = []\n        for group in groups:\n            max_unique = 0\n            selected_col = group[0]\n            for col in group:\n                unique_count = self.df[col].nunique()\n                if unique_count > max_unique:\n                    max_unique = unique_count\n                    selected_col = col\n            use.append(selected_col)\n        print('Use these:', use)\n        return use\n\n    def group_columns_by_correlation(self, matrix, threshold=0.8):\n        correlation_matrix = matrix.corr()\n        cols = list(matrix.columns)\n        groups = []\n\n        while cols:\n            base_col = cols.pop(0)\n            group = [base_col]\n            correlated_cols = [base_col]\n\n            for col in cols:\n                if correlation_matrix.loc[base_col, col] >= threshold:\n                    group.append(col)\n                    correlated_cols.append(col)\n\n            groups.append(group)\n            cols = [c for c in cols if c not in correlated_cols]\n        \n        return groups\n\n    def process_columns(self):\n        nums = self.select_numerical_columns()\n        nans_groups = self.calculate_nan_groups(nums)\n        uses = []\n\n        for count, columns in nans_groups.items():\n            if len(columns) > 1:\n                grps = self.group_columns_by_correlation(self.df[columns])\n                uses += self.reduce_group(grps)\n            else:\n                uses += columns\n\n            print('####### NAN count =', count)\n        \n        print('Selected numerical columns:', uses)\n        return uses\n\n    def update_dataframe(self):\n        uses = self.process_columns()\n        uses += list(self.df.select_dtypes(include='category').columns)\n        print('Total columns used:', len(uses))\n        self.df = self.df[uses]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.965213Z","iopub.execute_input":"2024-05-12T08:24:31.965814Z","iopub.status.idle":"2024-05-12T08:24:31.989744Z","shell.execute_reply.started":"2024-05-12T08:24:31.965775Z","shell.execute_reply":"2024-05-12T08:24:31.987493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feature_eng(**train_data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndel train_data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)\n\n\npreprocessor = DataPreprocessor(df_train)\npreprocessor.update_dataframe()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:24:31.991897Z","iopub.execute_input":"2024-05-12T08:24:31.992385Z","iopub.status.idle":"2024-05-12T08:26:49.763361Z","shell.execute_reply.started":"2024-05-12T08:24:31.992352Z","shell.execute_reply":"2024-05-12T08:26:49.760684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**test_data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel test_data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:49.765771Z","iopub.execute_input":"2024-05-12T08:26:49.766285Z","iopub.status.idle":"2024-05-12T08:26:50.526487Z","shell.execute_reply.started":"2024-05-12T08:26:49.766248Z","shell.execute_reply":"2024-05-12T08:26:50.524525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train.select_dtypes('category').columns:\n    df_train[col] = df_train[col].cat.add_categories(['Unknown']).fillna('Unknown')\n    df_test[col] = df_test[col].cat.add_categories(['Unknown']).fillna('Unknown')\n    \nfeatures = [col for col in df_train.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\nweeks = df_train['WEEK_NUM']","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:50.528342Z","iopub.execute_input":"2024-05-12T08:26:50.529807Z","iopub.status.idle":"2024-05-12T08:26:51.453169Z","shell.execute_reply.started":"2024-05-12T08:26:50.529748Z","shell.execute_reply":"2024-05-12T08:26:51.451389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nimport pandas as pd\n\nfeature_importances = np.zeros(df_train.shape[1])\ncat_features = df_train.select_dtypes('category').columns.tolist()\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = df_train[features].columns\nfeature_importances['importance'] = 0\n\ncv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor fold, (train_index, valid_index) in enumerate(cv.split(df_train[features], df_train[CFG.target_col], groups=weeks)):\n    print('-'*50)\n    print(f'training fold {fold+1}')\n\n    x_train = df_train[features].iloc[train_index]\n    y_train = df_train[CFG.target_col].iloc[train_index]\n    x_valid = df_train[features].iloc[valid_index]\n    y_valid = df_train[CFG.target_col].iloc[valid_index]\n\n\n    lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=cat_features)\n    lgb_valid = lgb.Dataset(x_valid, y_valid,categorical_feature=cat_features)\n\n    valification_lgb_params = {\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"max_depth\": 10,  \n        \"learning_rate\": 0.05,\n        \"n_estimators\": 2000, \n        \"verbose\": 1,\n        \"extra_trees\":True,\n        'num_leaves':64,\n        'seed': CFG.seed,\n    }\n    \n    # モデルを訓練して特徴量重要度を取得\n    model = lgb.train(\n             params = valification_lgb_params,\n            train_set = lgb_train,\n            num_boost_round = CFG.num_boost_round,\n            valid_sets = [lgb_train, lgb_valid],\n            #feval = CFG.metric,\n            callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n                                              verbose=CFG.verbose)]\n    )\n    fold_importance = pd.DataFrame()\n    fold_importance['feature'] = lgb_train.feature_name\n    fold_importance['importance'] = model.feature_importance()\n    feature_importances['importance'] += fold_importance['importance']\n\n# 特徴量重要度を平均化\nfeature_importances['importance'] /= cv.get_n_splits()\n\n# 特徴量重要度を表示\nprint(feature_importances.sort_values(by='importance', ascending=False))\nthreshold = feature_importances['importance'].median()\nselected_features = feature_importances[feature_importances['importance'] > threshold].index.tolist()\nprint(\"Selected Features:\", selected_features)\nselected_feature_names = df_train[features].columns[selected_features].tolist()\nprint(f'select_{selected_feature_names}')\ncat_features_names = df_train[selected_feature_names].select_dtypes('category').columns.tolist()\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:51.455055Z","iopub.execute_input":"2024-05-12T08:26:51.456267Z","iopub.status.idle":"2024-05-12T08:26:51.472090Z","shell.execute_reply.started":"2024-05-12T08:26:51.456223Z","shell.execute_reply":"2024-05-12T08:26:51.470381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nsorted_feature_importances = feature_importances.sort_values(by='importance', ascending=True)\n# 中央値と平均値を計算\nmedian_value = sorted_feature_importances['importance'].median()\nmean_value = sorted_feature_importances['importance'].mean()\n\n# プロットの作成\nplt.figure(figsize=(20, 8))\nplt.bar(sorted_feature_importances['feature'], sorted_feature_importances['importance'], label='Importance')\nplt.axhline(y=median_value, color='r', linestyle='--', label='Median')\nplt.axhline(y=mean_value, color='g', linestyle='-', label='Mean')\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title('Feature Importances with Median and Mean')\nplt.xticks(rotation=90)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:51.475100Z","iopub.execute_input":"2024-05-12T08:26:51.475800Z","iopub.status.idle":"2024-05-12T08:26:51.490000Z","shell.execute_reply.started":"2024-05-12T08:26:51.475757Z","shell.execute_reply":"2024-05-12T08:26:51.488198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_feature_names =['month_decision',\n 'weekday_decision',\n 'birthdate_574D',\n 'dateofbirth_337D',\n 'days120_123L',\n 'days180_256L',\n 'days30_165L',\n 'days360_512L',\n 'days90_310L',\n 'education_1103M',\n 'maritalst_385M',\n 'numberofqueries_373L',\n 'pmtaverage_3A',\n 'pmtaverage_4527227A',\n 'pmtscount_423L',\n 'pmtssum_45A',\n 'requesttype_4525192L',\n 'responsedate_4527233D',\n 'amtinstpaidbefduel24m_4187115A',\n 'annuity_780A',\n 'avgdbddpdlast3m_4187120P',\n 'avginstallast24m_3658937A',\n 'avglnamtstart24m_4525187A',\n 'avgmaxdpdlast9m_3716943P',\n 'avgoutstandbalancel6m_4187114A',\n 'avgpmtlast12m_4525200A',\n 'bankacctype_710L',\n 'cardtype_51L',\n 'cntincpaycont9m_3716944L',\n 'cntpmts24_3658933L',\n 'credamount_770A',\n 'credtype_322L',\n 'currdebt_22A',\n 'datefirstoffer_1144D',\n 'datelastinstal40dpd_247D',\n 'datelastunpaid_3546854D',\n 'disbursedcredamount_1113A',\n 'disbursementtype_67L',\n 'downpmt_116A',\n 'dtlastpmtallstes_4499206D',\n 'eir_270L',\n 'firstclxcampaign_1125D',\n 'firstdatedue_489D',\n 'inittransactionamount_650A',\n 'interestrate_311L',\n 'isbidproduct_1095L',\n 'lastactivateddate_801D',\n 'lastapplicationdate_877D',\n 'lastapprcommoditycat_1041M',\n 'lastapprcredamount_781A',\n 'lastcancelreason_561M',\n 'lastdelinqdate_224D',\n 'lastrejectcommoditycat_161M',\n 'lastrejectcredamount_222A',\n 'lastrejectdate_50D',\n 'lastrejectreason_759M',\n 'lastst_736L',\n 'maininc_215A',\n 'maxannuity_159A',\n 'maxdbddpdlast1m_3658939P',\n 'maxdbddpdtollast12m_3658940P',\n 'maxdebt4_972A',\n 'maxdpdinstldate_3546855D',\n 'maxdpdinstlnum_3546846P',\n 'maxinstallast24m_3658928A',\n 'maxlnamtstart6m_4525199A',\n 'maxoutstandbalancel12m_4187113A',\n 'maxpmtlast3m_4525190A',\n 'mindbddpdlast24m_3658935P',\n 'mindbdtollast24m_4525191P',\n 'mobilephncnt_593L',\n 'numinstlswithdpd10_728L',\n 'numinstpaidlastcontr_4325080L',\n 'numinsttopaygr_769L',\n 'numinstunpaidmax_3546851L',\n 'pctinstlsallpaidearl3d_427L',\n 'pctinstlsallpaidlat10d_839L',\n 'pctinstlsallpaidlate1d_3546856L',\n 'pctinstlsallpaidlate4d_3546849L',\n 'pctinstlsallpaidlate6d_3546844L',\n 'pmtnum_254L',\n 'posfpd10lastmonth_333P',\n 'posfpd30lastmonth_3976960P',\n 'posfstqpd30lastmonth_3976962P',\n 'price_1097A',\n 'sumoutstandtotalest_4493215A',\n 'totalsettled_863A',\n 'totinstallast1m_4525188A',\n 'twobodfilling_608L',\n 'typesuite_864L',\n 'validfrom_1069D',\n 'max_annuity_853A',\n 'max_credacc_actualbalance_314A',\n 'max_credacc_credlmt_575A',\n 'max_credamount_590A',\n 'max_downpmt_134A',\n 'max_mainoccupationinc_437A',\n 'max_outstandingdebt_522A',\n 'last_annuity_853A',\n 'last_credamount_590A',\n 'last_currdebt_94A',\n 'last_downpmt_134A',\n 'last_mainoccupationinc_437A',\n 'last_maxdpdtolerance_577P',\n 'mean_annuity_853A',\n 'mean_credacc_actualbalance_314A',\n 'mean_credacc_minhisbal_90A',\n 'mean_credamount_590A',\n 'mean_currdebt_94A',\n 'mean_downpmt_134A',\n 'mean_mainoccupationinc_437A',\n 'mean_maxdpdtolerance_577P',\n 'mean_outstandingdebt_522A',\n 'max_approvaldate_319D',\n 'max_creationdate_885D',\n 'max_dateactivated_425D',\n 'max_dtlastpmt_581D',\n 'max_dtlastpmtallstes_3545839D',\n 'max_employedfrom_700D',\n 'max_firstnonzeroinstldate_307D',\n 'last_approvaldate_319D',\n 'last_dateactivated_425D',\n 'last_dtlastpmt_581D',\n 'last_dtlastpmtallstes_3545839D',\n 'last_employedfrom_700D',\n 'last_firstnonzeroinstldate_307D',\n 'mean_dtlastpmt_581D',\n 'mean_dtlastpmtallstes_3545839D',\n 'mean_employedfrom_700D',\n 'mean_firstnonzeroinstldate_307D',\n 'max_postype_4733339M',\n 'last_education_1138M',\n 'last_postype_4733339M',\n 'max_byoccupationinc_3656910L',\n 'max_childnum_21L',\n 'max_familystate_726L',\n 'max_pmtnum_8L',\n 'max_tenor_203L',\n 'last_byoccupationinc_3656910L',\n 'last_childnum_21L',\n 'last_familystate_726L',\n 'last_pmtnum_8L',\n 'last_tenor_203L',\n 'max_num_group1',\n 'last_num_group1',\n 'max_amount_4527230A',\n 'last_amount_4527230A',\n 'mean_amount_4527230A',\n 'max_num_group1_3',\n 'max_amount_4917619A',\n 'last_amount_4917619A',\n 'mean_amount_4917619A',\n 'max_pmtamount_36A',\n 'last_pmtamount_36A',\n 'mean_pmtamount_36A',\n 'max_processingdate_168D',\n 'last_processingdate_168D',\n 'mean_processingdate_168D',\n 'max_credlmt_230A',\n 'max_credlmt_935A',\n 'max_debtoutstand_525A',\n 'max_dpdmax_757P',\n 'max_instlamount_768A',\n 'max_instlamount_852A',\n 'max_monthlyinstlamount_332A',\n 'max_monthlyinstlamount_674A',\n 'max_outstandingamount_362A',\n 'max_overdueamountmax2_14A',\n 'max_overdueamountmax2_398A',\n 'max_overdueamountmax_155A',\n 'max_overdueamountmax_35A',\n 'max_residualamount_856A',\n 'max_totalamount_6A',\n 'max_totalamount_996A',\n 'max_totaloutstanddebtvalue_39A',\n 'mean_credlmt_230A',\n 'mean_credlmt_935A',\n 'mean_debtoutstand_525A',\n 'mean_dpdmax_139P',\n 'mean_dpdmax_757P',\n 'mean_instlamount_768A',\n 'mean_instlamount_852A',\n 'mean_monthlyinstlamount_332A',\n 'mean_monthlyinstlamount_674A',\n 'mean_outstandingamount_362A',\n 'mean_overdueamountmax2_14A',\n 'mean_overdueamountmax2_398A',\n 'mean_overdueamountmax_155A',\n 'mean_overdueamountmax_35A',\n 'mean_residualamount_856A',\n 'mean_totalamount_6A',\n 'mean_totalamount_996A',\n 'mean_totaloutstanddebtvalue_39A',\n 'max_dateofcredend_289D',\n 'max_dateofcredend_353D',\n 'max_dateofcredstart_181D',\n 'max_dateofcredstart_739D',\n 'max_dateofrealrepmt_138D',\n 'max_lastupdate_1112D',\n 'max_lastupdate_388D',\n 'max_numberofoverdueinstlmaxdat_148D',\n 'max_numberofoverdueinstlmaxdat_641D',\n 'max_overdueamountmax2date_1002D',\n 'max_overdueamountmax2date_1142D',\n 'last_refreshdate_3813885D',\n 'mean_dateofcredend_289D',\n 'mean_dateofcredend_353D',\n 'mean_dateofcredstart_181D',\n 'mean_dateofcredstart_739D',\n 'mean_dateofrealrepmt_138D',\n 'mean_lastupdate_388D',\n 'mean_numberofoverdueinstlmaxdat_148D',\n 'mean_numberofoverdueinstlmaxdat_641D',\n 'mean_overdueamountmax2date_1002D',\n 'mean_overdueamountmax2date_1142D',\n 'mean_refreshdate_3813885D',\n 'max_contractst_964M',\n 'max_financialinstitution_382M',\n 'max_financialinstitution_591M',\n 'max_annualeffectiverate_199L',\n 'max_annualeffectiverate_63L',\n 'max_dpdmaxdatemonth_442T',\n 'max_dpdmaxdatemonth_89T',\n 'max_dpdmaxdateyear_596T',\n 'max_nominalrate_281L',\n 'max_nominalrate_498L',\n 'max_numberofcontrsvalue_358L',\n 'max_numberofinstls_229L',\n 'max_numberofinstls_320L',\n 'max_numberofoutstandinstls_59L',\n 'max_numberofoverdueinstlmax_1039L',\n 'max_numberofoverdueinstlmax_1151L',\n 'max_overdueamountmaxdatemonth_284T',\n 'max_overdueamountmaxdatemonth_365T',\n 'max_prolongationcount_1120L',\n 'max_mainoccupationinc_384A',\n 'last_mainoccupationinc_384A',\n 'mean_mainoccupationinc_384A',\n 'max_birth_259D',\n 'max_empl_employedfrom_271D',\n 'last_birth_259D',\n 'mean_birth_259D',\n 'mean_empl_employedfrom_271D',\n 'max_language1_981M',\n 'last_language1_981M',\n 'max_empl_industry_691L',\n 'max_familystate_447L',\n 'max_incometype_1044T',\n 'max_relationshiptoclient_415T',\n 'max_sex_738L',\n 'last_incometype_1044T',\n 'last_relationshiptoclient_642T',\n 'last_sex_738L',\n 'max_pmts_dpd_303P',\n 'max_pmts_overdue_1140A',\n 'max_pmts_overdue_1152A',\n 'mean_pmts_dpd_1073P',\n 'mean_pmts_dpd_303P',\n 'mean_pmts_overdue_1140A',\n 'mean_pmts_overdue_1152A',\n 'max_collater_valueofguarantee_1124L',\n 'max_collater_valueofguarantee_876L']","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:51.492401Z","iopub.execute_input":"2024-05-12T08:26:51.492986Z","iopub.status.idle":"2024-05-12T08:26:51.524546Z","shell.execute_reply.started":"2024-05-12T08:26:51.492940Z","shell.execute_reply":"2024-05-12T08:26:51.523235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning(df_train, selected_feature_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:51.526309Z","iopub.execute_input":"2024-05-12T08:26:51.526680Z","iopub.status.idle":"2024-05-12T08:26:51.540546Z","shell.execute_reply.started":"2024-05-12T08:26:51.526651Z","shell.execute_reply":"2024-05-12T08:26:51.539495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission(test_df, submission_dir, submission_file):\n    df_subm = pd.read_csv(submission_dir)\n    df_subm.set_index(\"case_id\", inplace=True)\n    \n    df_subm[\"score\"] = test_df.set_index(\"case_id\")[\"pred_prob\"]\n    df_subm.to_csv(submission_file)\n\ntest_df = Predicting(df_test, selected_feature_names)\ncreate_submission(test_df, SAMPLE_SUB, \"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T08:26:51.541726Z","iopub.execute_input":"2024-05-12T08:26:51.542073Z","iopub.status.idle":"2024-05-12T08:26:52.593527Z","shell.execute_reply.started":"2024-05-12T08:26:51.542046Z","shell.execute_reply":"2024-05-12T08:26:52.591878Z"},"trusted":true},"execution_count":null,"outputs":[]}]}