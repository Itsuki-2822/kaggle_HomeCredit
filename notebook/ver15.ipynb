{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":8151133,"sourceType":"datasetVersion","datasetId":4820822},{"sourceId":8154375,"sourceType":"datasetVersion","datasetId":4823277},{"sourceId":8156628,"sourceType":"datasetVersion","datasetId":4825049},{"sourceId":8160583,"sourceType":"datasetVersion","datasetId":4827991},{"sourceId":8319468,"sourceType":"datasetVersion","datasetId":4941559}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 標準ライブラリ\nimport gc\nimport os\nimport pickle\nimport random\nimport sys\nimport warnings\nfrom itertools import combinations, permutations\nfrom pathlib import Path\nimport pytz\n\n# サードパーティのライブラリ\nimport category_encoders as ce\nimport joblib\nimport lightgbm as lgb\nimport numpy as np\n#import fireducks.pandas as pd\nimport pandas as pd\nimport polars as pl\nimport scipy as sp\nimport seaborn as sns\nimport torch\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\nfrom sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n                                     StratifiedGroupKFold, TimeSeriesSplit,\n                                     train_test_split)\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\nimport glob\nimport datetime","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:10:00.538829Z","iopub.execute_input":"2024-05-05T07:10:00.540134Z","iopub.status.idle":"2024-05-05T07:10:04.415657Z","shell.execute_reply.started":"2024-05-05T07:10:00.540091Z","shell.execute_reply":"2024-05-05T07:10:04.414463Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n    \n    train_data_path = os.path.join(home_directory, 'train/')\n    test_data_path = os.path.join(home_directory, 'test/')\n    \n    OOF_DATA_PATH = Path(home_directory) / 'oof'\n    MODEL_DATA_PATH = Path(home_directory) / 'models'\n    SUB_DATA_PATH = Path(home_directory) / 'submission'\n\n    def __init__(self):\n        self.create_directories()\n    \n    def create_directories(self):\n        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n            path.mkdir(parents=True, exist_ok=True)\n    \n    \n    VER = 15\n    AUTHOR = 'Mira'\n    COMPETITION = 'HomeCredit'\n\n    METHOD_LIST = ['lightgbm','catboost']\n    seed = 28\n    n_folds = 5\n    target_col = 'target'\n    metric = 'auc'\n    \n    metric_maximize_flag = True\n    num_boost_round = 500\n    early_stopping_round = 200\n    verbose = 25\n    classification_lgb_params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': 0.05,\n        'seed': seed,\n        #\"device_type\": \"gpu\",\n    }\n    classification_xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'learning_rate': 0.05,\n        'random_state': seed,\n        \"tree_method\": \"gpu_hist\",\n    }\n\n    classification_cat_params = {\n        'learning_rate': 0.05,\n        'iterations': num_boost_round,\n        'random_seed': seed,\n        #\"task_type\": \"GPU\",\n    }\n    model_weight_dict = {'lightgbm': 0.5,'catboost':0.5}\n    \n\nclass is_kaggle:\n    def __init__(self, Kaggle):\n        if Kaggle == \"Yes\":\n            self.path = Path(CFG.kaggle_directory)\n            CFG.MODEL_DATA_PATH = Path('/kaggle/input/05051545/models')\n        else:\n            self.path = Path(CFG.home_directory)\n            CFG.MODEL_DATA_PATH = Path(CFG.home_directory) / 'models'\n\ndef create_timestamped_file():\n    tz_tokyo = pytz.timezone('Asia/Tokyo')\n    now = datetime.datetime.now(tz=tz_tokyo)\n    filename = now.strftime('%m%d-%H%M') + '.txt'\n    full_path = CFG.MODEL_DATA_PATH / filename\n    full_path.touch()\n\n#create_timestamped_file()\ncfg_instance = CFG()      \nselector = is_kaggle(\"Yes\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:07:05.539148Z","iopub.status.idle":"2024-05-05T07:07:05.539783Z","shell.execute_reply.started":"2024-05-05T07:07:05.539471Z","shell.execute_reply":"2024-05-05T07:07:05.539496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:07:05.541837Z","iopub.status.idle":"2024-05-05T07:07:05.542472Z","shell.execute_reply.started":"2024-05-05T07:07:05.542184Z","shell.execute_reply":"2024-05-05T07:07:05.542208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    @staticmethod\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        expr_std = [pl.std(col).alias(f\"std_{col}\") for col in cols] \n\n        return expr_max + expr_min + expr_mean + expr_std \n\n    @staticmethod\n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n\n        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_min + expr_max\n\n    @staticmethod\n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        \n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n\n    @staticmethod\n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        \n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n    \n    @staticmethod\n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n\n    @staticmethod\n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    \n    if depth in [1, 2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n    \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    for path in glob.glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        \n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        \n        chunks.append(df)\n        \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n        \n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n        \n    df_base = df_base.pipe(Pipeline.handle_dates)\n    \n    return df_base","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n\n    # bool型の列をint型に変換\n    bool_cols = df_data.select_dtypes('bool').columns\n    df_data[bool_cols] = df_data[bool_cols].astype(int)\n\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT = selector.path\nTRAIN_DIR       = ROOT / \"parquet_files/train\"\nTEST_DIR        = ROOT / \"parquet_files/test\"\nSAMPLE_SUB = ROOT / \"sample_submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    cat_features = x_train.select_dtypes('category').columns.tolist()\n    lgb_train = lgb.Dataset(x_train, y_train,categorical_feature=cat_features)\n    lgb_valid = lgb.Dataset(x_valid, y_valid,categorical_feature=cat_features)\n    model = lgb.train(\n                params = CFG.classification_lgb_params,\n                train_set = lgb_train,\n                num_boost_round = CFG.num_boost_round,\n                valid_sets = [lgb_train, lgb_valid],\n                #feval = CFG.metric,\n                callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n                                              verbose=CFG.verbose)]\n            )\n    valid_pred = model.predict(x_valid)\n    \n    importance_df = pd.DataFrame({\n        'feature': features,\n        'importance': model.feature_importance(importance_type='gain')\n    })\n    importance_df['importance'] = importance_df['importance'] / np.sum(importance_df['importance'])\n    importance_df = importance_df.sort_values(by='importance', ascending=False)\n    print(importance_df)\n    return model, valid_pred\ndef xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n    model = xgb.train(\n                CFG.classification_xgb_params,\n                dtrain = xgb_train,\n                num_boost_round = CFG.num_boost_round,\n                evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n                early_stopping_rounds = CFG.early_stopping_round,\n                verbose_eval = CFG.verbose,\n                #feval = CFG.metric,\n                maximize = CFG.metric_maximize_flag,\n        )\n    valid_pred = model.predict(xgb.DMatrix(x_valid))\n    return model, valid_pred\ndef catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    cat_features = x_train.select_dtypes('category').columns.tolist()\n    cat_train = Pool(data=x_train, label=y_train,cat_features=cat_features)\n    cat_valid = Pool(data=x_valid, label=y_valid,cat_features=cat_features)\n    model = CatBoostClassifier(**CFG.classification_cat_params)\n    model.fit(cat_train,\n              eval_set = [cat_valid],\n              early_stopping_rounds = CFG.early_stopping_round,\n              verbose = CFG.verbose,\n              use_best_model = True)\n    valid_pred = model.predict_proba(x_valid)[:, 1]\n    return model, valid_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list):\n    features = [col for col in train_df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n    weeks = train_df[\"WEEK_NUM\"]\n    X = train_df.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n    y = train_df[\"target\"]\n    \n    oof_predictions = np.zeros(len(train_df))\n    oof_fold = np.zeros(len(train_df))\n    cv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n    for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups=weeks)):\n        print('-'*50)\n        print(f'{method} training fold {fold+1}')\n\n        x_train = train_df[features].iloc[train_index]\n        y_train = train_df[CFG.target_col].iloc[train_index]\n        x_valid = train_df[features].iloc[valid_index]\n        y_valid = train_df[CFG.target_col].iloc[valid_index]\n        if method == 'lightgbm':\n            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'xgboost':\n            model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'catboost':\n            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features)\n\n        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n\n        oof_predictions[valid_index] = valid_pred\n        oof_fold[valid_index] = fold + 1\n        del x_train, x_valid, y_train, y_valid, model, valid_pred\n        gc.collect()\n\n    score = roc_auc_score(train_df[CFG.target_col], oof_predictions)\n    print(f'{method} our out of folds CV f1score is {score}')\n\n    oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n    oof_df.to_csv(CFG.OOF_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n\ndef Learning(input_df: pd.DataFrame, features: list):\n    for method in CFG.METHOD_LIST:\n        gradient_boosting_model_cv_training(method, input_df, features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unified_inference(method: str, x_test: pd.DataFrame):\n    test_pred = np.zeros(len(x_test))\n    for fold in range(CFG.n_folds):\n        model_path = CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl'\n        model = pickle.load(open(model_path, 'rb'))\n\n        if method == 'lightgbm':\n            pred = model.predict(x_test)\n        elif method == 'xgboost':\n            pred = model.predict(xgb.DMatrix(x_test))\n        elif method == 'catboost':\n            pred = model.predict_proba(x_test)[:, 1]\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        test_pred += pred\n    \n    return test_pred / CFG.n_folds\n\ndef gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list):\n    x_test = test_df[features]\n    test_pred = unified_inference(method, x_test)\n    return test_pred\n\ndef Predicting(input_df: pd.DataFrame, features: list):\n    output_df = input_df.copy()\n    output_df['pred_prob'] = 0\n    for method in CFG.METHOD_LIST:\n        output_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, input_df, features)\n        output_df['pred_prob'] += CFG.model_weight_dict[method] * output_df[f'{method}_pred_prob']\n    return output_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedGroupKFold\n\ndef target_enc(df, test, col):\n    # 機能のセットアップ\n    features = [col for col in df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n    weeks = df[\"WEEK_NUM\"]\n    X = df[features]\n    y = df[CFG.target_col]\n    encoded_features = []\n\n    # K-分割クロスバリデーション\n    cv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n    for train_idx, val_idx in cv.split(X, y, groups=weeks):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n        y_train = y.iloc[train_idx]\n        \n        target_encoder = ce.TargetEncoder()\n        target_encoder.fit(X_train[col], y_train)\n        \n        X_valid[f'{col}_target_Encoded'] = target_encoder.transform(X_valid[col])\n        encoded_features.append(X_valid)\n\n    encoded_df = pd.concat(encoded_features).sort_index()\n    df[f'{col}_target_Encoded'] = encoded_df[f'{col}_target_Encoded']\n    \n    # テストデータに対するエンコード\n    target_encoder.fit(df[[col]], df[CFG.target_col])\n    test[f'{col}_target_Encoded'] = target_encoder.transform(test[[col]])\n    \n    return df, test\n\ndef encoder(df, test):\n    object_columns = [col for col in df.columns if df[col].dtype.name in ['object', 'category']]\n    bool_columns = [col for col in df.columns if df[col].dtype == 'bool']\n    \n    # ブール列を整数に変換\n    for col in bool_columns:\n        df[col] = df[col].astype(int)\n        test[col] = test[col].astype(int)\n    \n    # カテゴリカルデータの処理\n    for col in object_columns:\n        df[col] = df[col].astype('category').cat.add_categories(['Unknown']).fillna('Unknown')\n        test[col] = test[col].astype('category').cat.add_categories(['Unknown']).fillna('Unknown')\n        df, test = target_enc(df, test, col)\n\n    # オブジェクトカラムをドロップ\n    df.drop(object_columns, axis=1, inplace=True)\n    test.drop(object_columns, axis=1, inplace=True)\n    \n    return df, test\n\ndef preprocess(df, test):\n    df, test = encoder(df, test)\n    return df, test\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataPreprocessor:\n    def __init__(self, df):\n        self.df = df\n\n    def select_numerical_columns(self):\n        return self.df.select_dtypes(exclude='category').columns\n\n    def calculate_nan_groups(self, nums):\n        nans_df = self.df[nums].isna()\n        nans_groups = {}\n        for col in nums:\n            cur_group = nans_df[col].sum()\n            nans_groups.setdefault(cur_group, []).append(col)\n        del nans_df\n        gc.collect()\n        return nans_groups\n\n    def reduce_group(self, groups):\n        use = []\n        for group in groups:\n            max_unique = 0\n            selected_col = group[0]\n            for col in group:\n                unique_count = self.df[col].nunique()\n                if unique_count > max_unique:\n                    max_unique = unique_count\n                    selected_col = col\n            use.append(selected_col)\n        print('Use these:', use)\n        return use\n\n    def group_columns_by_correlation(self, matrix, threshold=0.8):\n        correlation_matrix = matrix.corr()\n        cols = list(matrix.columns)\n        groups = []\n\n        while cols:\n            base_col = cols.pop(0)\n            group = [base_col]\n            correlated_cols = [base_col]\n\n            for col in cols:\n                if correlation_matrix.loc[base_col, col] >= threshold:\n                    group.append(col)\n                    correlated_cols.append(col)\n\n            groups.append(group)\n            cols = [c for c in cols if c not in correlated_cols]\n        \n        return groups\n\n    def process_columns(self):\n        nums = self.select_numerical_columns()\n        nans_groups = self.calculate_nan_groups(nums)\n        uses = []\n\n        for count, columns in nans_groups.items():\n            if len(columns) > 1:\n                grps = self.group_columns_by_correlation(self.df[columns])\n                uses += self.reduce_group(grps)\n            else:\n                uses += columns\n\n            print('####### NAN count =', count)\n        \n        print('Selected numerical columns:', uses)\n        return uses\n\n    def update_dataframe(self):\n        uses = self.process_columns()\n        uses += list(self.df.select_dtypes(include='category').columns)\n        print('Total columns used:', len(uses))\n        self.df = self.df[uses]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = feature_eng(**train_data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndel train_data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\n\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)\n\n\npreprocessor = DataPreprocessor(df_train)\npreprocessor.update_dataframe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = feature_eng(**test_data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel test_data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train,df_test = preprocess(df_train,df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df_train.select_dtypes('category').columns:\n    df_train[col] = df_train[col].cat.add_categories(['Unknown']).fillna('Unknown')\n    df_test[col] = df_test[col].cat.add_categories(['Unknown']).fillna('Unknown')\n    \nfeatures = [col for col in df_train.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Learning(df_train, features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission(test_df, submission_dir, submission_file):\n    df_subm = pd.read_csv(submission_dir)\n    df_subm.set_index(\"case_id\", inplace=True)\n    \n    df_subm[\"score\"] = test_df.set_index(\"case_id\")[\"pred_prob\"]\n    df_subm.to_csv(submission_file)\n\ntest_df = Predicting(df_test, features)\ncreate_submission(test_df, SAMPLE_SUB, \"submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}