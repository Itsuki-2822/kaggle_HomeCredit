{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:28.899510Z\",\"iopub.execute_input\":\"2024-05-24T08:32:28.900611Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.168765Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:28.900567Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.167481Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport gc\nimport lightgbm as lgb  # type: ignore\nimport numpy as np  # type: ignore\nimport pandas as pd  # type: ignore\nimport polars as pl  # type: ignore\nimport warnings\nimport os\nimport shutil\n\nfrom catboost import CatBoostClassifier, Pool  # type: ignore\nfrom glob import glob\nimport joblib\nfrom IPython.display import display  # type: ignore\nfrom pathlib import Path\nfrom sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\nfrom sklearn.metrics import roc_auc_score  # type: ignore\nfrom sklearn.model_selection import StratifiedGroupKFold  # type: ignore\nfrom typing import Any\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nwarnings.filterwarnings(\"ignore\")\n\nROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\nTRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR = ROOT / \"parquet_files\" / \"test\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.171165Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.171864Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.338974Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.171821Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.337456Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nclass Utility:\n    @staticmethod\n    def get_feat_defs(ending_with: str) -> None:\n        \"\"\"\n        Retrieves feature definitions from a CSV file based on the specified ending.\n\n        Args:\n        - ending_with (str): Ending to filter feature definitions.\n\n        Returns:\n        - pl.DataFrame: Filtered feature definitions.\n        \"\"\"\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n\n        filtered_feats: pl.DataFrame = feat_defs.filter(\n            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n        )\n\n        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n            print(filtered_feats)\n\n        filtered_feats = None\n        feat_defs = None\n\n    @staticmethod\n    def find_index(lst: list[Any], item: Any) -> int | None:\n        \"\"\"\n        Finds the index of an item in a list.\n\n        Args:\n        - lst (list): List to search.\n        - item (Any): Item to find in the list.\n\n        Returns:\n        - int | None: Index of the item if found, otherwise None.\n        \"\"\"\n        try:\n            return lst.index(item)\n        except ValueError:\n            return None\n\n    @staticmethod\n    def dtype_to_str(dtype: pl.DataType) -> str:\n        \"\"\"\n        Converts Polars data type to string representation.\n\n        Args:\n        - dtype (pl.DataType): Polars data type.\n\n        Returns:\n        - str: String representation of the data type.\n        \"\"\"\n        dtype_map = {\n            pl.Decimal: \"Decimal\",\n            pl.Float32: \"Float32\",\n            pl.Float64: \"Float64\",\n            pl.UInt8: \"UInt8\",\n            pl.UInt16: \"UInt16\",\n            pl.UInt32: \"UInt32\",\n            pl.UInt64: \"UInt64\",\n            pl.Int8: \"Int8\",\n            pl.Int16: \"Int16\",\n            pl.Int32: \"Int32\",\n            pl.Int64: \"Int64\",\n            pl.Date: \"Date\",\n            pl.Datetime: \"Datetime\",\n            pl.Duration: \"Duration\",\n            pl.Time: \"Time\",\n            pl.Array: \"Array\",\n            pl.List: \"List\",\n            pl.Struct: \"Struct\",\n            pl.String: \"String\",\n            pl.Categorical: \"Categorical\",\n            pl.Enum: \"Enum\",\n            pl.Utf8: \"Utf8\",\n            pl.Binary: \"Binary\",\n            pl.Boolean: \"Boolean\",\n            pl.Null: \"Null\",\n            pl.Object: \"Object\",\n            pl.Unknown: \"Unknown\",\n        }\n\n        return dtype_map.get(dtype)\n\n    @staticmethod\n    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n        \"\"\"\n        Finds occurrences of features ending with a specific string in Parquet files.\n\n        Args:\n        - regex_path (str): Regular expression to match Parquet file paths.\n        - ending_with (str): Ending to filter feature names.\n\n        Returns:\n        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n        \"\"\"\n        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n        )\n        feat_defs.sort(by=[\"Variable\"])\n\n        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n        feats.sort()\n\n        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n\n        for path in glob(str(regex_path)):\n            df_schema: dict = pl.read_parquet_schema(path)\n\n            for feat, dtype in df_schema.items():\n                index: int = Utility.find_index(feats, feat)\n                if index != None:\n                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n                    occurrences[index][1].add(Path(path).stem)\n\n        data_types: list[str] = [None] * feat_defs.height\n        file_locs: list[str] = [None] * feat_defs.height\n\n        for i, feat in enumerate(feats):\n            data_types[i] = list(occurrences[i][0])\n            file_locs[i] = list(occurrences[i][1])\n\n        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n\n        return feat_defs\n\n    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n        \"\"\"\n        Reduces memory usage of a DataFrame by converting column types.\n\n        Args:\n        - df (pl.DataFrame): DataFrame to optimize.\n        - name (str): Name of the DataFrame.\n\n        Returns:\n        - pl.DataFrame: Optimized DataFrame.\n        \"\"\"\n        print(\n            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n        )\n\n        int_types = [\n            pl.Int8,\n            pl.Int16,\n            pl.Int32,\n            pl.Int64,\n            pl.UInt8,\n            pl.UInt16,\n            pl.UInt32,\n            pl.UInt64,\n        ]\n        float_types = [pl.Float32, pl.Float64]\n\n        for col in df.columns:\n            col_type = df[col].dtype\n            if col_type in int_types + float_types:\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if c_min is not None and c_max is not None:\n                    if col_type in int_types:\n                        if c_min >= 0:\n                            if (\n                                c_min >= np.iinfo(np.uint8).min\n                                and c_max <= np.iinfo(np.uint8).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt8))\n                            elif (\n                                c_min >= np.iinfo(np.uint16).min\n                                and c_max <= np.iinfo(np.uint16).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt16))\n                            elif (\n                                c_min >= np.iinfo(np.uint32).min\n                                and c_max <= np.iinfo(np.uint32).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt32))\n                            elif (\n                                c_min >= np.iinfo(np.uint64).min\n                                and c_max <= np.iinfo(np.uint64).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.UInt64))\n                        else:\n                            if (\n                                c_min >= np.iinfo(np.int8).min\n                                and c_max <= np.iinfo(np.int8).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int8))\n                            elif (\n                                c_min >= np.iinfo(np.int16).min\n                                and c_max <= np.iinfo(np.int16).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int16))\n                            elif (\n                                c_min >= np.iinfo(np.int32).min\n                                and c_max <= np.iinfo(np.int32).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int32))\n                            elif (\n                                c_min >= np.iinfo(np.int64).min\n                                and c_max <= np.iinfo(np.int64).max\n                            ):\n                                df = df.with_columns(df[col].cast(pl.Int64))\n                    elif col_type in float_types:\n                        if (\n                            c_min > np.finfo(np.float32).min\n                            and c_max < np.finfo(np.float32).max\n                        ):\n                            df = df.with_columns(df[col].cast(pl.Float32))\n\n        print(\n            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n        )\n\n        return df\n\n    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n        \"\"\"\n        Converts a Polars DataFrame to a Pandas DataFrame.\n\n        Args:\n        - df (pl.DataFrame): Polars DataFrame to convert.\n        - cat_cols (list[str]): List of categorical columns. Default is None.\n\n        Returns:\n        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n        \"\"\"\n        df: pd.DataFrame = df.to_pandas()\n\n        if cat_cols is None:\n            cat_cols = list(df.select_dtypes(\"object\").columns)\n\n        df[cat_cols] = df[cat_cols].astype(\"str\")\n\n        return df, cat_cols\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.340577Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.341140Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.363081Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.341079Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.361494Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"P\")\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"M\")\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"A\")\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"D\")\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"T\")\n# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"L\")\n# feat_defs:pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n# with pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n#     print(feat_defs)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.366890Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.367697Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.390519Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.367637Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.389144Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nclass Aggregator:\n    @staticmethod\n    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating maximum values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for maximum values.\n        \"\"\"\n        cols: list[str] = [\n            col\n            for col in df.columns\n            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n        ]\n\n        expr_max: list[pl.Series] = [\n            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n        ]\n\n        return expr_max\n\n    @staticmethod\n    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating minimum values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for minimum values.\n        \"\"\"\n        cols: list[str] = [\n            col\n            for col in df.columns\n            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n        ]\n\n        expr_min: list[pl.Series] = [\n            pl.col(col).min().alias(f\"min_{col}\") for col in cols\n        ]\n\n        return expr_min\n\n    @staticmethod\n    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating mean values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for mean values.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n\n        expr_mean: list[pl.Series] = [\n            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n        ]\n\n        return expr_mean\n\n    @staticmethod\n    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating variance for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for variance.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n\n        expr_mean: list[pl.Series] = [\n            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n        ]\n\n        return expr_mean\n\n    @staticmethod\n    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Generates expressions for calculating mode values for specific columns.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of expressions for mode values.\n        \"\"\"\n        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n\n        expr_mode: list[pl.Series] = [\n            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n        ]\n\n        return expr_mode\n\n    @staticmethod\n    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n        \"\"\"\n        Combines expressions for maximum, mean, and variance calculations.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - list[pl.Series]: List of combined expressions.\n        \"\"\"\n        exprs = (\n            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n        )\n\n        return exprs\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.392482Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.392917Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.410491Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.392884Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.409025Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nclass SchemaGen:\n    @staticmethod\n    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n        \"\"\"\n        Changes the data types of columns in the DataFrame.\n\n        Args:\n        - df (pl.LazyFrame): Input LazyFrame.\n\n        Returns:\n        - pl.LazyFrame: LazyFrame with modified data types.\n        \"\"\"\n        for col in df.columns:\n            if col == \"case_id\":\n                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n            elif col == \"date_decision\" or col[-1] == \"D\":\n                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n            elif col[-1] in [\"P\", \"A\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n        return df\n\n    @staticmethod\n    def scan_files(glob_path: str, depth: int = None):\n        chunks = []\n        for path in glob(str(glob_path)):\n            df = pl.read_parquet(path, low_memory=True, rechunk=True)\n            df = df.pipe(SchemaGen.change_dtypes)\n            if depth in [1, 2]:\n                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n            chunks.append(df)\n        df = pl.concat(chunks, how=\"vertical_relaxed\")\n        del chunks\n        gc.collect()\n        df = df.unique(subset=[\"case_id\"])\n        return df\n\n    @staticmethod\n    def join_dataframes(df_base, depth_0, depth_1, depth_2):\n        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n        return df_base\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.412300Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.412825Z\",\"iopub.status.idle\":\"2024-05-24T08:32:34.435625Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.412785Z\",\"shell.execute_reply\":\"2024-05-24T08:32:34.434131Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with filtered columns.\n    \"\"\"\n    for col in df.columns:\n        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n            null_pct = df[col].is_null().mean()\n\n            if null_pct > 0.95:\n                df = df.drop(col)\n\n    for col in df.columns:\n        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n            df[col].dtype == pl.String\n        ):\n            freq = df[col].n_unique()\n\n            if (freq > 200) | (freq == 1):\n                df = df.drop(col)\n\n    return df\n\n\ndef transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Transforms columns in the DataFrame according to predefined rules.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with transformed columns.\n    \"\"\"\n    if \"riskassesment_302T\" in df.columns:\n        if df[\"riskassesment_302T\"].dtype == pl.Null:\n            df = df.with_columns(\n                [\n                    pl.Series(\n                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n                    ),\n                    pl.Series(\n                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n                    ),\n                ]\n            )\n        else:\n            pct_low: pl.Series = (\n                df[\"riskassesment_302T\"]\n                .str.split(\" - \")\n                .apply(lambda x: x[0].replace(\"%\", \"\"))\n                .cast(pl.UInt8)\n            )\n            pct_high: pl.Series = (\n                df[\"riskassesment_302T\"]\n                .str.split(\" - \")\n                .apply(lambda x: x[1].replace(\"%\", \"\"))\n                .cast(pl.UInt8)\n            )\n\n            diff: pl.Series = pct_high - pct_low\n            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n\n            del pct_high, pct_low\n            gc.collect()\n\n            df = df.with_columns(\n                [\n                    diff.alias(\"riskassesment_302T_rng\"),\n                    avg.alias(\"riskassesment_302T_mean\"),\n                ]\n            )\n\n        df.drop(\"riskassesment_302T\")\n\n    return df\n\n\ndef handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"\n    Handles date columns in the DataFrame.\n\n    Args:\n    - df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n    - pl.DataFrame: DataFrame with transformed date columns.\n    \"\"\"\n    for col in df.columns:\n        if col.endswith(\"D\"):\n            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n\n    df = df.rename(\n        {\n            \"MONTH\": \"month\",\n            \"WEEK_NUM\": \"week_num\"\n        }\n    )\n            \n    df = df.with_columns(\n        [\n            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n        ]\n    )\n\n    return df.drop(\"date_decision\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:32:34.437396Z\",\"iopub.execute_input\":\"2024-05-24T08:32:34.437811Z\",\"iopub.status.idle\":\"2024-05-24T08:35:54.241472Z\",\"shell.execute_reply.started\":\"2024-05-24T08:32:34.437775Z\",\"shell.execute_reply\":\"2024-05-24T08:35:54.240264Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata_store: dict = {\n    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n    ],\n}\n\ndf_train: pl.DataFrame = (\n    SchemaGen.join_dataframes(**data_store)\n    .pipe(filter_cols)\n    .pipe(transform_cols)\n    .pipe(handle_dates)\n    .pipe(Utility.reduce_memory_usage, \"df_train\")\n)\n\ndel data_store\ngc.collect()\n\nprint(f\"Train data shape: {df_train.shape}\")\ndisplay(df_train.head(10))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:35:54.242893Z\",\"iopub.execute_input\":\"2024-05-24T08:35:54.243287Z\",\"iopub.status.idle\":\"2024-05-24T08:35:56.559687Z\",\"shell.execute_reply.started\":\"2024-05-24T08:35:54.243256Z\",\"shell.execute_reply\":\"2024-05-24T08:35:56.558389Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata_store: dict = {\n    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n    ],\n}\n\ndf_test: pl.DataFrame = (\n    SchemaGen.join_dataframes(**data_store)\n    .pipe(transform_cols)\n    .pipe(handle_dates)\n    .select([col for col in df_train.columns if col != \"target\"])\n    .pipe(Utility.reduce_memory_usage, \"df_test\")\n)\n\ndel data_store\ngc.collect()\n\nprint(f\"Test data shape: {df_test.shape}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:35:56.561210Z\",\"iopub.execute_input\":\"2024-05-24T08:35:56.561654Z\",\"iopub.status.idle\":\"2024-05-24T08:36:22.870809Z\",\"shell.execute_reply.started\":\"2024-05-24T08:35:56.561613Z\",\"shell.execute_reply\":\"2024-05-24T08:36:22.869478Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndf_train, cat_cols = Utility.to_pandas(df_train)\ndf_test, cat_cols = Utility.to_pandas(df_test, cat_cols)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:36:22.875063Z\",\"iopub.execute_input\":\"2024-05-24T08:36:22.875585Z\",\"iopub.status.idle\":\"2024-05-24T08:36:22.886734Z\",\"shell.execute_reply.started\":\"2024-05-24T08:36:22.875542Z\",\"shell.execute_reply\":\"2024-05-24T08:36:22.885190Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nclass VotingModel(BaseEstimator, ClassifierMixin):\n    \"\"\"\n    A voting ensemble model that combines predictions from multiple estimators.\n\n    Parameters:\n    - estimators (list): List of base estimators.\n\n    Attributes:\n    - estimators (list): List of base estimators.\n\n    Methods:\n    - fit(X, y=None): Fit the model to the training data.\n    - predict(X): Predict class labels for samples.\n    - predict_proba(X): Predict class probabilities for samples.\n    \"\"\"\n\n    def __init__(self, estimators: list[BaseEstimator]):\n        \"\"\"\n        Initialize the VotingModel with a list of base estimators.\n\n        Args:\n        - estimators (list): List of base estimators.\n        \"\"\"\n        super().__init__()\n        self.estimators = estimators\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model to the training data.\n\n        Args:\n        - X: Input features.\n        - y: Target labels (ignored).\n\n        Returns:\n        - self: Returns the instance itself.\n        \"\"\"\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples.\n\n        Args:\n        - X: Input features.\n\n        Returns:\n        - numpy.ndarray: Predicted class labels.\n        \"\"\"\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n\n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for samples.\n\n        Args:\n        - X: Input features.\n\n        Returns:\n        - numpy.ndarray: Predicted class probabilities.\n        \"\"\"\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:04:29.320868Z","iopub.execute_input":"2024-05-27T06:04:29.321477Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set a seed for various non-deterministic processes for reproducibility\nimport random\ndef seed_it_all(seed=7):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nSEED = 0\n\n# set the seed for this run\nseed_it_all(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\ndevice='gpu'\nest_cnt=6000\nDRY_RUN = True if sample.shape[0] == 10 else False   \nif DRY_RUN:\n    device='cpu'\n    df_train = df_train.iloc[:5000]\n    est_cnt=600\n\nprint(device)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-05-24T08:36:22.920891Z\",\"iopub.execute_input\":\"2024-05-24T08:36:22.921381Z\",\"iopub.status.idle\":\"2024-05-24T08:36:24.421036Z\",\"shell.execute_reply.started\":\"2024-05-24T08:36:22.921340Z\",\"shell.execute_reply\":\"2024-05-24T08:36:24.419720Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nX = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\ny = df_train[\"target\"]\n\nweeks = df_train[\"week_num\"]\n\ndel df_train\ngc.collect()\n\ncv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"params_lgb = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params_lgb2 = {\n    \"boosting_type\": \"goss\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitted_models_cb = []\nfitted_models_lgb = []\nfitted_models_lgb2 = []\nfitted_models_eclf = []\ncv_scores_cb = []\ncv_scores_lgb = []\ncv_scores_lgb2 = []\ncv_scores_eclf = []\n\nfor idx_train, idx_valid in cv.split(X, y, groups=weeks):#\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]# \n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n    clf_cb = CatBoostClassifier(\n        best_model_min_trees = 1000,\n        boosting_type = \"Plain\",\n        eval_metric = \"AUC\",\n        iterations = est_cnt,\n        learning_rate = 0.05,\n        l2_leaf_reg = 10,\n        max_leaves = 64,\n        random_seed = SEED,\n        task_type = \"GPU\",\n        use_best_model = True\n    )\n    random_seed=SEED\n    clf_cb.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models_cb.append(clf_cb)\n    y_pred_valid = clf_cb.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_cb.append(auc_score)\n    \n    \n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n    \n    clf_lgb = LGBMClassifier(**params_lgb)\n    clf_lgb.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb.append(clf_lgb)\n    y_pred_valid = clf_lgb.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb.append(auc_score)\n    \n    clf_lgb2 = LGBMClassifier(**params_lgb2)\n    clf_lgb2.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb2.append(clf_lgb2)\n    y_pred_valid = clf_lgb2.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb2.append(auc_score)\n \n    eclf = VotingClassifier(\n     estimators=[('lgb', clf_lgb), ('lgb2', clf_lgb2)],\n     voting='soft', weights=[1, 1])   \n    eclf = eclf.fit(X_train, y_train)\n    fitted_models_eclf.append(eclf)\n    y_pred_valid = eclf.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_eclf.append(auc_score)","metadata":{"execution":{"iopub.status.idle":"2024-05-27T06:14:56.704068Z","shell.execute_reply.started":"2024-05-27T06:07:55.099877Z","shell.execute_reply":"2024-05-27T06:14:56.703190Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"300:\ttest: 0.7264932\tbest: 0.7460781 (75)\ttotal: 27.8s\tremaining: 27.6s\n599:\ttest: 0.7044074\tbest: 0.7460781 (75)\ttotal: 47.3s\tremaining: 0us\nbestTest = 0.7460781336\nbestIteration = 75\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.718568\nEarly stopping, best iteration is:\n[211]\tvalid_0's auc: 0.720322\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.726071\nEarly stopping, best iteration is:\n[153]\tvalid_0's auc: 0.731008\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.5700632\tbest: 0.5700632 (0)\ttotal: 72.7ms\tremaining: 43.5s\n300:\ttest: 0.7906042\tbest: 0.8068830 (45)\ttotal: 19.8s\tremaining: 19.6s\n599:\ttest: 0.7835118\tbest: 0.8068830 (45)\ttotal: 39.4s\tremaining: 0us\nbestTest = 0.8068830371\nbestIteration = 45\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[117]\tvalid_0's auc: 0.787733\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[126]\tvalid_0's auc: 0.77848\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.5700818\tbest: 0.5700818 (0)\ttotal: 69.6ms\tremaining: 41.7s\n300:\ttest: 0.8288592\tbest: 0.8349068 (150)\ttotal: 19.7s\tremaining: 19.5s\n599:\ttest: 0.8227294\tbest: 0.8349068 (150)\ttotal: 39.6s\tremaining: 0us\nbestTest = 0.8349068165\nbestIteration = 150\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.843828\nEarly stopping, best iteration is:\n[156]\tvalid_0's auc: 0.850204\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.849027\nEarly stopping, best iteration is:\n[232]\tvalid_0's auc: 0.851818\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.4743321\tbest: 0.4743321 (0)\ttotal: 70ms\tremaining: 41.9s\n300:\ttest: 0.7414767\tbest: 0.7427342 (235)\ttotal: 19.7s\tremaining: 19.6s\n599:\ttest: 0.7608149\tbest: 0.7609546 (590)\ttotal: 39.5s\tremaining: 0us\nbestTest = 0.7609546185\nbestIteration = 590\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[93]\tvalid_0's auc: 0.789152\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[94]\tvalid_0's auc: 0.77884\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"0:\ttest: 0.4868561\tbest: 0.4868561 (0)\ttotal: 64.7ms\tremaining: 38.7s\n300:\ttest: 0.7861982\tbest: 0.7936170 (70)\ttotal: 19.7s\tremaining: 19.6s\n599:\ttest: 0.8031915\tbest: 0.8031915 (599)\ttotal: 39.6s\tremaining: 0us\nbestTest = 0.803191483\nbestIteration = 599\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.824636\nEarly stopping, best iteration is:\n[251]\tvalid_0's auc: 0.828639\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.823488\nEarly stopping, best iteration is:\n[327]\tvalid_0's auc: 0.830767\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"CatBoost\")   \nprint(\"CV AUC scores: \", cv_scores_cb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_cb))\nprint(\"LightGBM\")\nprint(\"CV AUC scores: \", cv_scores_lgb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb))\nprint(\"LightGBM_goss\")\nprint(\"CV AUC scores: \", cv_scores_lgb2)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb2))\nprint(\"Ensemble of LGBM and LGBM_goss\")\nprint(\"CV AUC scores: \", cv_scores_eclf)\nprint(\"Maximum CV AUC score: \", max(cv_scores_eclf))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:14:56.705398Z","iopub.execute_input":"2024-05-27T06:14:56.705842Z","iopub.status.idle":"2024-05-27T06:14:56.713781Z","shell.execute_reply.started":"2024-05-27T06:14:56.705807Z","shell.execute_reply":"2024-05-27T06:14:56.712799Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CatBoost\nCV AUC scores:  [0.7044074182337848, 0.7835117700699112, 0.8227293872971567, 0.7608148893360162, 0.803191489361702]\nMaximum CV AUC score:  0.8227293872971567\nLightGBM\nCV AUC scores:  [0.7203221929910033, 0.7877334594211219, 0.8502038694141149, 0.789151576123407, 0.8286394176931691]\nMaximum CV AUC score:  0.8502038694141149\nLightGBM_goss\nCV AUC scores:  [0.73100782747085, 0.778479516363268, 0.8518184057138165, 0.7788397048960429, 0.8307670772676372]\nMaximum CV AUC score:  0.8518184057138165\nEnsemble of LGBM and LGBM_goss\nCV AUC scores:  [0.7045048556302576, 0.7527778715930966, 0.8206222806009359, 0.7828638497652582, 0.8146976483762598]\nMaximum CV AUC score:  0.8206222806009359\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# 交差検証の各分割での検証データのインデックスを用いてメタ特徴量と対応するラベルを集める関数\ndef collect_predictions(fitted_models, X, cv, y,cat_cols):\n    meta_features = []\n    meta_labels = []\n    X[cat_cols] = X[cat_cols].astype(\"category\")\n    for _, idx_valid in cv.split(X, y, groups=weeks):\n        X_valid = X.iloc[idx_valid]\n        X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n        preds = [model.predict_proba(X_valid)[:, 1] for model in fitted_models]\n        meta_features.append(preds)\n        meta_labels.append(y.iloc[idx_valid])\n    return np.concatenate(meta_features, axis=1), np.concatenate(meta_labels)\n\n# 各モデルからメタ特徴量を生成\nmeta_features_cb, meta_labels = collect_predictions(fitted_models_cb, X, cv, y,cat_cols)\nmeta_features_lgb, _ = collect_predictions(fitted_models_lgb, X, cv, y,cat_cols)\nmeta_features_lgb2, _ = collect_predictions(fitted_models_lgb2, X, cv, y,cat_cols)\nmeta_features_eclf, _ = collect_predictions(fitted_models_eclf, X, cv, y,cat_cols)\n\n# 全てのメタ特徴量を結合\nmeta_features = np.hstack([meta_features_cb, meta_features_lgb, meta_features_lgb2, meta_features_eclf])\ndisplay(meta_features)\ndisplay(meta_labels)\n\n# メタ特徴量のみを使用してメタモデルを訓練\nX_meta_train, X_meta_valid, y_meta_train, y_meta_valid = train_test_split(\n    meta_features, meta_labels, test_size=0.2, random_state=SEED)\n\n# メタモデル（LightGBM）の訓練\nmeta_clf = LGBMClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    num_leaves=31,\n    random_state=SEED\n)\nmeta_clf.fit(X_meta_train, y_meta_train)\n\n# メタモデルの評価\ny_meta_pred = meta_clf.predict_proba(X_meta_valid)[:, 1]\nmeta_auc_score = roc_auc_score(y_meta_valid, y_meta_pred)\nprint(f'Meta model AUC: {meta_auc_score:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:14:56.715395Z","iopub.execute_input":"2024-05-27T06:14:56.715951Z","iopub.status.idle":"2024-05-27T06:15:12.355616Z","shell.execute_reply.started":"2024-05-27T06:14:56.715918Z","shell.execute_reply":"2024-05-27T06:15:12.353978Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"array([[3.35632618e-02, 1.13002394e-02, 9.88102256e-02, ...,\n        6.91147278e-05, 5.05896140e-05, 4.13471753e-05],\n       [2.14529209e-02, 9.59534202e-03, 6.22929155e-02, ...,\n        1.30947781e-04, 5.75250667e-04, 1.03609841e-04],\n       [3.82794773e-02, 1.42616365e-02, 5.54670385e-02, ...,\n        8.79105134e-05, 1.68466978e-04, 5.26650457e-05],\n       [3.03608829e-02, 9.74713739e-03, 4.26370019e-02, ...,\n        1.22819122e-04, 4.02924259e-05, 2.49400077e-04],\n       [2.14414812e-02, 1.08690634e-02, 5.38447734e-02, ...,\n        8.20588244e-05, 2.10653742e-04, 3.54340330e-05]])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m display(meta_labels)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# メタ特徴量のみを使用してメタモデルを訓練\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m X_meta_train, X_meta_valid, y_meta_train, y_meta_valid \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# メタモデル（LightGBM）の訓練\u001b[39;00m\n\u001b[1;32m     35\u001b[0m meta_clf \u001b[38;5;241m=\u001b[39m LGBMClassifier(\n\u001b[1;32m     36\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     37\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     38\u001b[0m     num_leaves\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m,\n\u001b[1;32m     39\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m     40\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5, 5000]"],"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [5, 5000]","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, RegressorMixin\nclass VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        \n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n        \n        X[cat_cols] = X[cat_cols].astype(\"category\")\n        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[5:]]\n        \n        return np.mean(y_preds, axis=0)\n\n#model = VotingModel(fitted_models_cb+fitted_models_eclf)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:15:12.356591Z","iopub.status.idle":"2024-05-27T06:15:12.356928Z","shell.execute_reply.started":"2024-05-27T06:15:12.356772Z","shell.execute_reply":"2024-05-27T06:15:12.356785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"## import numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# テストデータの前処理\ndf_test = df_test.drop(columns=[\"week_num\"])\ndf_test = df_test.set_index(\"case_id\")\n\n# テストデータに対するメタ特徴量を生成する関数\ndef generate_test_meta_features(fitted_models, X):\n    meta_features = []\n    for model in fitted_models:\n        if hasattr(model, 'predict_proba'):\n            y_pred = model.predict_proba(X)[:, 1]\n        else:\n            y_pred = model.predict(X)\n        meta_features.append(y_pred)\n    return np.column_stack(meta_features)\n\n# テストデータでの予測\ntest_meta_features_cb = generate_test_meta_features(fitted_models_cb, df_test)\ntest_meta_features_lgb = generate_test_meta_features(fitted_models_lgb, df_test)\ntest_meta_features_lgb2 = generate_test_meta_features(fitted_models_lgb2, df_test)\ntest_meta_features_eclf = generate_test_meta_features(fitted_models_eclf, df_test)\n\n# 全てのテストメタ特徴量を結合\ntest_meta_features = np.hstack([test_meta_features_cb, test_meta_features_lgb, test_meta_features_lfgb2, test_meta_features_eclf])\n\n# メタモデルを用いてテストデータのスコアを予測\ny_pred_test = meta_clf.predict_proba(test_meta_features)[:, 1]\n\n# サブミッションファイルの準備\ndf_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\ndf_subm = df_subm.set_index(\"case_id\")\ndf_subm[\"score\"] = y_pred_test\n\n# サブミッションファイルの出力\ndf_subm.to_csv(\"submission.csv\")\n\n# サブミッションファイルを表示\ndf_subm\n","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:15:12.357908Z","iopub.status.idle":"2024-05-27T06:15:12.358302Z","shell.execute_reply.started":"2024-05-27T06:15:12.358111Z","shell.execute_reply":"2024-05-27T06:15:12.358129Z"},"trusted":true},"execution_count":null,"outputs":[]}]}