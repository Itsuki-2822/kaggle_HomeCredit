{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:43.545610Z","iopub.status.busy":"2024-05-15T00:26:43.545138Z","iopub.status.idle":"2024-05-15T00:26:47.864324Z","shell.execute_reply":"2024-05-15T00:26:47.862886Z","shell.execute_reply.started":"2024-05-15T00:26:43.545575Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mira/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["# 標準ライブラリ\n","import gc\n","import os\n","import pickle\n","import random\n","import sys\n","import warnings\n","from itertools import combinations, permutations\n","from pathlib import Path\n","import pytz\n","\n","\n","# サードパーティのライブラリ\n","import category_encoders as ce\n","import joblib\n","import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","import polars as pl\n","import scipy as sp\n","import seaborn as sns\n","import xgboost as xgb\n","from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n","from dateutil.relativedelta import relativedelta\n","from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\n","from sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n","                                     StratifiedGroupKFold, TimeSeriesSplit,\n","                                     train_test_split)\n","from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n","\n","from tqdm.auto import tqdm\n","from sklearn.linear_model import LogisticRegression\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","warnings.filterwarnings('ignore')\n","\n","\n","from catboost import CatBoostClassifier, Pool  # type: ignore\n","from glob import glob\n","from IPython.display import display  # type: ignore\n","from pathlib import Path\n","from sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\n","from sklearn.metrics import roc_auc_score  # type: ignore\n","from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n","from typing import Any"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class CFG:\n","    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n","    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n","    \n","    train_data_path = os.path.join(home_directory, 'train/')\n","    test_data_path = os.path.join(home_directory, 'test/')\n","    \n","    OOF_DATA_PATH = Path(home_directory) / 'oof'\n","    MODEL_DATA_PATH = Path(home_directory) / 'models'\n","    SUB_DATA_PATH = Path(home_directory) / 'submission'\n","\n","    def __init__(self):\n","        self.create_directories()\n","    \n","    def create_directories(self):\n","        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n","            path.mkdir(parents=True, exist_ok=True)\n","    \n","    \n","    VER = 20_2\n","    AUTHOR = 'Mira'\n","    COMPETITION = 'HomeCredit'\n","\n","    METHOD_LIST = ['lightgbm','catboost']\n","    #METHOD_LIST = ['lightgbm']\n","    seed = 28\n","    n_folds = 5\n","    target_col = 'target'\n","    metric = 'auc'\n","    \n","    metric_maximize_flag = True\n","    num_boost_round = 500\n","    early_stopping_round = 200\n","    verbose = 25\n","    classification_lgb_params = {\n","        \"boosting_type\": \"gbdt\",\n","        \"objective\": \"binary\",\n","        \"metric\": \"auc\",\n","        \"max_depth\": 10,  \n","        \"learning_rate\": 0.05,\n","        \"n_estimators\": num_boost_round,  \n","        \"colsample_bytree\": 0.8,\n","        \"colsample_bynode\": 0.8,\n","        \"verbose\": -1,\n","        \"reg_alpha\": 0.1,\n","        \"reg_lambda\": 10,\n","        \"extra_trees\":True,\n","        'num_leaves':64,\n","        'seed': seed,\n","    }\n","    classification_xgb_params = {\n","        'objective': 'binary:logistic',\n","        'eval_metric': 'logloss',\n","        'learning_rate': 0.05,\n","        'random_state': seed,\n","        \"tree_method\": \"gpu_hist\",\n","    }\n","\n","    classification_cat_params = {\n","        'iterations':num_boost_round,                \n","        'learning_rate':0.05,\n","        'depth':10,                     \n","        'l2_leaf_reg':10,                  \n","        'loss_function':'Logloss',        \n","        'eval_metric':'AUC',              \n","        'bootstrap_type':'Bernoulli',    \n","        'subsample':0.8,                  \n","        'colsample_bylevel':0.8,         \n","        'verbose':False,                  \n","        'leaf_estimation_iterations':10,       \n","        'random_seed':seed,\n","        #\"task_type\": \"GPU\",\n","    }\n","    model_weight_dict = {'lightgbm': 0.5,'catboost':0.5}\n","    #model_weight_dict = {'lightgbm': 1}\n","\n","class is_kaggle:\n","    def __init__(self, Kaggle):\n","        if Kaggle == \"Yes\":\n","            self.path = Path(CFG.kaggle_directory)\n","            CFG.MODEL_DATA_PATH = Path('/kaggle/input/05061800/models')\n","        else:\n","            self.path = Path(CFG.home_directory)\n","            CFG.MODEL_DATA_PATH = Path(CFG.home_directory) / 'models'\n","\n","def create_timestamped_file():\n","    tz_tokyo = pytz.timezone('Asia/Tokyo')\n","    now = datetime.datetime.now(tz=tz_tokyo)\n","    filename = now.strftime('%m%d-%H%M') + '.txt'\n","    full_path = CFG.MODEL_DATA_PATH / filename\n","    full_path.touch()\n","\n","#create_timestamped_file()\n","cfg_instance = CFG()      \n","selector = is_kaggle(\"No\")\n","\n","ROOT = selector.path\n","TRAIN_DIR       = ROOT / \"parquet_files/train\"\n","TEST_DIR        = ROOT / \"parquet_files/test\"\n","SAMPLE_SUB = ROOT / \"sample_submission.csv\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.867596Z","iopub.status.busy":"2024-05-15T00:26:47.867113Z","iopub.status.idle":"2024-05-15T00:26:47.908884Z","shell.execute_reply":"2024-05-15T00:26:47.907991Z","shell.execute_reply.started":"2024-05-15T00:26:47.867539Z"},"trusted":true},"outputs":[],"source":["class Utility:\n","    @staticmethod\n","    def get_feat_defs(ending_with: str) -> None:\n","        \"\"\"\n","        Retrieves feature definitions from a CSV file based on the specified ending.\n","\n","        Args:\n","        - ending_with (str): Ending to filter feature definitions.\n","\n","        Returns:\n","        - pl.DataFrame: Filtered feature definitions.\n","        \"\"\"\n","        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n","\n","        filtered_feats: pl.DataFrame = feat_defs.filter(\n","            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n","        )\n","\n","        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n","            print(filtered_feats)\n","\n","        filtered_feats = None\n","        feat_defs = None\n","\n","    @staticmethod\n","    def find_index(lst: list[Any], item: Any) -> int | None:\n","        \"\"\"\n","        Finds the index of an item in a list.\n","\n","        Args:\n","        - lst (list): List to search.\n","        - item (Any): Item to find in the list.\n","\n","        Returns:\n","        - int | None: Index of the item if found, otherwise None.\n","        \"\"\"\n","        try:\n","            return lst.index(item)\n","        except ValueError:\n","            return None\n","\n","    @staticmethod\n","    def dtype_to_str(dtype: pl.DataType) -> str:\n","        \"\"\"\n","        Converts Polars data type to string representation.\n","\n","        Args:\n","        - dtype (pl.DataType): Polars data type.\n","\n","        Returns:\n","        - str: String representation of the data type.\n","        \"\"\"\n","        dtype_map = {\n","            pl.Decimal: \"Decimal\",\n","            pl.Float32: \"Float32\",\n","            pl.Float64: \"Float64\",\n","            pl.UInt8: \"UInt8\",\n","            pl.UInt16: \"UInt16\",\n","            pl.UInt32: \"UInt32\",\n","            pl.UInt64: \"UInt64\",\n","            pl.Int8: \"Int8\",\n","            pl.Int16: \"Int16\",\n","            pl.Int32: \"Int32\",\n","            pl.Int64: \"Int64\",\n","            pl.Date: \"Date\",\n","            pl.Datetime: \"Datetime\",\n","            pl.Duration: \"Duration\",\n","            pl.Time: \"Time\",\n","            pl.Array: \"Array\",\n","            pl.List: \"List\",\n","            pl.Struct: \"Struct\",\n","            pl.String: \"String\",\n","            pl.Categorical: \"Categorical\",\n","            pl.Enum: \"Enum\",\n","            pl.Utf8: \"Utf8\",\n","            pl.Binary: \"Binary\",\n","            pl.Boolean: \"Boolean\",\n","            pl.Null: \"Null\",\n","            pl.Object: \"Object\",\n","            pl.Unknown: \"Unknown\",\n","        }\n","\n","        return dtype_map.get(dtype)\n","\n","    @staticmethod\n","    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n","        \"\"\"\n","        Finds occurrences of features ending with a specific string in Parquet files.\n","\n","        Args:\n","        - regex_path (str): Regular expression to match Parquet file paths.\n","        - ending_with (str): Ending to filter feature names.\n","\n","        Returns:\n","        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n","        \"\"\"\n","        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n","            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n","        )\n","        feat_defs.sort(by=[\"Variable\"])\n","\n","        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n","        feats.sort()\n","\n","        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n","\n","        for path in glob(str(regex_path)):\n","            df_schema: dict = pl.read_parquet_schema(path)\n","\n","            for feat, dtype in df_schema.items():\n","                index: int = Utility.find_index(feats, feat)\n","                if index != None:\n","                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n","                    occurrences[index][1].add(Path(path).stem)\n","\n","        data_types: list[str] = [None] * feat_defs.height\n","        file_locs: list[str] = [None] * feat_defs.height\n","\n","        for i, feat in enumerate(feats):\n","            data_types[i] = list(occurrences[i][0])\n","            file_locs[i] = list(occurrences[i][1])\n","\n","        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n","        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n","\n","        return feat_defs\n","\n","    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n","        \"\"\"\n","        Reduces memory usage of a DataFrame by converting column types.\n","\n","        Args:\n","        - df (pl.DataFrame): DataFrame to optimize.\n","        - name (str): Name of the DataFrame.\n","\n","        Returns:\n","        - pl.DataFrame: Optimized DataFrame.\n","        \"\"\"\n","        print(\n","            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n","        )\n","\n","        int_types = [\n","            pl.Int8,\n","            pl.Int16,\n","            pl.Int32,\n","            pl.Int64,\n","            pl.UInt8,\n","            pl.UInt16,\n","            pl.UInt32,\n","            pl.UInt64,\n","        ]\n","        float_types = [pl.Float32, pl.Float64]\n","\n","        for col in df.columns:\n","            col_type = df[col].dtype\n","            if col_type in int_types + float_types:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","\n","                if c_min is not None and c_max is not None:\n","                    if col_type in int_types:\n","                        if c_min >= 0:\n","                            if (\n","                                c_min >= np.iinfo(np.uint8).min\n","                                and c_max <= np.iinfo(np.uint8).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt8))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint16).min\n","                                and c_max <= np.iinfo(np.uint16).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt16))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint32).min\n","                                and c_max <= np.iinfo(np.uint32).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt32))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint64).min\n","                                and c_max <= np.iinfo(np.uint64).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt64))\n","                        else:\n","                            if (\n","                                c_min >= np.iinfo(np.int8).min\n","                                and c_max <= np.iinfo(np.int8).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int8))\n","                            elif (\n","                                c_min >= np.iinfo(np.int16).min\n","                                and c_max <= np.iinfo(np.int16).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int16))\n","                            elif (\n","                                c_min >= np.iinfo(np.int32).min\n","                                and c_max <= np.iinfo(np.int32).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int32))\n","                            elif (\n","                                c_min >= np.iinfo(np.int64).min\n","                                and c_max <= np.iinfo(np.int64).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int64))\n","                    elif col_type in float_types:\n","                        if (\n","                            c_min > np.finfo(np.float32).min\n","                            and c_max < np.finfo(np.float32).max\n","                        ):\n","                            df = df.with_columns(df[col].cast(pl.Float32))\n","\n","        print(\n","            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n","        )\n","\n","        return df\n","\n","    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n","        \"\"\"\n","        Converts a Polars DataFrame to a Pandas DataFrame.\n","\n","        Args:\n","        - df (pl.DataFrame): Polars DataFrame to convert.\n","        - cat_cols (list[str]): List of categorical columns. Default is None.\n","\n","        Returns:\n","        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n","        \"\"\"\n","        df: pd.DataFrame = df.to_pandas()\n","\n","        if cat_cols is None:\n","            cat_cols = list(df.select_dtypes(\"object\").columns)\n","\n","        df[cat_cols] = df[cat_cols].astype(\"str\")\n","\n","        return df, cat_cols"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.910676Z","iopub.status.busy":"2024-05-15T00:26:47.910274Z","iopub.status.idle":"2024-05-15T00:26:47.918475Z","shell.execute_reply":"2024-05-15T00:26:47.917607Z","shell.execute_reply.started":"2024-05-15T00:26:47.910636Z"},"trusted":true},"outputs":[],"source":["# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"P\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"M\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"A\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"D\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"T\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"L\")\n","# feat_defs:pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n","# with pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n","#     print(feat_defs)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.920983Z","iopub.status.busy":"2024-05-15T00:26:47.920664Z","iopub.status.idle":"2024-05-15T00:26:47.937047Z","shell.execute_reply":"2024-05-15T00:26:47.936162Z","shell.execute_reply.started":"2024-05-15T00:26:47.920959Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    @staticmethod\n","    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating maximum values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for maximum values.\n","        \"\"\"\n","        cols: list[str] = [\n","            col\n","            for col in df.columns\n","            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n","        ]\n","\n","        expr_max: list[pl.Series] = [\n","            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n","        ]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating minimum values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for minimum values.\n","        \"\"\"\n","        cols: list[str] = [\n","            col\n","            for col in df.columns\n","            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n","        ]\n","\n","        expr_min: list[pl.Series] = [\n","            pl.col(col).min().alias(f\"min_{col}\") for col in cols\n","        ]\n","\n","        return expr_min\n","\n","    @staticmethod\n","    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating mean values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for mean values.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n","\n","        expr_mean: list[pl.Series] = [\n","            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n","        ]\n","\n","        return expr_mean\n","\n","    @staticmethod\n","    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating variance for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for variance.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n","\n","        expr_mean: list[pl.Series] = [\n","            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n","        ]\n","\n","        return expr_mean\n","\n","    @staticmethod\n","    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating mode values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for mode values.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n","\n","        expr_mode: list[pl.Series] = [\n","            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n","        ]\n","\n","        return expr_mode\n","\n","    @staticmethod\n","    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Combines expressions for maximum, mean, and variance calculations.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of combined expressions.\n","        \"\"\"\n","        exprs = (\n","            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n","        )\n","\n","        return exprs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.939396Z","iopub.status.busy":"2024-05-15T00:26:47.938388Z","iopub.status.idle":"2024-05-15T00:26:47.952385Z","shell.execute_reply":"2024-05-15T00:26:47.951556Z","shell.execute_reply.started":"2024-05-15T00:26:47.939363Z"},"trusted":true},"outputs":[],"source":["class SchemaGen:\n","    @staticmethod\n","    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n","        \"\"\"\n","        Changes the data types of columns in the DataFrame.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - pl.LazyFrame: LazyFrame with modified data types.\n","        \"\"\"\n","        for col in df.columns:\n","            if col == \"case_id\":\n","                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n","            elif col == \"date_decision\" or col[-1] == \"D\":\n","                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n","            elif col[-1] in [\"P\", \"A\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","        return df\n","\n","    @staticmethod\n","    def scan_files(glob_path: str, depth: int = None):\n","        chunks = []\n","        for path in glob(str(glob_path)):\n","            df = pl.read_parquet(path, low_memory=True, rechunk=True)\n","            df = df.pipe(SchemaGen.change_dtypes)\n","            if depth in [1, 2]:\n","                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","            chunks.append(df)\n","        df = pl.concat(chunks, how=\"vertical_relaxed\")\n","        del chunks\n","        gc.collect()\n","        df = df.unique(subset=[\"case_id\"])\n","        return df\n","\n","    @staticmethod\n","    def join_dataframes(df_base, depth_0, depth_1, depth_2):\n","        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","        return df_base\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.954021Z","iopub.status.busy":"2024-05-15T00:26:47.953671Z","iopub.status.idle":"2024-05-15T00:26:47.971691Z","shell.execute_reply":"2024-05-15T00:26:47.970809Z","shell.execute_reply.started":"2024-05-15T00:26:47.953992Z"},"trusted":true},"outputs":[],"source":["def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with filtered columns.\n","    \"\"\"\n","    for col in df.columns:\n","        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n","            null_pct = df[col].is_null().mean()\n","\n","            if null_pct > 0.95:\n","                df = df.drop(col)\n","\n","    for col in df.columns:\n","        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n","            df[col].dtype == pl.String\n","        ):\n","            freq = df[col].n_unique()\n","\n","            if (freq > 200) | (freq == 1):\n","                df = df.drop(col)\n","\n","    return df\n","\n","\n","def transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Transforms columns in the DataFrame according to predefined rules.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with transformed columns.\n","    \"\"\"\n","    if \"riskassesment_302T\" in df.columns:\n","        if df[\"riskassesment_302T\"].dtype == pl.Null:\n","            df = df.with_columns(\n","                [\n","                    pl.Series(\n","                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n","                    ),\n","                    pl.Series(\n","                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n","                    ),\n","                ]\n","            )\n","        else:\n","            pct_low: pl.Series = (\n","                df[\"riskassesment_302T\"]\n","                .str.split(\" - \")\n","                .apply(lambda x: x[0].replace(\"%\", \"\"))\n","                .cast(pl.UInt8)\n","            )\n","            pct_high: pl.Series = (\n","                df[\"riskassesment_302T\"]\n","                .str.split(\" - \")\n","                .apply(lambda x: x[1].replace(\"%\", \"\"))\n","                .cast(pl.UInt8)\n","            )\n","\n","            diff: pl.Series = pct_high - pct_low\n","            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n","\n","            del pct_high, pct_low\n","            gc.collect()\n","\n","            df = df.with_columns(\n","                [\n","                    diff.alias(\"riskassesment_302T_rng\"),\n","                    avg.alias(\"riskassesment_302T_mean\"),\n","                ]\n","            )\n","\n","        df.drop(\"riskassesment_302T\")\n","\n","    return df\n","\n","\n","def handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Handles date columns in the DataFrame.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with transformed date columns.\n","    \"\"\"\n","    for col in df.columns:\n","        if col.endswith(\"D\"):\n","            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n","\n","    df = df.rename(\n","        {\n","            \"MONTH\": \"month\",\n","            \"WEEK_NUM\": \"week_num\"\n","        }\n","    )\n","            \n","    df = df.with_columns(\n","        [\n","            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n","            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n","        ]\n","    )\n","\n","    return df.drop(\"date_decision\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.975509Z","iopub.status.busy":"2024-05-15T00:26:47.975244Z","iopub.status.idle":"2024-05-15T00:29:29.707398Z","shell.execute_reply":"2024-05-15T00:29:29.706503Z","shell.execute_reply.started":"2024-05-15T00:26:47.975487Z"},"trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m data_store: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_base\u001b[39m\u001b[38;5;124m\"\u001b[39m: SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_base.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_0\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_static_cb_0.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_static_0_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      8\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_applprev_1_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      9\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_tax_registry_a_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     10\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_tax_registry_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_tax_registry_c_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m---> 12\u001b[0m         \u001b[43mSchemaGen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_credit_bureau_a_1_*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     13\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_credit_bureau_b_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     14\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_other_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     15\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_person_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     16\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_deposit_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     17\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_debitcard_1.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth_2\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     20\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_credit_bureau_a_2_*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     21\u001b[0m         SchemaGen\u001b[38;5;241m.\u001b[39mscan_files(TRAIN_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_credit_bureau_b_2.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m     ],\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m df_train: pl\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     26\u001b[0m     SchemaGen\u001b[38;5;241m.\u001b[39mjoin_dataframes(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_store)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(filter_cols)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;241m.\u001b[39mpipe(Utility\u001b[38;5;241m.\u001b[39mreduce_memory_usage, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m data_store\n","Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mSchemaGen.scan_files\u001b[0;34m(glob_path, depth)\u001b[0m\n\u001b[1;32m     31\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mpipe(SchemaGen\u001b[38;5;241m.\u001b[39mchange_dtypes)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m---> 33\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_by\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcase_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAggregator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exprs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(chunks, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/polars/dataframe/group_by.py:250\u001b[0m, in \u001b[0;36mGroupBy.agg\u001b[0;34m(self, *aggs, **named_aggs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg\u001b[39m(\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;241m*\u001b[39maggs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_aggs: IntoExpr,\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    Compute aggregations for each group of a group by operation.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    └─────┴───────┴────────────────┘\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_by\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaintain_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaintain_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_aggs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_optimization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     )\n","File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py:1934\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\u001b[0m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m background:\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InProcessQuery(ldf\u001b[38;5;241m.\u001b[39mcollect_concurrently())\n\u001b[0;32m-> 1934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(ldf\u001b[38;5;241m.\u001b[39mcollect())\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["data_store: dict = {\n","    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n","    \"depth_0\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","    ],\n","}\n","\n","df_train: pl.DataFrame = (\n","    SchemaGen.join_dataframes(**data_store)\n","    .pipe(filter_cols)\n","    .pipe(transform_cols)\n","    .pipe(handle_dates)\n","    .pipe(Utility.reduce_memory_usage, \"df_train\")\n",")\n","\n","del data_store\n","gc.collect()\n","\n","print(f\"Train data shape: {df_train.shape}\")\n","display(df_train.head(10))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nmaxDates = (creditA.lazy()\\n    .select([pl.col(\"case_id\"), pl.col(\"refreshdate_3813885D\").str.to_datetime()])\\n    .group_by(\"case_id\")\\n    .max()\\n    ).collect().to_pandas().set_index(\"case_id\").refreshdate_3813885D.sort_index()\\n\\npredictDates = maxDates - pd.to_timedelta(str(14) + \" days\")\\ntesting = pd.concat([predictDates, testing], axis=1).sort_index()\\n\\ndateMin = testing.refreshdate_3813885D.min()\\ndateMax = testing.refreshdate_3813885D.max()\\nfirstMonday = testing.refreshdate_3813885D[testing.refreshdate_3813885D.dt.day_of_week==1].min()\\n\\ndayBetween = (dateMax - firstMonday).days\\ndayRange = [firstMonday + pd.to_timedelta(str(i) + \" days\") for i in range(0, dayBetween, 7)]\\ntesting[\"WEEK_NUM\"] = 0\\n\\nfor i in range(-1, len(dayRange)):\\n    if i < 0 and testing.refreshdate_3813885D.min() != firstMonday:\\n        testing[\"WEEK_NUM\"] += ~testing.refreshdate_3813885D.isna()\\n        continue\\n\\n\\n    testing[\"WEEK_NUM\"] += testing.refreshdate_3813885D >= dayRange[i]\\n\\n# クレジットデータの読み込みと処理\\nmaxDates = (creditA.lazy()\\n    .select([pl.col(\"case_id\"), pl.col(\"refreshdate_3813885D\").str.to_datetime()])\\n    .group_by(\"case_id\")\\n    .max()\\n    ).collect().to_pandas().set_index(\"case_id\").refreshdate_3813885D.sort_index()\\n\\npredictDates = maxDates - pd.to_timedelta(str(14) + \" days\")\\ntesting = pd.concat([predictDates, testing], axis=1).sort_index()\\n\\ndateMin = testing.refreshdate_3813885D.min()\\ndateMax = testing.refreshdate_3813885D.max()\\nfirstMonday = testing.refreshdate_3813885D[testing.refreshdate_3813885D.dt.day_of_week==1].min()\\n\\n\\n\\n# 最初の月曜日から最大日付までの範囲で、週ごとの日付リストを作成\\ndayBetween = (dateMax - firstMonday).days\\ndayRange = [firstMonday + pd.to_timedelta(str(i) + \" days\") for i in range(0, dayBetween, 7)]\\n\\n# WEEK_NUM列の初期化と計算\\ntesting[\"WEEK_NUM\"] = 0\\nfor i in range(-1, len(dayRange)):\\n    if i < 0 and testing.refreshdate_3813885D.min() != firstMonday:\\n        testing[\"WEEK_NUM\"] += ~testing.refreshdate_3813885D.isna()\\n        continue\\n    testing[\"WEEK_NUM\"] += testing.refreshdate_3813885D >= dayRange[i]\\n\\n# case_idがインデックスとして設定されている場合、リセットする\\nif \\'case_id\\' not in testing.columns:\\n    testing = testing.reset_index()\\n\\nif \\'case_id\\' not in df_train.columns:\\n    df_train = df_train.reset_index()\\n\\ndf_train = df_train.to_pandas()\\n\\n# データフレームをcase_idでマージ\\nmerged_df = testing[[\\'case_id\\', \\'WEEK_NUM\\']].merge(df_train[[\\'case_id\\', \\'week_num\\']], on=\\'case_id\\')\\n\\n# WEEK_NUM列の比較\\nmerged_df[\\'WEEK_NUM_match\\'] = merged_df[\\'WEEK_NUM\\'] == merged_df[\\'week_num\\']\\n\\n# 結果の表示\\ndisplay(merged_df)\\n\\n# 比較結果の集計や可視化\\nnum_matches = merged_df[\\'WEEK_NUM_match\\'].sum()\\ntotal_cases = len(merged_df)\\nmatch_percentage = num_matches / total_cases * 100\\n\\nprint(f\\'Matching WEEK_NUM cases: {num_matches} / {total_cases} ({match_percentage:.2f}%)\\')\\n'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import glob as glob_module\n","import polars as pl\n","\n","def readFiles(path_pattern: str) -> pl.DataFrame:\n","    \"\"\"\n","    Reads multiple Parquet files matching the given path pattern and combines them into a single DataFrame.\n","\n","    Args:\n","    - path_pattern (str): The file path pattern to match (e.g., \"/path/to/files/*.parquet\").\n","\n","    Returns:\n","    - pl.DataFrame: Combined DataFrame from all matching files.\n","    \"\"\"\n","    # List of all matching file paths\n","    file_paths = glob_module.glob(path_pattern)\n","    \n","    # Initialize an empty list to hold DataFrames\n","    df_list = []\n","    \n","    # Iterate over each file path and read the Parquet file\n","    for file_path in file_paths:\n","        df = pl.read_parquet(file_path)\n","        df_list.append(df)\n","    \n","    # Standardize column data types across all DataFrames\n","    standardized_df_list = []\n","    for df in df_list:\n","        # Ensure all columns have consistent types\n","        for col in df.columns:\n","            if df[col].dtype == pl.Null:\n","                df = df.with_columns(pl.lit(None).cast(pl.Utf8).alias(col))  # Convert Null columns to String type\n","            elif df[col].dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64]:\n","                df = df.with_columns(df[col].cast(pl.Float64))  # Convert integer columns to Float64\n","            elif df[col].dtype == pl.Boolean:\n","                df = df.with_columns(df[col].cast(pl.Utf8))  # Convert boolean columns to String type\n","            elif df[col].dtype != pl.Utf8 and df[col].dtype != pl.Float64:\n","                df = df.with_columns(df[col].cast(pl.Utf8))  # Convert other types to String type for consistency\n","        standardized_df_list.append(df)\n","\n","    # Concatenate all standardized DataFrames into a single DataFrame\n","    combined_df = pl.concat(standardized_df_list)\n","    \n","    return combined_df\n","# 使用例\n","\n","testing = pd.read_parquet(\"/Users/i.itsuki/kaggle_HomeCredit/parquet_files/train/train_base.parquet\").set_index(\"case_id\")\n","#testing = testing.drop(['date_decision', 'WEEK_NUM', 'MONTH'], axis=1)\n","\n","creditA = readFiles(\"/Users/i.itsuki/kaggle_HomeCredit/parquet_files/train/train_credit_bureau_a_1_*.parquet\")\n","\n","\"\"\"\n","maxDates = (creditA.lazy()\n","    .select([pl.col(\"case_id\"), pl.col(\"refreshdate_3813885D\").str.to_datetime()])\n","    .group_by(\"case_id\")\n","    .max()\n","    ).collect().to_pandas().set_index(\"case_id\").refreshdate_3813885D.sort_index()\n","\n","predictDates = maxDates - pd.to_timedelta(str(14) + \" days\")\n","testing = pd.concat([predictDates, testing], axis=1).sort_index()\n","\n","dateMin = testing.refreshdate_3813885D.min()\n","dateMax = testing.refreshdate_3813885D.max()\n","firstMonday = testing.refreshdate_3813885D[testing.refreshdate_3813885D.dt.day_of_week==1].min()\n","\n","dayBetween = (dateMax - firstMonday).days\n","dayRange = [firstMonday + pd.to_timedelta(str(i) + \" days\") for i in range(0, dayBetween, 7)]\n","testing[\"WEEK_NUM\"] = 0\n","\n","for i in range(-1, len(dayRange)):\n","    if i < 0 and testing.refreshdate_3813885D.min() != firstMonday:\n","        testing[\"WEEK_NUM\"] += ~testing.refreshdate_3813885D.isna()\n","        continue\n","\n","\n","    testing[\"WEEK_NUM\"] += testing.refreshdate_3813885D >= dayRange[i]\n","\n","# クレジットデータの読み込みと処理\n","maxDates = (creditA.lazy()\n","    .select([pl.col(\"case_id\"), pl.col(\"refreshdate_3813885D\").str.to_datetime()])\n","    .group_by(\"case_id\")\n","    .max()\n","    ).collect().to_pandas().set_index(\"case_id\").refreshdate_3813885D.sort_index()\n","\n","predictDates = maxDates - pd.to_timedelta(str(14) + \" days\")\n","testing = pd.concat([predictDates, testing], axis=1).sort_index()\n","\n","dateMin = testing.refreshdate_3813885D.min()\n","dateMax = testing.refreshdate_3813885D.max()\n","firstMonday = testing.refreshdate_3813885D[testing.refreshdate_3813885D.dt.day_of_week==1].min()\n","\n","\n","\n","# 最初の月曜日から最大日付までの範囲で、週ごとの日付リストを作成\n","dayBetween = (dateMax - firstMonday).days\n","dayRange = [firstMonday + pd.to_timedelta(str(i) + \" days\") for i in range(0, dayBetween, 7)]\n","\n","# WEEK_NUM列の初期化と計算\n","testing[\"WEEK_NUM\"] = 0\n","for i in range(-1, len(dayRange)):\n","    if i < 0 and testing.refreshdate_3813885D.min() != firstMonday:\n","        testing[\"WEEK_NUM\"] += ~testing.refreshdate_3813885D.isna()\n","        continue\n","    testing[\"WEEK_NUM\"] += testing.refreshdate_3813885D >= dayRange[i]\n","\n","# case_idがインデックスとして設定されている場合、リセットする\n","if 'case_id' not in testing.columns:\n","    testing = testing.reset_index()\n","\n","if 'case_id' not in df_train.columns:\n","    df_train = df_train.reset_index()\n","\n","df_train = df_train.to_pandas()\n","\n","# データフレームをcase_idでマージ\n","merged_df = testing[['case_id', 'WEEK_NUM']].merge(df_train[['case_id', 'week_num']], on='case_id')\n","\n","# WEEK_NUM列の比較\n","merged_df['WEEK_NUM_match'] = merged_df['WEEK_NUM'] == merged_df['week_num']\n","\n","# 結果の表示\n","display(merged_df)\n","\n","# 比較結果の集計や可視化\n","num_matches = merged_df['WEEK_NUM_match'].sum()\n","total_cases = len(merged_df)\n","match_percentage = num_matches / total_cases * 100\n","\n","print(f'Matching WEEK_NUM cases: {num_matches} / {total_cases} ({match_percentage:.2f}%)')\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['level_0', 'index', 'case_id', 'date_decision', 'MONTH', 'WEEK_NUM',\n","       'target', 'refreshdate_3813885D', 'day_of_week'],\n","      dtype='object')"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WEEK_NUM</th>\n","      <th>refreshdate_3813885D</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>NaT</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1526654</th>\n","      <td>36.0</td>\n","      <td>2019-09-08</td>\n","    </tr>\n","    <tr>\n","      <th>1526655</th>\n","      <td>36.0</td>\n","      <td>2019-09-08</td>\n","    </tr>\n","    <tr>\n","      <th>1526656</th>\n","      <td>36.0</td>\n","      <td>2019-09-08</td>\n","    </tr>\n","    <tr>\n","      <th>1526657</th>\n","      <td>36.0</td>\n","      <td>2019-09-08</td>\n","    </tr>\n","    <tr>\n","      <th>1526658</th>\n","      <td>36.0</td>\n","      <td>2019-09-08</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1526659 rows × 2 columns</p>\n","</div>"],"text/plain":["         WEEK_NUM refreshdate_3813885D\n","0             NaN                  NaT\n","1             NaN                  NaT\n","2             NaN                  NaT\n","3             NaN                  NaT\n","4             NaN                  NaT\n","...           ...                  ...\n","1526654      36.0           2019-09-08\n","1526655      36.0           2019-09-08\n","1526656      36.0           2019-09-08\n","1526657      36.0           2019-09-08\n","1526658      36.0           2019-09-08\n","\n","[1526659 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Matching WEEK_NUM cases: 3741 / 1526659 (0.25%)\n"]}],"source":["# クレジットデータの読み込みと処理\n","maxDates = (creditA.lazy()\n","    .select([pl.col(\"case_id\"), pl.col(\"refreshdate_3813885D\").str.to_datetime()])\n","    .group_by(\"case_id\")\n","    .max()\n","    ).collect().to_pandas().set_index(\"case_id\").refreshdate_3813885D.sort_index()\n","\n","# 予測日の計算\n","predictDates = maxDates - pd.to_timedelta(str(14) + \" days\")\n","predictDates = predictDates.rename(\"refreshdate_3813885D\")  # 列名を変更\n","\n","# `testing`データフレームに予測日付を追加\n","testing['refreshdate_3813885D'] = predictDates\n","testing = testing.reset_index().sort_index()\n","\n","# refreshdate_3813885D列が正しく設定されているか確認\n","display(testing.columns)\n","\n","# `refreshdate_3813885D`列の日付から曜日を取得\n","testing['day_of_week'] = testing['refreshdate_3813885D'].dt.dayofweek\n","\n","# 最初の月曜日を基準にして`week_num`を計算\n","# 月曜日を0、日曜日を6とする\n","first_monday = testing[testing['day_of_week'] == 0]['refreshdate_3813885D'].min()\n","testing['WEEK_NUM'] = ((testing['refreshdate_3813885D'] - first_monday).dt.days // 7) + 1\n","\n","# df_trainをPandas DataFrameに変換（必要な場合）\n","#df_train = df_train.to_pandas()\n","\n","# case_idがインデックスとして設定されている場合、リセットする\n","if 'case_id' not in df_train.columns:\n","    df_train = df_train.reset_index()\n","\n","# データフレームをcase_idでマージ\n","merged_df = testing[['case_id', 'WEEK_NUM']].merge(df_train[['case_id', 'week_num']], on='case_id')\n","\n","# WEEK_NUM列の比較\n","merged_df['WEEK_NUM_match'] = merged_df['WEEK_NUM'] == merged_df['week_num']\n","\n","\n","# 結果の表示\n","display(testing[['WEEK_NUM','refreshdate_3813885D']])\n","\n","# 比較結果の集計や可視化\n","num_matches = merged_df['WEEK_NUM_match'].sum()\n","total_cases = len(merged_df)\n","match_percentage = num_matches / total_cases * 100\n","\n","print(f'Matching WEEK_NUM cases: {num_matches} / {total_cases} ({match_percentage:.2f}%)')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["merged_df[['week_num','refreshdate_3813885D']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testing['refreshdate_3813885D'].isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["merged_df['WEEK_NUM_match'] = merged_df['WEEK_NUM'] == merged_df['week_num']\n","merged_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["non_matching_cases = merged_df[merged_df['WEEK_NUM_match'] == False]\n","print(\"\\nNon-matching cases:\")\n","print(non_matching_cases.head(10))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:29.709140Z","iopub.status.busy":"2024-05-15T00:29:29.708842Z","iopub.status.idle":"2024-05-15T00:29:31.789671Z","shell.execute_reply":"2024-05-15T00:29:31.788744Z","shell.execute_reply.started":"2024-05-15T00:29:29.709116Z"},"trusted":true},"outputs":[],"source":["data_store: dict = {\n","    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n","    \"depth_0\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","    ],\n","}\n","\n","df_test: pl.DataFrame = (\n","    SchemaGen.join_dataframes(**data_store)\n","    .pipe(transform_cols)\n","    .pipe(handle_dates)\n","    .select([col for col in df_train.columns if col != \"target\"])\n","    .pipe(Utility.reduce_memory_usage, \"df_test\")\n",")\n","\n","del data_store\n","gc.collect()\n","\n","print(f\"Test data shape: {df_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_test['week_num']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train['week_num'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testing.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_train[['case_id','week_num']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["merged_df = testing[['case_id', 'WEEK_NUM']].merge(df_train[['case_id', 'week_num']], on='case_id', suffixes=('_testing', '_train'))\n","display(merged_df.head())\n","merged_df['WEEK_NUM_match'] = merged_df['WEEK_NUM_testing'] == merged_df['WEEK_NUM_train']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_matches = merged_df['WEEK_NUM_match'].sum()\n","total_cases = len(merged_df)\n","match_percentage = num_matches / total_cases * 100\n","\n","print(f'Matching WEEK_NUM cases: {num_matches} / {total_cases} ({match_percentage:.2f}%)')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:31.791148Z","iopub.status.busy":"2024-05-15T00:29:31.790868Z","iopub.status.idle":"2024-05-15T00:29:53.959324Z","shell.execute_reply":"2024-05-15T00:29:53.958327Z","shell.execute_reply.started":"2024-05-15T00:29:31.791109Z"},"trusted":true},"outputs":[],"source":["df_train, cat_cols = Utility.to_pandas(df_train)\n","df_test, cat_cols = Utility.to_pandas(df_test, cat_cols)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.962394Z","iopub.status.busy":"2024-05-15T00:29:53.962109Z","iopub.status.idle":"2024-05-15T00:29:53.971446Z","shell.execute_reply":"2024-05-15T00:29:53.970438Z","shell.execute_reply.started":"2024-05-15T00:29:53.962370Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, ClassifierMixin):\n","    \"\"\"\n","    A voting ensemble model that combines predictions from multiple estimators.\n","\n","    Parameters:\n","    - estimators (list): List of base estimators.\n","\n","    Attributes:\n","    - estimators (list): List of base estimators.\n","\n","    Methods:\n","    - fit(X, y=None): Fit the model to the training data.\n","    - predict(X): Predict class labels for samples.\n","    - predict_proba(X): Predict class probabilities for samples.\n","    \"\"\"\n","\n","    def __init__(self, estimators: list[BaseEstimator]):\n","        \"\"\"\n","        Initialize the VotingModel with a list of base estimators.\n","\n","        Args:\n","        - estimators (list): List of base estimators.\n","        \"\"\"\n","        super().__init__()\n","        self.estimators = estimators\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fit the model to the training data.\n","\n","        Args:\n","        - X: Input features.\n","        - y: Target labels (ignored).\n","\n","        Returns:\n","        - self: Returns the instance itself.\n","        \"\"\"\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict class labels for samples.\n","\n","        Args:\n","        - X: Input features.\n","\n","        Returns:\n","        - numpy.ndarray: Predicted class labels.\n","        \"\"\"\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predict class probabilities for samples.\n","\n","        Args:\n","        - X: Input features.\n","\n","        Returns:\n","        - numpy.ndarray: Predicted class probabilities.\n","        \"\"\"\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.973697Z","iopub.status.busy":"2024-05-15T00:29:53.972916Z","iopub.status.idle":"2024-05-15T00:29:53.994382Z","shell.execute_reply":"2024-05-15T00:29:53.993352Z","shell.execute_reply.started":"2024-05-15T00:29:53.973670Z"},"trusted":true},"outputs":[],"source":["df_subm: pd.DataFrame = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")\n","\n","device: str = \"gpu\"\n","est_cnt: int = 6000\n","\n","DRY_RUN = True if df_subm.shape[0] == 10 else False\n","if DRY_RUN:\n","    device = \"cpu\"\n","    df_train = df_train.iloc[:50000]\n","    est_cnt: int = 600\n","\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.995934Z","iopub.status.busy":"2024-05-15T00:29:53.995650Z","iopub.status.idle":"2024-05-15T00:37:25.759807Z","shell.execute_reply":"2024-05-15T00:37:25.758872Z","shell.execute_reply.started":"2024-05-15T00:29:53.995910Z"},"trusted":true},"outputs":[],"source":["X = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n","y = df_train[\"target\"]\n","\n","weeks = df_train[\"week_num\"]\n","\n","#del df_train\n","gc.collect()\n","\n","cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n","\n","params1 = {\n","    \"boosting_type\": \"gbdt\",\n","    \"colsample_bynode\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"device\": device,\n","    \"extra_trees\": True,\n","    \"learning_rate\": 0.05,\n","    \"l1_regularization\": 0.1,\n","    \"l2_regularization\": 10,\n","    \"max_depth\": 20,\n","    \"metric\": \"auc\",\n","    \"n_estimators\": 2000,\n","    \"num_leaves\": 64,\n","    \"objective\": \"binary\",\n","    \"random_state\": 42,\n","    \"verbose\": -1,\n","}\n","\n","params2 = {\n","    \"boosting_type\": \"gbdt\",\n","    \"colsample_bynode\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"device\": device,\n","    \"extra_trees\": True,\n","    \"learning_rate\": 0.03,\n","    \"l1_regularization\": 0.1,\n","    \"l2_regularization\": 10,\n","    \"max_depth\": 16,\n","    \"metric\": \"auc\",\n","    \"n_estimators\": 2000,\n","    \"num_leaves\": 54,\n","    \"objective\": \"binary\",\n","    \"random_state\": 42,\n","    \"verbose\": -1,\n","}\n","\n","fitted_models_cat = []\n","fitted_models_lgb = []\n","\n","cv_scores_cat = []\n","cv_scores_lgb = []\n","\n","iter_cnt = 0\n","for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n","    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","    '''\n","    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n","    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n","\n","    clf = CatBoostClassifier(\n","        best_model_min_trees = 1000,\n","        boosting_type = \"Plain\",\n","        eval_metric = \"AUC\",\n","        iterations = est_cnt,\n","        learning_rate = 0.05,\n","        l2_leaf_reg = 10,\n","        max_leaves = 64,\n","        random_seed = 42,\n","        task_type = \"GPU\",\n","        use_best_model = True\n","    )\n","\n","    clf.fit(train_pool, eval_set=val_pool, verbose=False)\n","    fitted_models_cat.append(clf)\n","\n","    y_pred_valid = clf.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores_cat.append(auc_score)\n","    '''\n","    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n","    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n","\n","    if iter_cnt % 2 == 0:\n","        model = lgb.LGBMClassifier(**params1)\n","    else:\n","        model = lgb.LGBMClassifier(**params2)\n","\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_valid, y_valid)],\n","        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n","    )\n","    fitted_models_lgb.append(model)\n","\n","    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores_lgb.append(auc_score)\n","\n","    iter_cnt += 1\n","\n","model = VotingModel(fitted_models_cat + fitted_models_lgb)\n","\n","#print(f\"\\nCV AUC scores for CatBoost: {cv_scores_cat}\")\n","#print(f\"Maximum CV AUC score for Catboost: {max(cv_scores_cat)}\", end=\"\\n\\n\")\n","\n","\n","print(f\"CV AUC scores for LGBM: {cv_scores_lgb}\")\n","print(f\"Maximum CV AUC score for LGBM: {max(cv_scores_lgb)}\", end=\"\\n\\n\")\n","\n","del X, y\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.829186\n","[200]\tvalid_0's auc: 0.830062\n","Early stopping, best iteration is:\n","[143]\tvalid_0's auc: 0.832625\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.817886\n","[200]\tvalid_0's auc: 0.824315\n","[300]\tvalid_0's auc: 0.825302\n","Early stopping, best iteration is:\n","[277]\tvalid_0's auc: 0.82588\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.832977\n","[200]\tvalid_0's auc: 0.836605\n","[300]\tvalid_0's auc: 0.838039\n","Early stopping, best iteration is:\n","[288]\tvalid_0's auc: 0.838424\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.798351\n","[200]\tvalid_0's auc: 0.812143\n","[300]\tvalid_0's auc: 0.812793\n","Early stopping, best iteration is:\n","[251]\tvalid_0's auc: 0.81398\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.806045\n","...\n","[152]\tvalid_0's auc: 0.808167\n","CV AUC scores for LGBM: [0.832625157557228, 0.8258797409515442, 0.8384238678191509, 0.8139802713768391, 0.8081673991999244]\n","Maximum CV AUC score for LGBM: 0.8384238678191509"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:37:25.761217Z","iopub.status.busy":"2024-05-15T00:37:25.760942Z","iopub.status.idle":"2024-05-15T00:37:26.293500Z","shell.execute_reply":"2024-05-15T00:37:26.292579Z","shell.execute_reply.started":"2024-05-15T00:37:25.761195Z"},"trusted":true},"outputs":[],"source":["X_test: pd.DataFrame = df_test.drop(columns=[\"week_num\"]).set_index(\"case_id\")\n","\n","X_test[cat_cols] = X_test[cat_cols].astype(\"category\")\n","\n","y_pred: pd.Series = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n","\n","df_subm[\"score\"] = y_pred\n","\n","display(df_subm)\n","\n","df_subm.to_csv(\"submission.csv\")\n","\n","del X_test, y_pred, df_subm\n","gc.collect()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
