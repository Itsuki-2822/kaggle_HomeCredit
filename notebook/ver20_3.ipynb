{"cells":[{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:43.545610Z","iopub.status.busy":"2024-05-15T00:26:43.545138Z","iopub.status.idle":"2024-05-15T00:26:47.864324Z","shell.execute_reply":"2024-05-15T00:26:47.862886Z","shell.execute_reply.started":"2024-05-15T00:26:43.545575Z"},"trusted":true},"outputs":[],"source":["import gc\n","import os\n","import lightgbm as lgb  # type: ignore\n","import numpy as np  # type: ignore\n","import pandas as pd  # type: ignore\n","import polars as pl  # type: ignore\n","import warnings\n","\n","from catboost import CatBoostClassifier, Pool  # type: ignore\n","from glob import glob\n","from IPython.display import display  # type: ignore\n","from pathlib import Path\n","from sklearn.base import BaseEstimator, ClassifierMixin  # type: ignore\n","from sklearn.metrics import roc_auc_score  # type: ignore\n","from sklearn.model_selection import StratifiedGroupKFold  # type: ignore\n","from typing import Any\n","import pytz\n","import datetime\n","import pickle\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n","TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n","TEST_DIR = ROOT / \"parquet_files\" / \"test\""]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["class CFG:\n","    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n","    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n","    \n","    train_data_path = os.path.join(home_directory, 'train/')\n","    test_data_path = os.path.join(home_directory, 'test/')\n","    \n","    OOF_DATA_PATH = Path(home_directory) / 'oof'\n","    MODEL_DATA_PATH = Path(home_directory) / 'models'\n","    SUB_DATA_PATH = Path(home_directory) / 'submission'\n","\n","    def __init__(self):\n","        self.create_directories()\n","    \n","    def create_directories(self):\n","        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n","            path.mkdir(parents=True, exist_ok=True)\n","    \n","    \n","    VER = 203\n","    AUTHOR = 'Mira'\n","    COMPETITION = 'HomeCredit'\n","\n","    seed = 28\n","    n_folds = 5\n","    target_col = 'target'\n","    metric = 'auc'\n","    \n","\n","class is_kaggle:\n","    def __init__(self, Kaggle):\n","        if Kaggle == \"Yes\":\n","            self.path = Path(CFG.kaggle_directory)\n","            CFG.MODEL_DATA_PATH = Path('/kaggle/input/04191103/models')\n","        else:\n","            self.path = Path(CFG.home_directory)\n","            CFG.MODEL_DATA_PATH = Path(CFG.home_directory) / 'models'\n","\n","def create_timestamped_file():\n","    tz_tokyo = pytz.timezone('Asia/Tokyo')\n","    now = datetime.datetime.now(tz=tz_tokyo)\n","    filename = now.strftime('%m%d-%H%M') + '.txt'\n","    full_path = CFG.MODEL_DATA_PATH / filename\n","    full_path.touch()\n","\n","#create_timestamped_file()\n","cfg_instance = CFG()      \n","selector = is_kaggle(\"No\")\n","ROOT = selector.path\n","TRAIN_DIR       = ROOT / \"parquet_files/train\"\n","TEST_DIR        = ROOT / \"parquet_files/test\"\n","SAMPLE_SUB = ROOT / \"sample_submission.csv\""]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.867596Z","iopub.status.busy":"2024-05-15T00:26:47.867113Z","iopub.status.idle":"2024-05-15T00:26:47.908884Z","shell.execute_reply":"2024-05-15T00:26:47.907991Z","shell.execute_reply.started":"2024-05-15T00:26:47.867539Z"},"trusted":true},"outputs":[],"source":["class Utility:\n","    @staticmethod\n","    def get_feat_defs(ending_with: str) -> None:\n","        \"\"\"\n","        Retrieves feature definitions from a CSV file based on the specified ending.\n","\n","        Args:\n","        - ending_with (str): Ending to filter feature definitions.\n","\n","        Returns:\n","        - pl.DataFrame: Filtered feature definitions.\n","        \"\"\"\n","        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n","\n","        filtered_feats: pl.DataFrame = feat_defs.filter(\n","            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n","        )\n","\n","        with pl.Config(fmt_str_lengths=200, tbl_rows=-1):\n","            print(filtered_feats)\n","\n","        filtered_feats = None\n","        feat_defs = None\n","\n","    @staticmethod\n","    def find_index(lst: list[Any], item: Any) -> int | None:\n","        \"\"\"\n","        Finds the index of an item in a list.\n","\n","        Args:\n","        - lst (list): List to search.\n","        - item (Any): Item to find in the list.\n","\n","        Returns:\n","        - int | None: Index of the item if found, otherwise None.\n","        \"\"\"\n","        try:\n","            return lst.index(item)\n","        except ValueError:\n","            return None\n","\n","    @staticmethod\n","    def dtype_to_str(dtype: pl.DataType) -> str:\n","        \"\"\"\n","        Converts Polars data type to string representation.\n","\n","        Args:\n","        - dtype (pl.DataType): Polars data type.\n","\n","        Returns:\n","        - str: String representation of the data type.\n","        \"\"\"\n","        dtype_map = {\n","            pl.Decimal: \"Decimal\",\n","            pl.Float32: \"Float32\",\n","            pl.Float64: \"Float64\",\n","            pl.UInt8: \"UInt8\",\n","            pl.UInt16: \"UInt16\",\n","            pl.UInt32: \"UInt32\",\n","            pl.UInt64: \"UInt64\",\n","            pl.Int8: \"Int8\",\n","            pl.Int16: \"Int16\",\n","            pl.Int32: \"Int32\",\n","            pl.Int64: \"Int64\",\n","            pl.Date: \"Date\",\n","            pl.Datetime: \"Datetime\",\n","            pl.Duration: \"Duration\",\n","            pl.Time: \"Time\",\n","            pl.Array: \"Array\",\n","            pl.List: \"List\",\n","            pl.Struct: \"Struct\",\n","            pl.String: \"String\",\n","            pl.Categorical: \"Categorical\",\n","            pl.Enum: \"Enum\",\n","            pl.Utf8: \"Utf8\",\n","            pl.Binary: \"Binary\",\n","            pl.Boolean: \"Boolean\",\n","            pl.Null: \"Null\",\n","            pl.Object: \"Object\",\n","            pl.Unknown: \"Unknown\",\n","        }\n","\n","        return dtype_map.get(dtype)\n","\n","    @staticmethod\n","    def find_feat_occur(regex_path: str, ending_with: str) -> pl.DataFrame:\n","        \"\"\"\n","        Finds occurrences of features ending with a specific string in Parquet files.\n","\n","        Args:\n","        - regex_path (str): Regular expression to match Parquet file paths.\n","        - ending_with (str): Ending to filter feature names.\n","\n","        Returns:\n","        - pl.DataFrame: DataFrame containing feature definitions, data types, and file locations.\n","        \"\"\"\n","        feat_defs: pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\").filter(\n","            pl.col(\"Variable\").apply(lambda var: var.endswith(ending_with))\n","        )\n","        feat_defs.sort(by=[\"Variable\"])\n","\n","        feats: list[pl.String] = feat_defs[\"Variable\"].to_list()\n","        feats.sort()\n","\n","        occurrences: list[list] = [[set(), set()] for _ in range(feat_defs.height)]\n","\n","        for path in glob(str(regex_path)):\n","            df_schema: dict = pl.read_parquet_schema(path)\n","\n","            for feat, dtype in df_schema.items():\n","                index: int = Utility.find_index(feats, feat)\n","                if index != None:\n","                    occurrences[index][0].add(Utility.dtype_to_str(dtype))\n","                    occurrences[index][1].add(Path(path).stem)\n","\n","        data_types: list[str] = [None] * feat_defs.height\n","        file_locs: list[str] = [None] * feat_defs.height\n","\n","        for i, feat in enumerate(feats):\n","            data_types[i] = list(occurrences[i][0])\n","            file_locs[i] = list(occurrences[i][1])\n","\n","        feat_defs = feat_defs.with_columns(pl.Series(data_types).alias(\"Data_Type(s)\"))\n","        feat_defs = feat_defs.with_columns(pl.Series(file_locs).alias(\"File_Loc(s)\"))\n","\n","        return feat_defs\n","\n","    def reduce_memory_usage(df: pl.DataFrame, name) -> pl.DataFrame:\n","        \"\"\"\n","        Reduces memory usage of a DataFrame by converting column types.\n","\n","        Args:\n","        - df (pl.DataFrame): DataFrame to optimize.\n","        - name (str): Name of the DataFrame.\n","\n","        Returns:\n","        - pl.DataFrame: Optimized DataFrame.\n","        \"\"\"\n","        print(\n","            f\"Memory usage of dataframe \\\"{name}\\\" is {round(df.estimated_size('mb'), 4)} MB.\"\n","        )\n","\n","        int_types = [\n","            pl.Int8,\n","            pl.Int16,\n","            pl.Int32,\n","            pl.Int64,\n","            pl.UInt8,\n","            pl.UInt16,\n","            pl.UInt32,\n","            pl.UInt64,\n","        ]\n","        float_types = [pl.Float32, pl.Float64]\n","\n","        for col in df.columns:\n","            col_type = df[col].dtype\n","            if col_type in int_types + float_types:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","\n","                if c_min is not None and c_max is not None:\n","                    if col_type in int_types:\n","                        if c_min >= 0:\n","                            if (\n","                                c_min >= np.iinfo(np.uint8).min\n","                                and c_max <= np.iinfo(np.uint8).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt8))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint16).min\n","                                and c_max <= np.iinfo(np.uint16).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt16))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint32).min\n","                                and c_max <= np.iinfo(np.uint32).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt32))\n","                            elif (\n","                                c_min >= np.iinfo(np.uint64).min\n","                                and c_max <= np.iinfo(np.uint64).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.UInt64))\n","                        else:\n","                            if (\n","                                c_min >= np.iinfo(np.int8).min\n","                                and c_max <= np.iinfo(np.int8).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int8))\n","                            elif (\n","                                c_min >= np.iinfo(np.int16).min\n","                                and c_max <= np.iinfo(np.int16).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int16))\n","                            elif (\n","                                c_min >= np.iinfo(np.int32).min\n","                                and c_max <= np.iinfo(np.int32).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int32))\n","                            elif (\n","                                c_min >= np.iinfo(np.int64).min\n","                                and c_max <= np.iinfo(np.int64).max\n","                            ):\n","                                df = df.with_columns(df[col].cast(pl.Int64))\n","                    elif col_type in float_types:\n","                        if (\n","                            c_min > np.finfo(np.float32).min\n","                            and c_max < np.finfo(np.float32).max\n","                        ):\n","                            df = df.with_columns(df[col].cast(pl.Float32))\n","\n","        print(\n","            f\"Memory usage of dataframe \\\"{name}\\\" became {round(df.estimated_size('mb'), 4)} MB.\"\n","        )\n","\n","        return df\n","\n","    def to_pandas(df: pl.DataFrame, cat_cols: list[str] = None) -> (pd.DataFrame, list[str]):  # type: ignore\n","        \"\"\"\n","        Converts a Polars DataFrame to a Pandas DataFrame.\n","\n","        Args:\n","        - df (pl.DataFrame): Polars DataFrame to convert.\n","        - cat_cols (list[str]): List of categorical columns. Default is None.\n","\n","        Returns:\n","        - (pd.DataFrame, list[str]): Tuple containing the converted Pandas DataFrame and categorical columns.\n","        \"\"\"\n","        df: pd.DataFrame = df.to_pandas()\n","\n","        if cat_cols is None:\n","            cat_cols = list(df.select_dtypes(\"object\").columns)\n","\n","        df[cat_cols] = df[cat_cols].astype(\"str\")\n","\n","        return df, cat_cols"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.910676Z","iopub.status.busy":"2024-05-15T00:26:47.910274Z","iopub.status.idle":"2024-05-15T00:26:47.918475Z","shell.execute_reply":"2024-05-15T00:26:47.917607Z","shell.execute_reply.started":"2024-05-15T00:26:47.910636Z"},"trusted":true},"outputs":[],"source":["# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"P\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"M\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"A\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"D\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"T\")\n","# feat_defs:pl.DataFrame = Utility.find_feat_occur(TRAIN_DIR / \"train_*.parquet\", \"L\")\n","# feat_defs:pl.DataFrame = pl.read_csv(ROOT / \"feature_definitions.csv\")\n","# with pl.Config(fmt_str_lengths=1000, tbl_rows=-1, tbl_width_chars=180):\n","#     print(feat_defs)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.920983Z","iopub.status.busy":"2024-05-15T00:26:47.920664Z","iopub.status.idle":"2024-05-15T00:26:47.937047Z","shell.execute_reply":"2024-05-15T00:26:47.936162Z","shell.execute_reply.started":"2024-05-15T00:26:47.920959Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    @staticmethod\n","    def max_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating maximum values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for maximum values.\n","        \"\"\"\n","        cols: list[str] = [\n","            col\n","            for col in df.columns\n","            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n","        ]\n","\n","        expr_max: list[pl.Series] = [\n","            pl.col(col).max().alias(f\"max_{col}\") for col in cols\n","        ]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def min_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating minimum values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for minimum values.\n","        \"\"\"\n","        cols: list[str] = [\n","            col\n","            for col in df.columns\n","            if (col[-1] in (\"P\", \"M\", \"A\", \"D\", \"T\", \"L\")) or (\"num_group\" in col)\n","        ]\n","\n","        expr_min: list[pl.Series] = [\n","            pl.col(col).min().alias(f\"min_{col}\") for col in cols\n","        ]\n","\n","        return expr_min\n","\n","    @staticmethod\n","    def mean_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating mean values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for mean values.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n","\n","        expr_mean: list[pl.Series] = [\n","            pl.col(col).mean().alias(f\"mean_{col}\") for col in cols\n","        ]\n","\n","        return expr_mean\n","\n","    @staticmethod\n","    def var_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating variance for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for variance.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith((\"P\", \"A\", \"D\"))]\n","\n","        expr_mean: list[pl.Series] = [\n","            pl.col(col).var().alias(f\"var_{col}\") for col in cols\n","        ]\n","\n","        return expr_mean\n","\n","    @staticmethod\n","    def mode_expr(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Generates expressions for calculating mode values for specific columns.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of expressions for mode values.\n","        \"\"\"\n","        cols: list[str] = [col for col in df.columns if col.endswith(\"M\")]\n","\n","        expr_mode: list[pl.Series] = [\n","            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n","        ]\n","\n","        return expr_mode\n","\n","    @staticmethod\n","    def get_exprs(df: pl.LazyFrame) -> list[pl.Series]:\n","        \"\"\"\n","        Combines expressions for maximum, mean, and variance calculations.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - list[pl.Series]: List of combined expressions.\n","        \"\"\"\n","        exprs = (\n","            Aggregator.max_expr(df) + Aggregator.mean_expr(df) + Aggregator.var_expr(df)\n","        )\n","\n","        return exprs"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.939396Z","iopub.status.busy":"2024-05-15T00:26:47.938388Z","iopub.status.idle":"2024-05-15T00:26:47.952385Z","shell.execute_reply":"2024-05-15T00:26:47.951556Z","shell.execute_reply.started":"2024-05-15T00:26:47.939363Z"},"trusted":true},"outputs":[],"source":["class SchemaGen:\n","    @staticmethod\n","    def change_dtypes(df: pl.LazyFrame) -> pl.LazyFrame:\n","        \"\"\"\n","        Changes the data types of columns in the DataFrame.\n","\n","        Args:\n","        - df (pl.LazyFrame): Input LazyFrame.\n","\n","        Returns:\n","        - pl.LazyFrame: LazyFrame with modified data types.\n","        \"\"\"\n","        for col in df.columns:\n","            if col == \"case_id\":\n","                df = df.with_columns(pl.col(col).cast(pl.UInt32).alias(col))\n","            elif col in [\"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.UInt16).alias(col))\n","            elif col == \"date_decision\" or col[-1] == \"D\":\n","                df = df.with_columns(pl.col(col).cast(pl.Date).alias(col))\n","            elif col[-1] in [\"P\", \"A\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Float64).alias(col))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","        return df\n","\n","    @staticmethod\n","    def scan_files(glob_path: str, depth: int = None):\n","        chunks = []\n","        for path in glob(str(glob_path)):\n","            df = pl.read_parquet(path, low_memory=True, rechunk=True)\n","            df = df.pipe(SchemaGen.change_dtypes)\n","            if depth in [1, 2]:\n","                df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","            chunks.append(df)\n","        df = pl.concat(chunks, how=\"vertical_relaxed\")\n","        del chunks\n","        gc.collect()\n","        df = df.unique(subset=[\"case_id\"])\n","        return df\n","\n","    @staticmethod\n","    def join_dataframes(df_base, depth_0, depth_1, depth_2):\n","        for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","            df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","        return df_base\n"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.954021Z","iopub.status.busy":"2024-05-15T00:26:47.953671Z","iopub.status.idle":"2024-05-15T00:26:47.971691Z","shell.execute_reply":"2024-05-15T00:26:47.970809Z","shell.execute_reply.started":"2024-05-15T00:26:47.953992Z"},"trusted":true},"outputs":[],"source":["def filter_cols(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Filters columns in the DataFrame based on null percentage and unique values for string columns.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with filtered columns.\n","    \"\"\"\n","    for col in df.columns:\n","        if col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]:\n","            null_pct = df[col].is_null().mean()\n","\n","            if null_pct > 0.95:\n","                df = df.drop(col)\n","\n","    for col in df.columns:\n","        if (col not in [\"case_id\", \"year\", \"month\", \"week_num\", \"target\"]) & (\n","            df[col].dtype == pl.String\n","        ):\n","            freq = df[col].n_unique()\n","\n","            if (freq > 200) | (freq == 1):\n","                df = df.drop(col)\n","\n","    return df\n","\n","\n","def transform_cols(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Transforms columns in the DataFrame according to predefined rules.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with transformed columns.\n","    \"\"\"\n","    if \"riskassesment_302T\" in df.columns:\n","        if df[\"riskassesment_302T\"].dtype == pl.Null:\n","            df = df.with_columns(\n","                [\n","                    pl.Series(\n","                        \"riskassesment_302T_rng\", df[\"riskassesment_302T\"], pl.UInt8\n","                    ),\n","                    pl.Series(\n","                        \"riskassesment_302T_mean\", df[\"riskassesment_302T\"], pl.UInt8\n","                    ),\n","                ]\n","            )\n","        else:\n","            pct_low: pl.Series = (\n","                df[\"riskassesment_302T\"]\n","                .str.split(\" - \")\n","                .apply(lambda x: x[0].replace(\"%\", \"\"))\n","                .cast(pl.UInt8)\n","            )\n","            pct_high: pl.Series = (\n","                df[\"riskassesment_302T\"]\n","                .str.split(\" - \")\n","                .apply(lambda x: x[1].replace(\"%\", \"\"))\n","                .cast(pl.UInt8)\n","            )\n","\n","            diff: pl.Series = pct_high - pct_low\n","            avg: pl.Series = ((pct_low + pct_high) / 2).cast(pl.Float32)\n","\n","            del pct_high, pct_low\n","            gc.collect()\n","\n","            df = df.with_columns(\n","                [\n","                    diff.alias(\"riskassesment_302T_rng\"),\n","                    avg.alias(\"riskassesment_302T_mean\"),\n","                ]\n","            )\n","\n","        df.drop(\"riskassesment_302T\")\n","\n","    return df\n","\n","\n","def handle_dates(df: pl.DataFrame) -> pl.DataFrame:\n","    \"\"\"\n","    Handles date columns in the DataFrame.\n","\n","    Args:\n","    - df (pl.DataFrame): Input DataFrame.\n","\n","    Returns:\n","    - pl.DataFrame: DataFrame with transformed date columns.\n","    \"\"\"\n","    for col in df.columns:\n","        if col.endswith(\"D\"):\n","            df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","            df = df.with_columns(pl.col(col).dt.total_days().cast(pl.Int32))\n","\n","    df = df.rename(\n","        {\n","            \"MONTH\": \"month\",\n","            \"WEEK_NUM\": \"week_num\"\n","        }\n","    )\n","            \n","    df = df.with_columns(\n","        [\n","            pl.col(\"date_decision\").dt.year().alias(\"year\").cast(pl.Int16),\n","            pl.col(\"date_decision\").dt.day().alias(\"day\").cast(pl.UInt8),\n","        ]\n","    )\n","\n","    return df.drop(\"date_decision\")"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:26:47.975509Z","iopub.status.busy":"2024-05-15T00:26:47.975244Z","iopub.status.idle":"2024-05-15T00:29:29.707398Z","shell.execute_reply":"2024-05-15T00:29:29.706503Z","shell.execute_reply.started":"2024-05-15T00:26:47.975487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory usage of dataframe \"df_train\" is 4711.2195 MB.\n","Memory usage of dataframe \"df_train\" became 2665.6302 MB.\n","Train data shape: (1526659, 472)\n"]},{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (10, 472)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>month</th><th>week_num</th><th>target</th><th>assignmentdate_238D</th><th>assignmentdate_4527235D</th><th>birthdate_574D</th><th>contractssum_5085716L</th><th>dateofbirth_337D</th><th>days120_123L</th><th>days180_256L</th><th>days30_165L</th><th>days360_512L</th><th>days90_310L</th><th>description_5085714M</th><th>education_1103M</th><th>education_88M</th><th>firstquarter_103L</th><th>fourthquarter_440L</th><th>maritalst_385M</th><th>maritalst_893M</th><th>numberofqueries_373L</th><th>pmtaverage_3A</th><th>pmtaverage_4527227A</th><th>pmtcount_4527229L</th><th>pmtcount_693L</th><th>pmtscount_423L</th><th>pmtssum_45A</th><th>requesttype_4525192L</th><th>responsedate_1012D</th><th>responsedate_4527233D</th><th>responsedate_4917613D</th><th>secondquarter_766L</th><th>thirdquarter_1082L</th><th>actualdpdtolerance_344P</th><th>amtinstpaidbefduel24m_4187115A</th><th>annuity_780A</th><th>&hellip;</th><th>mean_mainoccupationinc_384A</th><th>max_amount_416A</th><th>max_num_group1_10</th><th>max_openingdate_313D</th><th>mean_amount_416A</th><th>mean_openingdate_313D</th><th>max_num_group1_11</th><th>max_openingdate_857D</th><th>mean_openingdate_857D</th><th>max_collater_typofvalofguarant_298M</th><th>max_collater_typofvalofguarant_407M</th><th>max_collater_valueofguarantee_1124L</th><th>max_collater_valueofguarantee_876L</th><th>max_collaterals_typeofguarante_359M</th><th>max_collaterals_typeofguarante_669M</th><th>max_num_group1_12</th><th>max_num_group2</th><th>max_pmts_dpd_1073P</th><th>max_pmts_dpd_303P</th><th>max_pmts_month_158T</th><th>max_pmts_month_706T</th><th>max_pmts_overdue_1140A</th><th>max_pmts_overdue_1152A</th><th>max_pmts_year_1139T</th><th>max_pmts_year_507T</th><th>max_subjectroles_name_541M</th><th>max_subjectroles_name_838M</th><th>mean_pmts_dpd_1073P</th><th>mean_pmts_dpd_303P</th><th>mean_pmts_overdue_1140A</th><th>mean_pmts_overdue_1152A</th><th>var_pmts_dpd_1073P</th><th>var_pmts_dpd_303P</th><th>var_pmts_overdue_1140A</th><th>var_pmts_overdue_1152A</th><th>year</th><th>day</th></tr><tr><td>u32</td><td>u32</td><td>u8</td><td>u8</td><td>i16</td><td>u8</td><td>i16</td><td>f32</td><td>i32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>i8</td><td>u8</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>u8</td><td>i16</td><td>f32</td><td>i16</td><td>u8</td><td>i16</td><td>i16</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>u16</td><td>u8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>str</td><td>str</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>u16</td><td>u8</td></tr></thead><tbody><tr><td>884546</td><td>201912</td><td>47</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-11658</td><td>6.0</td><td>7.0</td><td>1.0</td><td>9.0</td><td>4.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>5.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>9.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>5.0</td><td>5.0</td><td>0.0</td><td>null</td><td>5386.600098</td><td>&hellip;</td><td>80000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>4.7145e6</td><td>832945.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>3</td><td>35</td><td>0.0</td><td>277.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>68687.515625</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>29.170214</td><td>0.0</td><td>5719.446777</td><td>0.0</td><td>5179.144531</td><td>0.0</td><td>2.23055552e8</td><td>2019</td><td>2</td></tr><tr><td>979307</td><td>202004</td><td>65</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-16255</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>3026.0</td><td>&hellip;</td><td>54000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>23</td><td>0.0</td><td>null</td><td>12.0</td><td>null</td><td>0.0</td><td>null</td><td>2021.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>0.0</td><td>null</td><td>2020</td><td>2</td></tr><tr><td>1449766</td><td>201907</td><td>28</td><td>0</td><td>null</td><td>null</td><td>-13925</td><td>null</td><td>-13925</td><td>2.0</td><td>3.0</td><td>0.0</td><td>4.0</td><td>2.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>0.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>1345.200073</td><td>null</td><td>14</td><td>null</td><td>null</td><td>3.0</td><td>1.0</td><td>0.0</td><td>47449.199219</td><td>2712.199951</td><td>&hellip;</td><td>40000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>3</td><td>35</td><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>0.0</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2019</td><td>17</td></tr><tr><td>687399</td><td>201905</td><td>18</td><td>0</td><td>null</td><td>null</td><td>-9291</td><td>null</td><td>-9291</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2.0</td><td>2301.0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>2.0</td><td>0.0</td><td>null</td><td>null</td><td>2348.0</td><td>&hellip;</td><td>70000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>35</td><td>16.0</td><td>null</td><td>12.0</td><td>null</td><td>740.781982</td><td>null</td><td>2020.0</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.958333</td><td>null</td><td>114.942413</td><td>null</td><td>11.085145</td><td>null</td><td>70024.265625</td><td>null</td><td>2019</td><td>10</td></tr><tr><td>1888868</td><td>202008</td><td>83</td><td>0</td><td>null</td><td>null</td><td>null</td><td>70697.0</td><td>-17753</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;2fc785b2&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>14</td><td>2.0</td><td>2.0</td><td>0.0</td><td>8894.600586</td><td>2845.600098</td><td>&hellip;</td><td>70000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>1</td><td>35</td><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>0.0</td><td>2021.0</td><td>2021.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2020</td><td>9</td></tr><tr><td>942210</td><td>202001</td><td>55</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-21388</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>2.0</td><td>0.0</td><td>null</td><td>null</td><td>3180.800049</td><td>&hellip;</td><td>50000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>0</td><td>35</td><td>0.0</td><td>36.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>3081.600098</td><td>2021.0</td><td>2010.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>1.958333</td><td>0.0</td><td>869.891663</td><td>0.0</td><td>52.824276</td><td>0.0</td><td>958459.1875</td><td>2020</td><td>21</td></tr><tr><td>1592890</td><td>201910</td><td>42</td><td>0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>-18313</td><td>3.0</td><td>4.0</td><td>1.0</td><td>8.0</td><td>3.0</td><td>&quot;a55475b1&quot;</td><td>&quot;6b2ae0fa&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>1.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>8.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>5.0</td><td>5.0</td><td>0.0</td><td>0.0</td><td>1452.200073</td><td>&hellip;</td><td>24000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>1.273125e7</td><td>279460.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>13</td><td>35</td><td>4.0</td><td>69.0</td><td>12.0</td><td>12.0</td><td>7524.936035</td><td>15292.570312</td><td>2020.0</td><td>2020.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;be4fd70b&quot;</td><td>0.068966</td><td>0.753165</td><td>129.74028</td><td>238.620331</td><td>0.275862</td><td>37.970531</td><td>976287.25</td><td>2.2524e6</td><td>2019</td><td>22</td></tr><tr><td>7488</td><td>201903</td><td>9</td><td>0</td><td>-3596</td><td>null</td><td>-25424</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>null</td><td>6987.100098</td><td>null</td><td>null</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>14</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>3201.199951</td><td>&hellip;</td><td>40000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2019</td><td>11</td></tr><tr><td>1804709</td><td>202003</td><td>61</td><td>0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>-22434</td><td>2.0</td><td>4.0</td><td>0.0</td><td>5.0</td><td>2.0</td><td>&quot;a55475b1&quot;</td><td>&quot;717ddd49&quot;</td><td>&quot;a55475b1&quot;</td><td>2.0</td><td>3.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>5.0</td><td>null</td><td>9491.799805</td><td>11.0</td><td>null</td><td>null</td><td>null</td><td>&quot;PENSION_6&quot;</td><td>null</td><td>14</td><td>null</td><td>1.0</td><td>3.0</td><td>0.0</td><td>29245.599609</td><td>5426.600098</td><td>&hellip;</td><td>22000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>6</td><td>35</td><td>1171.0</td><td>33.0</td><td>12.0</td><td>12.0</td><td>134302.234375</td><td>4665.200195</td><td>2021.0</td><td>2021.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;be4fd70b&quot;</td><td>489.760864</td><td>1.392405</td><td>51139.425781</td><td>164.492935</td><td>236240.453125</td><td>34.010712</td><td>2.6879e9</td><td>480705.09375</td><td>2020</td><td>3</td></tr><tr><td>147995</td><td>201908</td><td>30</td><td>0</td><td>null</td><td>null</td><td>-12084</td><td>null</td><td>-12084</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>1.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11.0</td><td>17972.0</td><td>null</td><td>14</td><td>null</td><td>null</td><td>2.0</td><td>1.0</td><td>0.0</td><td>384975.59375</td><td>18689.599609</td><td>&hellip;</td><td>200000.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>0.0</td><td>0.0</td><td>&quot;c7a5ad39&quot;</td><td>&quot;c7a5ad39&quot;</td><td>1</td><td>35</td><td>0.0</td><td>0.0</td><td>12.0</td><td>12.0</td><td>0.0</td><td>0.0</td><td>2020.0</td><td>2018.0</td><td>&quot;ab3c25cf&quot;</td><td>&quot;ab3c25cf&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2019</td><td>1</td></tr></tbody></table></div>"],"text/plain":["shape: (10, 472)\n","┌─────────┬────────┬──────────┬────────┬───┬─────────────────────┬────────────────────┬──────┬─────┐\n","│ case_id ┆ month  ┆ week_num ┆ target ┆ … ┆ var_pmts_overdue_11 ┆ var_pmts_overdue_1 ┆ year ┆ day │\n","│ ---     ┆ ---    ┆ ---      ┆ ---    ┆   ┆ 40A                 ┆ 152A               ┆ ---  ┆ --- │\n","│ u32     ┆ u32    ┆ u8       ┆ u8     ┆   ┆ ---                 ┆ ---                ┆ u16  ┆ u8  │\n","│         ┆        ┆          ┆        ┆   ┆ f32                 ┆ f32                ┆      ┆     │\n","╞═════════╪════════╪══════════╪════════╪═══╪═════════════════════╪════════════════════╪══════╪═════╡\n","│ 884546  ┆ 201912 ┆ 47       ┆ 0      ┆ … ┆ 0.0                 ┆ 2.23055552e8       ┆ 2019 ┆ 2   │\n","│ 979307  ┆ 202004 ┆ 65       ┆ 0      ┆ … ┆ 0.0                 ┆ null               ┆ 2020 ┆ 2   │\n","│ 1449766 ┆ 201907 ┆ 28       ┆ 0      ┆ … ┆ 0.0                 ┆ 0.0                ┆ 2019 ┆ 17  │\n","│ 687399  ┆ 201905 ┆ 18       ┆ 0      ┆ … ┆ 70024.265625        ┆ null               ┆ 2019 ┆ 10  │\n","│ 1888868 ┆ 202008 ┆ 83       ┆ 0      ┆ … ┆ 0.0                 ┆ 0.0                ┆ 2020 ┆ 9   │\n","│ 942210  ┆ 202001 ┆ 55       ┆ 0      ┆ … ┆ 0.0                 ┆ 958459.1875        ┆ 2020 ┆ 21  │\n","│ 1592890 ┆ 201910 ┆ 42       ┆ 0      ┆ … ┆ 976287.25           ┆ 2.2524e6           ┆ 2019 ┆ 22  │\n","│ 7488    ┆ 201903 ┆ 9        ┆ 0      ┆ … ┆ null                ┆ null               ┆ 2019 ┆ 11  │\n","│ 1804709 ┆ 202003 ┆ 61       ┆ 0      ┆ … ┆ 2.6879e9            ┆ 480705.09375       ┆ 2020 ┆ 3   │\n","│ 147995  ┆ 201908 ┆ 30       ┆ 0      ┆ … ┆ 0.0                 ┆ 0.0                ┆ 2019 ┆ 1   │\n","└─────────┴────────┴──────────┴────────┴───┴─────────────────────┴────────────────────┴──────┴─────┘"]},"metadata":{},"output_type":"display_data"}],"source":["data_store: dict = {\n","    \"df_base\": SchemaGen.scan_files(TRAIN_DIR / \"train_base.parquet\"),\n","    \"depth_0\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","        SchemaGen.scan_files(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","    ],\n","}\n","\n","df_train: pl.DataFrame = (\n","    SchemaGen.join_dataframes(**data_store)\n","    .pipe(filter_cols)\n","    .pipe(transform_cols)\n","    .pipe(handle_dates)\n","    .pipe(Utility.reduce_memory_usage, \"df_train\")\n",")\n","\n","del data_store\n","gc.collect()\n","\n","print(f\"Train data shape: {df_train.shape}\")\n","display(df_train.head(10))"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:29.709140Z","iopub.status.busy":"2024-05-15T00:29:29.708842Z","iopub.status.idle":"2024-05-15T00:29:31.789671Z","shell.execute_reply":"2024-05-15T00:29:31.788744Z","shell.execute_reply.started":"2024-05-15T00:29:29.709116Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory usage of dataframe \"df_test\" is 0.0295 MB.\n","Memory usage of dataframe \"df_test\" became 0.0219 MB.\n","Test data shape: (10, 471)\n"]}],"source":["data_store: dict = {\n","    \"df_base\": SchemaGen.scan_files(TEST_DIR / \"test_base.parquet\"),\n","    \"depth_0\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        SchemaGen.scan_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_other_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_person_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        SchemaGen.scan_files(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","        SchemaGen.scan_files(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","    ],\n","}\n","\n","df_test: pl.DataFrame = (\n","    SchemaGen.join_dataframes(**data_store)\n","    .pipe(transform_cols)\n","    .pipe(handle_dates)\n","    .select([col for col in df_train.columns if col != \"target\"])\n","    .pipe(Utility.reduce_memory_usage, \"df_test\")\n",")\n","\n","del data_store\n","gc.collect()\n","\n","print(f\"Test data shape: {df_test.shape}\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:31.791148Z","iopub.status.busy":"2024-05-15T00:29:31.790868Z","iopub.status.idle":"2024-05-15T00:29:53.959324Z","shell.execute_reply":"2024-05-15T00:29:53.958327Z","shell.execute_reply.started":"2024-05-15T00:29:31.791109Z"},"trusted":true},"outputs":[],"source":["df_train, cat_cols = Utility.to_pandas(df_train)\n","df_test, cat_cols = Utility.to_pandas(df_test, cat_cols)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.962394Z","iopub.status.busy":"2024-05-15T00:29:53.962109Z","iopub.status.idle":"2024-05-15T00:29:53.971446Z","shell.execute_reply":"2024-05-15T00:29:53.970438Z","shell.execute_reply.started":"2024-05-15T00:29:53.962370Z"},"trusted":true},"outputs":[],"source":["class VotingModel(BaseEstimator, ClassifierMixin):\n","    \"\"\"\n","    A voting ensemble model that combines predictions from multiple estimators.\n","\n","    Parameters:\n","    - estimators (list): List of base estimators.\n","\n","    Attributes:\n","    - estimators (list): List of base estimators.\n","\n","    Methods:\n","    - fit(X, y=None): Fit the model to the training data.\n","    - predict(X): Predict class labels for samples.\n","    - predict_proba(X): Predict class probabilities for samples.\n","    \"\"\"\n","\n","    def __init__(self, estimators: list[BaseEstimator]):\n","        \"\"\"\n","        Initialize the VotingModel with a list of base estimators.\n","\n","        Args:\n","        - estimators (list): List of base estimators.\n","        \"\"\"\n","        super().__init__()\n","        self.estimators = estimators\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fit the model to the training data.\n","\n","        Args:\n","        - X: Input features.\n","        - y: Target labels (ignored).\n","\n","        Returns:\n","        - self: Returns the instance itself.\n","        \"\"\"\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict class labels for samples.\n","\n","        Args:\n","        - X: Input features.\n","\n","        Returns:\n","        - numpy.ndarray: Predicted class labels.\n","        \"\"\"\n","        y_preds = [estimator.predict(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predict class probabilities for samples.\n","\n","        Args:\n","        - X: Input features.\n","\n","        Returns:\n","        - numpy.ndarray: Predicted class probabilities.\n","        \"\"\"\n","        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n","        return np.mean(y_preds, axis=0)"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.973697Z","iopub.status.busy":"2024-05-15T00:29:53.972916Z","iopub.status.idle":"2024-05-15T00:29:53.994382Z","shell.execute_reply":"2024-05-15T00:29:53.993352Z","shell.execute_reply.started":"2024-05-15T00:29:53.973670Z"},"trusted":true},"outputs":[],"source":["df_subm: pd.DataFrame = pd.read_csv(ROOT / \"sample_submission.csv\")\n","df_subm = df_subm.set_index(\"case_id\")"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def Learning_lgb(X_train, y_train, X_valid, y_valid, params, version):\n","    model = lgb.LGBMClassifier(**params)\n","    model.fit(\n","        X_train,\n","        y_train,\n","        eval_set=[(X_valid, y_valid)],\n","        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)],\n","    )\n","    pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'lgb_ver{CFG.VER}_{version}.pkl', 'wb'))\n","    return model\n","\n","def Learning_cat(X_train,y_train,X_valid,y_valid):\n","    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n","    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n","\n","    model = CatBoostClassifier(\n","        best_model_min_trees = 1000,\n","        boosting_type = \"Plain\",\n","        eval_metric = \"AUC\",\n","        iterations = 6000,\n","        learning_rate = 0.05,\n","        l2_leaf_reg = 10,\n","        max_leaves = 64,\n","        random_seed = 42,\n","        task_type = \"CPU\",\n","        use_best_model = True\n","    )\n","\n","    model.fit(train_pool, eval_set=val_pool, verbose=False)\n","    pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'cat_ver{CFG.VER}.pkl', 'wb'))\n","    return model"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:29:53.995934Z","iopub.status.busy":"2024-05-15T00:29:53.995650Z","iopub.status.idle":"2024-05-15T00:37:25.759807Z","shell.execute_reply":"2024-05-15T00:37:25.758872Z","shell.execute_reply.started":"2024-05-15T00:29:53.995910Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nX = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\\ny = df_train[\"target\"]\\n\\nweeks = df_train[\"week_num\"]\\n\\ndel df_train\\ngc.collect()\\n\\ncv = StratifiedGroupKFold(n_splits=5, shuffle=False)\\n\\nlgb_params1 = {\\n    \"boosting_type\": \"gbdt\",\\n    \"colsample_bynode\": 0.8,\\n    \"colsample_bytree\": 0.8,\\n    \"extra_trees\": True,\\n    \"learning_rate\": 0.05,\\n    \"l1_regularization\": 0.1,\\n    \"l2_regularization\": 10,\\n    \"max_depth\": 20,\\n    \"metric\": \"auc\",\\n    \"n_estimators\": 2000,\\n    \"num_leaves\": 64,\\n    \"objective\": \"binary\",\\n    \"random_state\": 42,\\n    \"verbose\": -1,\\n}\\n\\nlgb_params2 = {\\n    \"boosting_type\": \"gbdt\",\\n    \"colsample_bynode\": 0.8,\\n    \"colsample_bytree\": 0.8,\\n    \"extra_trees\": True,\\n    \"learning_rate\": 0.03,\\n    \"l1_regularization\": 0.1,\\n    \"l2_regularization\": 10,\\n    \"max_depth\": 16,\\n    \"metric\": \"auc\",\\n    \"n_estimators\": 2000,\\n    \"num_leaves\": 54,\\n    \"objective\": \"binary\",\\n    \"random_state\": 42,\\n    \"verbose\": -1,\\n}\\n\\n\\nfitted_models_cat = []\\nfitted_models_lgb = []\\n\\ncv_scores_cat = []\\ncv_scores_lgb = []\\n\\niter_cnt = 0\\nfor idx_train, idx_valid in cv.split(X, y, groups=weeks):\\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\\n    \\n    model = Learning_cat(X_train,y_train,X_valid,y_valid)\\n    fitted_models_cat.append(model)\\n\\n    y_pred_valid = model.predict_proba(X_valid)[:, 1]\\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\\n    cv_scores_cat.append(auc_score)\\n\\n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\\n\\n    if iter_cnt % 2 == 0:\\n        version = f\\'ver1_iter{iter_cnt}\\'\\n        model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params1, version)\\n    else:\\n        version = f\\'ver2_iter{iter_cnt}\\'\\n        model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params2, version)\\n    \\n    fitted_models_lgb.append(model)\\n    y_pred_valid = model.predict_proba(X_valid)[:, 1]\\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\\n    cv_scores_lgb.append(auc_score)\\n    \\n    iter_cnt += 1\\n\\nmodel = VotingModel(fitted_models_cat + fitted_models_lgb)\\n\\nprint(f\"\\nCV AUC scores for CatBoost: {cv_scores_cat}\")\\nprint(f\"Maximum CV AUC score for Catboost: {max(cv_scores_cat)}\", end=\"\\n\\n\")\\n\\n\\nprint(f\"CV AUC scores for LGBM: {cv_scores_lgb}\")\\nprint(f\"Maximum CV AUC score for LGBM: {max(cv_scores_lgb)}\", end=\"\\n\\n\")\\n\\ndel X, y\\ngc.collect()\\n'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","X = df_train.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n","y = df_train[\"target\"]\n","\n","weeks = df_train[\"week_num\"]\n","\n","del df_train\n","gc.collect()\n","\n","cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n","\n","lgb_params1 = {\n","    \"boosting_type\": \"gbdt\",\n","    \"colsample_bynode\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"extra_trees\": True,\n","    \"learning_rate\": 0.05,\n","    \"l1_regularization\": 0.1,\n","    \"l2_regularization\": 10,\n","    \"max_depth\": 20,\n","    \"metric\": \"auc\",\n","    \"n_estimators\": 2000,\n","    \"num_leaves\": 64,\n","    \"objective\": \"binary\",\n","    \"random_state\": 42,\n","    \"verbose\": -1,\n","}\n","\n","lgb_params2 = {\n","    \"boosting_type\": \"gbdt\",\n","    \"colsample_bynode\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"extra_trees\": True,\n","    \"learning_rate\": 0.03,\n","    \"l1_regularization\": 0.1,\n","    \"l2_regularization\": 10,\n","    \"max_depth\": 16,\n","    \"metric\": \"auc\",\n","    \"n_estimators\": 2000,\n","    \"num_leaves\": 54,\n","    \"objective\": \"binary\",\n","    \"random_state\": 42,\n","    \"verbose\": -1,\n","}\n","\n","\n","fitted_models_cat = []\n","fitted_models_lgb = []\n","\n","cv_scores_cat = []\n","cv_scores_lgb = []\n","\n","iter_cnt = 0\n","for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n","    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","    \n","    model = Learning_cat(X_train,y_train,X_valid,y_valid)\n","    fitted_models_cat.append(model)\n","\n","    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores_cat.append(auc_score)\n","\n","    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n","    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n","\n","    if iter_cnt % 2 == 0:\n","        version = f'ver1_iter{iter_cnt}'\n","        model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params1, version)\n","    else:\n","        version = f'ver2_iter{iter_cnt}'\n","        model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params2, version)\n","    \n","    fitted_models_lgb.append(model)\n","    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","    auc_score = roc_auc_score(y_valid, y_pred_valid)\n","    cv_scores_lgb.append(auc_score)\n","    \n","    iter_cnt += 1\n","\n","model = VotingModel(fitted_models_cat + fitted_models_lgb)\n","\n","print(f\"\\nCV AUC scores for CatBoost: {cv_scores_cat}\")\n","print(f\"Maximum CV AUC score for Catboost: {max(cv_scores_cat)}\", end=\"\\n\\n\")\n","\n","\n","print(f\"CV AUC scores for LGBM: {cv_scores_lgb}\")\n","print(f\"Maximum CV AUC score for LGBM: {max(cv_scores_lgb)}\", end=\"\\n\\n\")\n","\n","del X, y\n","gc.collect()\n","\"\"\""]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.841405\n","[200]\tvalid_0's auc: 0.849776\n","[300]\tvalid_0's auc: 0.85258\n","[400]\tvalid_0's auc: 0.853498\n","[500]\tvalid_0's auc: 0.853978\n","[600]\tvalid_0's auc: 0.854164\n","[700]\tvalid_0's auc: 0.854368\n","[800]\tvalid_0's auc: 0.854381\n","[900]\tvalid_0's auc: 0.85458\n","[1000]\tvalid_0's auc: 0.85473\n","[1100]\tvalid_0's auc: 0.854774\n","Early stopping, best iteration is:\n","[1096]\tvalid_0's auc: 0.85479\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.832735\n","[200]\tvalid_0's auc: 0.844662\n","[300]\tvalid_0's auc: 0.849483\n","[400]\tvalid_0's auc: 0.851768\n","[500]\tvalid_0's auc: 0.853186\n","[600]\tvalid_0's auc: 0.854068\n","[700]\tvalid_0's auc: 0.854723\n","[800]\tvalid_0's auc: 0.855113\n","[900]\tvalid_0's auc: 0.855331\n","[1000]\tvalid_0's auc: 0.855608\n","[1100]\tvalid_0's auc: 0.855739\n","[1200]\tvalid_0's auc: 0.855769\n","[1300]\tvalid_0's auc: 0.855825\n","[1400]\tvalid_0's auc: 0.855939\n","[1500]\tvalid_0's auc: 0.856166\n","[1600]\tvalid_0's auc: 0.856347\n","[1700]\tvalid_0's auc: 0.856401\n","[1800]\tvalid_0's auc: 0.856455\n","Early stopping, best iteration is:\n","[1726]\tvalid_0's auc: 0.856491\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.846613\n","[200]\tvalid_0's auc: 0.855493\n","[300]\tvalid_0's auc: 0.858526\n","[400]\tvalid_0's auc: 0.859558\n","[500]\tvalid_0's auc: 0.859977\n","[600]\tvalid_0's auc: 0.8604\n","[700]\tvalid_0's auc: 0.860669\n","[800]\tvalid_0's auc: 0.86057\n","Early stopping, best iteration is:\n","[777]\tvalid_0's auc: 0.860702\n","Training until validation scores don't improve for 100 rounds\n","[100]\tvalid_0's auc: 0.837385\n","[200]\tvalid_0's auc: 0.849569\n","[300]\tvalid_0's auc: 0.854339\n","[400]\tvalid_0's auc: 0.856856\n","[500]\tvalid_0's auc: 0.858464\n","[600]\tvalid_0's auc: 0.859532\n","[700]\tvalid_0's auc: 0.85999\n","[800]\tvalid_0's auc: 0.860417\n","[900]\tvalid_0's auc: 0.86063\n","[1000]\tvalid_0's auc: 0.860879\n","[1100]\tvalid_0's auc: 0.861102\n","[1200]\tvalid_0's auc: 0.861114\n","[1300]\tvalid_0's auc: 0.86127\n","[1400]\tvalid_0's auc: 0.861378\n","[1500]\tvalid_0's auc: 0.861399\n","[1600]\tvalid_0's auc: 0.861505\n","[1700]\tvalid_0's auc: 0.861635\n","[1800]\tvalid_0's auc: 0.861719\n","[1900]\tvalid_0's auc: 0.861791\n","[2000]\tvalid_0's auc: 0.861925\n","Did not meet early stopping. Best iteration is:\n","[1999]\tvalid_0's auc: 0.861928\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["def run(df,cat_cols,Learning=True):\n","    X = df.drop(columns=[\"target\", \"case_id\", \"week_num\"])\n","    y = df[\"target\"]\n","\n","    weeks = df[\"week_num\"]\n","\n","    del df\n","    gc.collect()\n","\n","    cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n","    lgb_params1 = {\n","    \"boosting_type\": \"gbdt\",\n","    \"colsample_bynode\": 0.8,\n","    \"colsample_bytree\": 0.8,\n","    \"extra_trees\": True,\n","    \"learning_rate\": 0.05,\n","    \"l1_regularization\": 0.1,\n","    \"l2_regularization\": 10,\n","    \"max_depth\": 20,\n","    \"metric\": \"auc\",\n","    \"n_estimators\": 2000,\n","    \"num_leaves\": 64,\n","    \"objective\": \"binary\",\n","    \"random_state\": 42,\n","    \"verbose\": -1,\n","    }\n","\n","    lgb_params2 = {\n","        \"boosting_type\": \"gbdt\",\n","        \"colsample_bynode\": 0.8,\n","        \"colsample_bytree\": 0.8,\n","        \"extra_trees\": True,\n","        \"learning_rate\": 0.03,\n","        \"l1_regularization\": 0.1,\n","        \"l2_regularization\": 10,\n","        \"max_depth\": 16,\n","        \"metric\": \"auc\",\n","        \"n_estimators\": 2000,\n","        \"num_leaves\": 54,\n","        \"objective\": \"binary\",\n","        \"random_state\": 42,\n","        \"verbose\": -1,\n","    }\n","    fitted_models_cat = []\n","    fitted_models_lgb = []\n","\n","    cv_scores_cat = []\n","    cv_scores_lgb = []\n","    if Learning:\n","        iter_cnt = 0\n","        for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n","            X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n","            X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n","            \n","            model = Learning_cat(X_train,y_train,X_valid,y_valid)\n","            fitted_models_cat.append(model)\n","\n","            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","            auc_score = roc_auc_score(y_valid, y_pred_valid)\n","            cv_scores_cat.append(auc_score)\n","\n","            X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n","            X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n","\n","            if iter_cnt % 2 == 0:\n","                version = f'iter_{iter_cnt}'\n","                model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params1, version)\n","            else:\n","                version = f'iter_{iter_cnt}'\n","                model = Learning_lgb(X_train, y_train, X_valid, y_valid, lgb_params2, version)\n","            \n","            fitted_models_lgb.append(model)\n","            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n","            auc_score = roc_auc_score(y_valid, y_pred_valid)\n","            cv_scores_lgb.append(auc_score)\n","            \n","            iter_cnt += 1\n","    else:\n","        model_path = CFG.MODEL_DATA_PATH / f'cat_ver{CFG.VER}.pkl'\n","        with open(model_path, 'rb') as file:\n","            model = pickle.load(file)\n","        fitted_models_cat.append(model)\n","\n","        model_paths = [(CFG.MODEL_DATA_PATH / f'lgb_ver{CFG.VER}_iter_{i}.pkl') for i in range(1)]\n","\n","        fitted_models_lgb = []\n","        for model_path in model_paths:\n","            with open(model_path, 'rb') as file:\n","                model = pickle.load(file)\n","            fitted_models_lgb.append(model)\n","\n","    model = VotingModel(fitted_models_cat + fitted_models_lgb)\n","    del X,y\n","    return model\n","\n","model = run(df_train,cat_cols,Learning=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-15T00:37:25.761217Z","iopub.status.busy":"2024-05-15T00:37:25.760942Z","iopub.status.idle":"2024-05-15T00:37:26.293500Z","shell.execute_reply":"2024-05-15T00:37:26.292579Z","shell.execute_reply.started":"2024-05-15T00:37:25.761195Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>score</th>\n","    </tr>\n","    <tr>\n","      <th>case_id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>57543</th>\n","      <td>0.002270</td>\n","    </tr>\n","    <tr>\n","      <th>57549</th>\n","      <td>0.001800</td>\n","    </tr>\n","    <tr>\n","      <th>57551</th>\n","      <td>0.000484</td>\n","    </tr>\n","    <tr>\n","      <th>57552</th>\n","      <td>0.000724</td>\n","    </tr>\n","    <tr>\n","      <th>57569</th>\n","      <td>0.000534</td>\n","    </tr>\n","    <tr>\n","      <th>57630</th>\n","      <td>0.000632</td>\n","    </tr>\n","    <tr>\n","      <th>57631</th>\n","      <td>0.000634</td>\n","    </tr>\n","    <tr>\n","      <th>57632</th>\n","      <td>0.000602</td>\n","    </tr>\n","    <tr>\n","      <th>57633</th>\n","      <td>0.001808</td>\n","    </tr>\n","    <tr>\n","      <th>57634</th>\n","      <td>0.000667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            score\n","case_id          \n","57543    0.002270\n","57549    0.001800\n","57551    0.000484\n","57552    0.000724\n","57569    0.000534\n","57630    0.000632\n","57631    0.000634\n","57632    0.000602\n","57633    0.001808\n","57634    0.000667"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["19"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["X_test: pd.DataFrame = df_test.drop(columns=[\"week_num\"]).set_index(\"case_id\")\n","\n","X_test[cat_cols] = X_test[cat_cols].astype(\"category\")\n","\n","y_pred: pd.Series = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n","\n","df_subm[\"score\"] = y_pred\n","\n","display(df_subm)\n","\n","df_subm.to_csv(\"submission.csv\")\n","\n","del X_test, y_pred, df_subm\n","gc.collect()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
