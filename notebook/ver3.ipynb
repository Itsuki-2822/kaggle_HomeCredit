{"cells":[{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.317434Z","iopub.status.busy":"2024-04-09T01:09:46.316880Z","iopub.status.idle":"2024-04-09T01:09:46.329470Z","shell.execute_reply":"2024-04-09T01:09:46.327678Z","shell.execute_reply.started":"2024-04-09T01:09:46.317390Z"},"trusted":true},"outputs":[],"source":["# 標準ライブラリ\n","import gc\n","import os\n","import pickle\n","import random\n","import sys\n","import warnings\n","from itertools import combinations, permutations\n","from pathlib import Path\n","import time\n","from typing import Tuple\n","\n","# サードパーティのライブラリ\n","import category_encoders as ce\n","from joblib import Parallel, delayed\n","import lightgbm as lgb\n","import numpy as np\n","import pandas as pd\n","import fireducks.pandas as pd\n","import polars as pl\n","import scipy as sp\n","import seaborn as sns\n","import torch\n","import xgboost as xgb\n","from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n","from dateutil.relativedelta import relativedelta\n","from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n","from sklearn.impute import KNNImputer\n","from sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\n","from sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n","                                     StratifiedGroupKFold, TimeSeriesSplit,\n","                                     train_test_split)\n","from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n","from tqdm.auto import tqdm\n","\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings('ignore')\n","\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.333671Z","iopub.status.busy":"2024-04-09T01:09:46.332612Z","iopub.status.idle":"2024-04-09T01:09:46.351401Z","shell.execute_reply":"2024-04-09T01:09:46.349841Z","shell.execute_reply.started":"2024-04-09T01:09:46.333627Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n","    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n","    \n","    train_data_path = os.path.join(home_directory, 'train/')\n","    test_data_path = os.path.join(home_directory, 'test/')\n","    \n","    OOF_DATA_PATH = Path(home_directory) / 'oof'\n","    MODEL_DATA_PATH = Path(home_directory) / 'models'\n","    SUB_DATA_PATH = Path(home_directory) / 'submission'\n","\n","    def __init__(self):\n","        self.create_directories()\n","    \n","    def create_directories(self):\n","        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n","            path.mkdir(parents=True, exist_ok=True)\n","    \n","    \n","    VER = 3\n","    AUTHOR = 'Mira'\n","    COMPETITION = 'HomeCredit'\n","\n","    METHOD_LIST = ['lightgbm']\n","    seed = 28\n","    n_folds = 5\n","    target_col = 'target'\n","    metric = 'auc'\n","    \n","    metric_maximize_flag = True\n","    num_boost_round = 500\n","    early_stopping_round = 200\n","    verbose = 25\n","    classification_lgb_params = {\n","        'objective': 'binary',\n","        'metric': 'auc',\n","        'learning_rate': 0.05,\n","        'seed': seed,\n","        #\"device_type\": \"gpu\",\n","    }\n","    classification_xgb_params = {\n","        'objective': 'binary:logistic',\n","        'eval_metric': 'logloss',\n","        'learning_rate': 0.05,\n","        'random_state': seed,\n","        \"tree_method\": \"gpu_hist\",\n","    }\n","\n","    classification_cat_params = {\n","        'learning_rate': 0.05,\n","        'iterations': num_boost_round,\n","        'random_seed': seed,\n","        \"task_type\": \"GPU\",\n","    }\n","    model_weight_dict = {'lightgbm': 1}\n","    \n","\n","class is_kaggle:\n","    def __init__(self, Kaggle):\n","        if Kaggle == \"Yes\":\n","            self.path = Path(CFG.kaggle_directory)\n","        else:\n","            self.path = Path(CFG.home_directory)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.354751Z","iopub.status.busy":"2024-04-09T01:09:46.353688Z","iopub.status.idle":"2024-04-09T01:09:46.372287Z","shell.execute_reply":"2024-04-09T01:09:46.370691Z","shell.execute_reply.started":"2024-04-09T01:09:46.354706Z"},"trusted":true},"outputs":[],"source":["selector = is_kaggle(\"No\")\n","cfg_instance = CFG()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.376460Z","iopub.status.busy":"2024-04-09T01:09:46.376006Z","iopub.status.idle":"2024-04-09T01:09:46.395285Z","shell.execute_reply":"2024-04-09T01:09:46.393443Z","shell.execute_reply.started":"2024-04-09T01:09:46.376423Z"},"trusted":true},"outputs":[],"source":["class Pipeline:\n","    @staticmethod\n","    def set_table_dtypes(df):\n","        for col in df.columns:\n","            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Int32))\n","            elif col in [\"date_decision\"]:\n","                df = df.with_columns(pl.col(col).cast(pl.Date))\n","            elif col[-1] in (\"P\", \"A\"):\n","                df = df.with_columns(pl.col(col).cast(pl.Float64))\n","            elif col[-1] in (\"M\",):\n","                df = df.with_columns(pl.col(col).cast(pl.String))\n","            elif col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col).cast(pl.Date))            \n","\n","        return df\n","    \n","    @staticmethod\n","    def handle_dates(df):\n","        for col in df.columns:\n","            if col[-1] in (\"D\",):\n","                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n","                df = df.with_columns(pl.col(col).dt.total_days())\n","                df = df.with_columns(pl.col(col).cast(pl.Float32))\n","                \n","        df = df.drop(\"date_decision\", \"MONTH\")\n","\n","        return df\n","    \n","    @staticmethod\n","    def filter_cols(df):\n","        for col in df.columns:\n","            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n","                isnull = df[col].is_null().mean()\n","\n","                if isnull > 0.95:\n","                    df = df.drop(col)\n","\n","        for col in df.columns:\n","            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n","                freq = df[col].n_unique()\n","\n","                if (freq == 1) | (freq > 200):\n","                    df = df.drop(col)\n","\n","        return df"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.398730Z","iopub.status.busy":"2024-04-09T01:09:46.397311Z","iopub.status.idle":"2024-04-09T01:09:46.414731Z","shell.execute_reply":"2024-04-09T01:09:46.413331Z","shell.execute_reply.started":"2024-04-09T01:09:46.398687Z"},"trusted":true},"outputs":[],"source":["class Aggregator:\n","    @staticmethod\n","    def num_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n","\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def date_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n","\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def str_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n","        \n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def other_expr(df):\n","        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n","        \n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        return expr_max\n","    \n","    @staticmethod\n","    def count_expr(df):\n","        cols = [col for col in df.columns if \"num_group\" in col]\n","\n","        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n","\n","        return expr_max\n","\n","    @staticmethod\n","    def get_exprs(df):\n","        exprs = Aggregator.num_expr(df) + \\\n","                Aggregator.date_expr(df) + \\\n","                Aggregator.str_expr(df) + \\\n","                Aggregator.other_expr(df) + \\\n","                Aggregator.count_expr(df)\n","\n","        return exprs"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.418394Z","iopub.status.busy":"2024-04-09T01:09:46.417948Z","iopub.status.idle":"2024-04-09T01:09:46.436699Z","shell.execute_reply":"2024-04-09T01:09:46.434442Z","shell.execute_reply.started":"2024-04-09T01:09:46.418360Z"},"trusted":true},"outputs":[],"source":["def read_file(path, depth=None):\n","    df = pl.read_parquet(path)\n","    df = df.pipe(Pipeline.set_table_dtypes)\n","    \n","    if depth in [1, 2]:\n","        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","    \n","    return df\n","\n","def read_files(regex_path, depth=None):\n","    chunks = []\n","    for path in glob.glob(str(regex_path)):\n","        df = pl.read_parquet(path)\n","        df = df.pipe(Pipeline.set_table_dtypes)\n","        \n","        if depth in [1, 2]:\n","            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n","        \n","        chunks.append(df)\n","        \n","    df = pl.concat(chunks, how=\"vertical_relaxed\")\n","    df = df.unique(subset=[\"case_id\"])\n","    \n","    return df"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.441509Z","iopub.status.busy":"2024-04-09T01:09:46.440428Z","iopub.status.idle":"2024-04-09T01:09:46.454710Z","shell.execute_reply":"2024-04-09T01:09:46.453080Z","shell.execute_reply.started":"2024-04-09T01:09:46.441454Z"},"trusted":true},"outputs":[],"source":["def feature_eng(df_base, depth_0, depth_1, depth_2):\n","    df_base = (\n","        df_base\n","        .with_columns(\n","            month_decision = pl.col(\"date_decision\").dt.month(),\n","            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n","        )\n","    )\n","        \n","    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n","        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n","        \n","    df_base = df_base.pipe(Pipeline.handle_dates)\n","    \n","    return df_base"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.457368Z","iopub.status.busy":"2024-04-09T01:09:46.456881Z","iopub.status.idle":"2024-04-09T01:09:46.470863Z","shell.execute_reply":"2024-04-09T01:09:46.469469Z","shell.execute_reply.started":"2024-04-09T01:09:46.457335Z"},"trusted":true},"outputs":[],"source":["def to_pandas(df_data, cat_cols=None):\n","    df_data = df_data.to_pandas()\n","    if cat_cols is None:\n","        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n","    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n","    return df_data, cat_cols\n","\n","def to_polars(df_pandas, cat_cols=None):\n","    df_polars = pl.from_pandas(df_pandas)\n","    \n","    if cat_cols is not None:\n","        for col in cat_cols:\n","            if col in df_polars.columns:\n","                df_polars[col] = df_polars[col].cast(pl.Categorical)\n","    \n","    return df_polars"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.473762Z","iopub.status.busy":"2024-04-09T01:09:46.473274Z","iopub.status.idle":"2024-04-09T01:09:46.491075Z","shell.execute_reply":"2024-04-09T01:09:46.489259Z","shell.execute_reply.started":"2024-04-09T01:09:46.473715Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","    \n","    for col in df.columns:\n","        col_type = df[col].dtype\n","        if str(col_type)==\"category\":\n","            continue\n","        \n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)\n","        else:\n","            continue\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    \n","    return df"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.495225Z","iopub.status.busy":"2024-04-09T01:09:46.494455Z","iopub.status.idle":"2024-04-09T01:09:46.510408Z","shell.execute_reply":"2024-04-09T01:09:46.508715Z","shell.execute_reply.started":"2024-04-09T01:09:46.495188Z"},"trusted":true},"outputs":[],"source":["ROOT = selector.path\n","TRAIN_DIR       = ROOT / \"parquet_files/train\"\n","TEST_DIR        = ROOT / \"parquet_files/test\"\n","SAMPLE_SUB = ROOT / \"sample_submission.csv\""]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:09:46.512892Z","iopub.status.busy":"2024-04-09T01:09:46.512453Z","iopub.status.idle":"2024-04-09T01:12:44.942633Z","shell.execute_reply":"2024-04-09T01:12:44.940853Z","shell.execute_reply.started":"2024-04-09T01:09:46.512861Z"},"trusted":true},"outputs":[],"source":["train_data_store = {\n","    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n","        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n","        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n","        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n","    ]\n","}"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:12:44.947483Z","iopub.status.busy":"2024-04-09T01:12:44.944684Z","iopub.status.idle":"2024-04-09T01:12:45.245115Z","shell.execute_reply":"2024-04-09T01:12:45.244061Z","shell.execute_reply.started":"2024-04-09T01:12:44.947413Z"},"trusted":true},"outputs":[],"source":["test_data_store = {\n","    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n","    \"depth_0\": [\n","        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n","        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n","    ],\n","    \"depth_1\": [\n","        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n","        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n","        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n","    ],\n","    \"depth_2\": [\n","        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n","        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n","    ]\n","}"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:13:31.779150Z","iopub.status.busy":"2024-04-09T01:13:31.778134Z","iopub.status.idle":"2024-04-09T01:13:31.797424Z","shell.execute_reply":"2024-04-09T01:13:31.796385Z","shell.execute_reply.started":"2024-04-09T01:13:31.779110Z"},"trusted":true},"outputs":[],"source":["def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n","    lgb_train = lgb.Dataset(x_train, y_train)\n","    lgb_valid = lgb.Dataset(x_valid, y_valid)\n","    model = lgb.train(\n","                params = CFG.classification_lgb_params,\n","                train_set = lgb_train,\n","                num_boost_round = CFG.num_boost_round,\n","                valid_sets = [lgb_train, lgb_valid],\n","                #feval = CFG.metric,\n","                callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n","                                              verbose=CFG.verbose)]\n","            )\n","    valid_pred = model.predict(x_valid)\n","    \n","    importance_df = pd.DataFrame({\n","        'feature': features,\n","        'importance': model.feature_importance(importance_type='gain')\n","    })\n","    importance_df['importance'] = importance_df['importance'] / np.sum(importance_df['importance'])\n","    importance_df = importance_df.sort_values(by='importance', ascending=False)\n","    #print(importance_df)\n","    return model, valid_pred\n","def xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n","    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n","    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n","    model = xgb.train(\n","                CFG.classification_xgb_params,\n","                dtrain = xgb_train,\n","                num_boost_round = CFG.num_boost_round,\n","                evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n","                early_stopping_rounds = CFG.early_stopping_round,\n","                verbose_eval = CFG.verbose,\n","                #feval = CFG.metric,\n","                maximize = CFG.metric_maximize_flag,\n","        )\n","    valid_pred = model.predict(xgb.DMatrix(x_valid))\n","    return model, valid_pred\n","def catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n","    cat_train = Pool(data=x_train, label=y_train)\n","    cat_valid = Pool(data=x_valid, label=y_valid)\n","    model = CatBoostClassifier(**CFG.classification_cat_params)\n","    model.fit(cat_train,\n","              eval_set = [cat_valid],\n","              early_stopping_rounds = CFG.early_stopping_round,\n","              verbose = CFG.verbose,\n","              use_best_model = True)\n","    valid_pred = model.predict_proba(x_valid)[:, 1]\n","    return model, valid_pred"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:13:31.799977Z","iopub.status.busy":"2024-04-09T01:13:31.798711Z","iopub.status.idle":"2024-04-09T01:13:31.824286Z","shell.execute_reply":"2024-04-09T01:13:31.822166Z","shell.execute_reply.started":"2024-04-09T01:13:31.799935Z"},"trusted":true},"outputs":[],"source":["def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list):\n","    weeks = train_df[\"WEEK_NUM\"]\n","    X = train_df.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n","    y = train_df[\"target\"]\n","    \n","    oof_predictions = np.zeros(len(train_df))\n","    oof_fold = np.zeros(len(train_df))\n","    cv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n","    for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups=weeks)):\n","        print('-'*50)\n","        print(f'{method} training fold {fold+1}')\n","\n","        x_train = train_df[features].iloc[train_index]\n","        y_train = train_df[CFG.target_col].iloc[train_index]\n","        x_valid = train_df[features].iloc[valid_index]\n","        y_valid = train_df[CFG.target_col].iloc[valid_index]\n","        if method == 'lightgbm':\n","            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features)\n","        if method == 'xgboost':\n","            model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features)\n","        if method == 'catboost':\n","            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features)\n","\n","        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n","\n","        oof_predictions[valid_index] = valid_pred\n","        oof_fold[valid_index] = fold + 1\n","        del x_train, x_valid, y_train, y_valid, model, valid_pred\n","        gc.collect()\n","\n","    score = roc_auc_score(train_df[CFG.target_col], oof_predictions)\n","    print(f'{method} our out of folds CV f1score is {score}')\n","\n","    oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n","    oof_df.to_csv(CFG.OOF_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n","\n","def Learning(input_df: pd.DataFrame, features: list):\n","    for method in CFG.METHOD_LIST:\n","        gradient_boosting_model_cv_training(method, input_df, features)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:13:31.828920Z","iopub.status.busy":"2024-04-09T01:13:31.828392Z","iopub.status.idle":"2024-04-09T01:13:31.846828Z","shell.execute_reply":"2024-04-09T01:13:31.845211Z","shell.execute_reply.started":"2024-04-09T01:13:31.828885Z"},"trusted":true},"outputs":[],"source":["def unified_inference(method: str, x_test: pd.DataFrame):\n","    test_pred = np.zeros(len(x_test))\n","    for fold in range(CFG.n_folds):\n","        model_path = CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl'\n","        model = pickle.load(open(model_path, 'rb'))\n","\n","        if method == 'lightgbm':\n","            pred = model.predict(x_test)\n","        elif method == 'xgboost':\n","            pred = model.predict(xgb.DMatrix(x_test))\n","        elif method == 'catboost':\n","            pred = model.predict_proba(x_test)[:, 1]\n","        else:\n","            raise ValueError(f\"Unsupported method: {method}\")\n","\n","        test_pred += pred\n","    \n","    return test_pred / CFG.n_folds\n","\n","def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list):\n","    x_test = test_df[features]\n","    test_pred = unified_inference(method, x_test)\n","    return test_pred\n","\n","def Predicting(input_df: pd.DataFrame, features: list):\n","    output_df = input_df.copy()\n","    output_df['pred_prob'] = 0\n","    for method in CFG.METHOD_LIST:\n","        output_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, input_df, features)\n","        output_df['pred_prob'] += CFG.model_weight_dict[method] * output_df[f'{method}_pred_prob']\n","    return output_df"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:13:31.849247Z","iopub.status.busy":"2024-04-09T01:13:31.848552Z","iopub.status.idle":"2024-04-09T01:13:31.869319Z","shell.execute_reply":"2024-04-09T01:13:31.868084Z","shell.execute_reply.started":"2024-04-09T01:13:31.849065Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import category_encoders as ce\n","from sklearn.model_selection import KFold\n","\n","def target_enc(df, test, col):\n","    features = [c for c in df.columns if c != CFG.target_col]\n","    kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n","    encoded_features = []\n","\n","    for train_idx, val_idx in kf.split(df):\n","        X_train, X_valid = df.loc[train_idx, features], df.loc[val_idx, features]\n","        y_train = df[CFG.target_col].iloc[train_idx]\n","\n","        target_encoder = ce.TargetEncoder()\n","        target_encoder.fit(X_train[col], y_train)\n","\n","        X_valid = X_valid.copy()\n","        X_valid[f'{col}_target_Encoded'] = target_encoder.transform(X_valid[col])\n","        encoded_features.append(X_valid)\n","\n","    encoded_df = pd.concat(encoded_features).sort_index()\n","    df[f'{col}_target_Encoded'] = encoded_df[f'{col}_target_Encoded']\n","    \n","    target_encoder = ce.TargetEncoder()\n","    target_encoder.fit(df[[col]], df[CFG.target_col])\n","    test[f'{col}_target_Encoded'] = target_encoder.transform(test[[col]])\n","    \n","    return df, test\n","\n","def encoder_v1(df, test):\n","    object_columns = [\n","        col for col in df.columns \n","        if df[col].dtype.name == 'object' or df[col].dtype.name == 'category'\n","    ]\n","\n","    bool_columns = [\n","        col for col in df.columns \n","        if df[col].dtype == 'bool'\n","    ]\n","\n","    for col in object_columns:\n","        df, test = target_enc(df, test, col)\n","        \n","    for col in bool_columns:\n","        df[col] = df[col].astype(int)\n","        test[col] = test[col].astype(int)\n","\n","    df = df.drop(object_columns, axis=1)\n","    test = test.drop(object_columns, axis=1)\n","        \n","    return df, test\n","\n","def preprocess_v1(df, test):\n","    df, test = encoder_v1(df, test)\n","    features = [col for col in df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n","    \n","    return df, test, features\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def parallel_target_enc_wrapper(df, test, col):\n","    return target_enc(df.copy(), test.copy(), col)\n","\n","def encoder_v2(df, test):\n","    object_columns = [col for col in df.columns if df[col].dtype.name == 'object' or df[col].dtype.name == 'category']\n","    \n","    results = Parallel(n_jobs=4)(delayed(parallel_target_enc_wrapper)(df, test, col) for col in object_columns)\n","    \n","    for res_df, res_test in results:\n","        df[res_df.columns] = res_df\n","        test[res_test.columns] = res_test\n","\n","    bool_columns = [col for col in df.columns if df[col].dtype == 'bool']\n","    for col in bool_columns:\n","        df[col] = df[col].astype(int)\n","        test[col] = test[col].astype(int)\n","\n","    df.drop(object_columns, axis=1, inplace=True)\n","    test.drop(object_columns, axis=1, inplace=True)\n","        \n","    return df, test\n","\n","def preprocess_v2(df, test):\n","    df, test = encoder_v2(df, test)  \n","    features = [col for col in df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n","    \n","    return df, test, features\n"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class TargetEncoderPL:\n","    def __init__(self, category_column, target_column):\n","        self.category_column = category_column\n","        self.target_column = target_column\n","        self.mapping = None\n","\n","    def fit(self, df):\n","        encoded_col_name = f\"encoded_{self.category_column}\"\n","        self.mapping = (\n","            df.groupby(self.category_column)\n","            .agg(pl.mean(self.target_column).alias(encoded_col_name))\n","        )\n","\n","    def transform(self, df, fill_value=\"\"):\n","        if self.mapping is None:\n","            raise RuntimeError(\"The encoder has not been fitted yet.\")\n","\n","        if self.category_column in df.columns:\n","            df = df.with_columns(df[self.category_column].cast(pl.Utf8).fill_null(fill_value))\n","        if self.category_column in self.mapping.columns:\n","            self.mapping = self.mapping.with_columns(self.mapping[self.category_column].cast(pl.Utf8).fill_null(fill_value))\n","        \n","        return df.join(self.mapping, on=self.category_column)\n","\n","    def fit_transform(self, df, fill_value=\"\"):\n","        self.fit(df)\n","        return self.transform(df, fill_value=fill_value)\n","\n","def run_target_enc_pl_kfold(df: pl.DataFrame, test_df: pl.DataFrame, col: str) -> Tuple[pl.DataFrame, pl.DataFrame]:\n","    encoded_parts = []\n","    \n","    for i in range(5):\n","        #df = df.shuffle()\n","        split_point = int(len(df) * 0.8)\n","        train_df = df[:split_point]\n","        valid_df = df[split_point:]\n","        \n","        encoder = TargetEncoderPL(category_column=col, target_column=CFG.target_col)\n","        encoder.fit(train_df)\n","        valid_encoded = encoder.transform(valid_df)\n","        \n","        encoded_parts.append(valid_encoded)\n","    \n","    encoded_df = pl.concat(encoded_parts)\n","    \n","    full_encoder = TargetEncoderPL(category_column=col, target_column=CFG.target_col)\n","    full_encoder.fit(df)\n","\n","    test_encoded = full_encoder.transform(test_df)\n","    return encoded_df, test_encoded\n","\n","\n","def encoder_v3_pl(df, test):\n","    object_columns = [col for col in df.columns if df[col].dtype == pl.Utf8 or df[col].dtype == pl.Categorical]\n","    bool_columns = [col for col in df.columns if df[col].dtype == pl.Boolean]\n","\n","    for col in object_columns:\n","        df, test = run_target_enc_pl_kfold(df, test, col)\n","\n","    for col in bool_columns:\n","        df = df.with_columns(df[col].cast(pl.Int32))\n","        test = test.with_columns(test[col].cast(pl.Int32))\n","\n","    df = df.drop(object_columns)\n","    test = test.drop(object_columns)\n","    \n","    return df, test\n","\n","def preprocess_v3(df, test):\n","    df, test = encoder_v3_pl(df, test)\n","    features = [col for col in df.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n","    return df, test, features"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["def convert_dict_columns_to_str(df):\n","    \"\"\"\n","    DataFrame内の辞書型データを含むすべての列を文字列に変換します。\n","    \"\"\"\n","    for col in df.columns:\n","        # カテゴリカル列の場合はチェックをスキップ\n","        if pd.api.types.is_categorical_dtype(df[col]):\n","            continue\n","\n","        # 列の各要素が辞書型かどうかをチェックし、辞書型であれば文字列に変換\n","        if df[col].apply(lambda x: isinstance(x, dict) if pd.notnull(x) else False).any():\n","            df[col] = df[col].apply(lambda x: str(x) if isinstance(x, dict) else x)\n","    return df\n","\n","def to_polars(df_pandas):\n","    \"\"\"\n","    PandasのDataFrameをPolarsのDataFrameに変換します。\n","    辞書型データを含む列は事前に文字列に変換されます。\n","    \"\"\"\n","    df_pandas = convert_dict_columns_to_str(df_pandas)\n","    df_polars = pl.from_pandas(df_pandas)\n","    return df_polars"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape:\t (1526659, 472)\n","train data shape:\t (10, 471)\n","train data shape:\t (1526659, 361)\n","test data shape:\t (10, 360)\n"]}],"source":["df_train = feature_eng(**train_data_store)\n","print(\"train data shape:\\t\", df_train.shape)\n","df_test = feature_eng(**test_data_store)\n","print(\"train data shape:\\t\", df_test.shape)\n","\n","df_train = df_train.pipe(Pipeline.filter_cols)\n","df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n","\n","print(\"train data shape:\\t\", df_train.shape)\n","print(\"test data shape:\\t\", df_test.shape)\n","#df_train, cat_cols = to_pandas(df_train)\n","#df_test, cat_cols = to_pandas(df_test, cat_cols)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["1123"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["del train_data_store,test_data_store\n","gc.collect()"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["df_train_pd, cat_cols = to_pandas(df_train)\n","df_test_pd, cat_cols = to_pandas(df_test, cat_cols)\n","\n","df_train_pl = df_train.clone()\n","df_test_pl = df_test.clone()"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-04-09T01:13:32.767430Z","iopub.status.busy":"2024-04-09T01:13:32.766941Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory usage of dataframe is 3159.43 MB\n","Memory usage after optimization is: 1191.00 MB\n","Decreased by 62.3%\n","Memory usage of dataframe is 0.03 MB\n","Memory usage after optimization is: 0.02 MB\n","Decreased by 25.0%\n","ver1のtarget_encoding実行時間 : 1213.022468328476\n","ver3のtarget_encoding実行時間 : 91.98863506317139\n"]}],"source":["df_train = reduce_mem_usage(df_train_pd)\n","df_test = reduce_mem_usage(df_test_pd)\n","weeks = df_train_pd[\"WEEK_NUM\"]\n","features = [col for col in df_train_pd.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n","\n","df_train_1 = df_train.copy()\n","df_test_1 = df_test.copy()\n","start_time_1 = time.time()\n","df_train_1,df_test_1,features_v1 = preprocess_v1(df_train_1,df_test_1)\n","end_time_1 = time.time()\n","print(f'ver1のtarget_encoding実行時間 : {end_time_1 - start_time_1}')\n","\"\"\"\n","df_train_2= df_train.copy()\n","df_test_2 = df_test.copy()\n","start_time_2 = time.time()\n","df_train_2,df_test_2,features = preprocess_v2(df_train_2,df_test_2)\n","end_time_2 = time.time()\n","print(f'ver2のtarget_encoding実行時間 : {end_time_2 - start_time_2}')\n","\"\"\"\n","start_time_3 = time.time()\n","df_train_3,df_test_3,features_v3 = preprocess_v3(df_train_pl,df_test_pl)\n","end_time_3 = time.time()\n","print(f'ver3のtarget_encoding実行時間 : {end_time_3 - start_time_3}')\n","\n","df_train_3, cat_cols = to_pandas(df_train_3)\n","df_test_3, cat_cols = to_pandas(df_test_3, cat_cols)\n"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","lightgbm training fold 1\n","[LightGBM] [Info] Number of positive: 36635, number of negative: 1134229\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207429 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 47642\n","[LightGBM] [Info] Number of data points in the train set: 1170864, number of used features: 356\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031289 -> initscore=-3.432704\n","[LightGBM] [Info] Start training from score -3.432704\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[483]\ttraining's auc: 0.893206\tvalid_1's auc: 0.85628\n","--------------------------------------------------\n","lightgbm training fold 2\n","[LightGBM] [Info] Number of positive: 38784, number of negative: 1190698\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243302 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 47632\n","[LightGBM] [Info] Number of data points in the train set: 1229482, number of used features: 356\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031545 -> initscore=-3.424287\n","[LightGBM] [Info] Start training from score -3.424287\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[496]\ttraining's auc: 0.893256\tvalid_1's auc: 0.851279\n","--------------------------------------------------\n","lightgbm training fold 3\n","[LightGBM] [Info] Number of positive: 39477, number of negative: 1197295\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.207356 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 47607\n","[LightGBM] [Info] Number of data points in the train set: 1236772, number of used features: 357\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031919 -> initscore=-3.412102\n","[LightGBM] [Info] Start training from score -3.412102\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[490]\ttraining's auc: 0.891655\tvalid_1's auc: 0.854587\n","--------------------------------------------------\n","lightgbm training fold 4\n","[LightGBM] [Info] Number of positive: 39478, number of negative: 1213340\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231726 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 47692\n","[LightGBM] [Info] Number of data points in the train set: 1252818, number of used features: 356\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031511 -> initscore=-3.425389\n","[LightGBM] [Info] Start training from score -3.425389\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[500]\ttraining's auc: 0.893295\tvalid_1's auc: 0.852197\n","--------------------------------------------------\n","lightgbm training fold 5\n","[LightGBM] [Info] Number of positive: 37602, number of negative: 1179098\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202029 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 47650\n","[LightGBM] [Info] Number of data points in the train set: 1216700, number of used features: 356\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030905 -> initscore=-3.445448\n","[LightGBM] [Info] Start training from score -3.445448\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[500]\ttraining's auc: 0.894633\tvalid_1's auc: 0.850797\n","lightgbm our out of folds CV f1score is 0.8531761198164567\n","--------------------------------------------------\n","lightgbm training fold 1\n","[LightGBM] [Info] Number of positive: 37765, number of negative: 1280880\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.760955 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 44751\n","[LightGBM] [Info] Number of data points in the train set: 1318645, number of used features: 353\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.028639 -> initscore=-3.523920\n","[LightGBM] [Info] Start training from score -3.523920\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[459]\ttraining's auc: 0.97699\tvalid_1's auc: 0.802016\n","--------------------------------------------------\n","lightgbm training fold 2\n","[LightGBM] [Info] Number of positive: 33745, number of negative: 1091945\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227950 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 44672\n","[LightGBM] [Info] Number of data points in the train set: 1125690, number of used features: 353\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029977 -> initscore=-3.476884\n","[LightGBM] [Info] Start training from score -3.476884\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[356]\ttraining's auc: 0.968315\tvalid_1's auc: 0.860052\n","--------------------------------------------------\n","lightgbm training fold 3\n","[LightGBM] [Info] Number of positive: 35475, number of negative: 1133200\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275319 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 44663\n","[LightGBM] [Info] Number of data points in the train set: 1168675, number of used features: 351\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030355 -> initscore=-3.463973\n","[LightGBM] [Info] Start training from score -3.463973\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[386]\ttraining's auc: 0.970528\tvalid_1's auc: 0.856263\n","--------------------------------------------------\n","lightgbm training fold 4\n","[LightGBM] [Info] Number of positive: 36655, number of negative: 1193615\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263252 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 44792\n","[LightGBM] [Info] Number of data points in the train set: 1230270, number of used features: 351\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029794 -> initscore=-3.483192\n","[LightGBM] [Info] Start training from score -3.483192\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[500]\ttraining's auc: 0.98316\tvalid_1's auc: 0.824043\n","--------------------------------------------------\n","lightgbm training fold 5\n","[LightGBM] [Info] Number of positive: 38680, number of negative: 1224680\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.062891 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 44805\n","[LightGBM] [Info] Number of data points in the train set: 1263360, number of used features: 353\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030617 -> initscore=-3.455112\n","[LightGBM] [Info] Start training from score -3.455112\n","Training until validation scores don't improve for 200 rounds\n","Did not meet early stopping. Best iteration is:\n","[492]\ttraining's auc: 0.978145\tvalid_1's auc: 0.822866\n","lightgbm our out of folds CV f1score is 0.838886689845164\n"]}],"source":["Learning(df_train_1, features_v1)\n","#Learning(df_train_2, features)\n","Learning(df_train_3, features_v3)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"train and valid dataset categorical_feature do not match.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     df_subm[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     df_subm\u001b[38;5;241m.\u001b[39mto_csv(submission_file)\n\u001b[0;32m----> 9\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mPredicting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m create_submission(test_df, SAMPLE_SUB, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[40], line 29\u001b[0m, in \u001b[0;36mPredicting\u001b[0;34m(input_df, features)\u001b[0m\n\u001b[1;32m     27\u001b[0m output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m CFG\u001b[38;5;241m.\u001b[39mMETHOD_LIST:\n\u001b[0;32m---> 29\u001b[0m     output_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_boosting_model_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m CFG\u001b[38;5;241m.\u001b[39mmodel_weight_dict[method] \u001b[38;5;241m*\u001b[39m output_df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_df\n","Cell \u001b[0;32mIn[40], line 22\u001b[0m, in \u001b[0;36mgradient_boosting_model_inference\u001b[0;34m(method, test_df, features)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient_boosting_model_inference\u001b[39m(method: \u001b[38;5;28mstr\u001b[39m, test_df: pd\u001b[38;5;241m.\u001b[39mDataFrame, features: \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     21\u001b[0m     x_test \u001b[38;5;241m=\u001b[39m test_df[features]\n\u001b[0;32m---> 22\u001b[0m     test_pred \u001b[38;5;241m=\u001b[39m \u001b[43munified_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_pred\n","Cell \u001b[0;32mIn[40], line 8\u001b[0m, in \u001b[0;36munified_inference\u001b[0;34m(method, x_test)\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightgbm\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 8\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(xgb\u001b[38;5;241m.\u001b[39mDMatrix(x_test))\n","File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/lightgbm/basic.py:4453\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4451\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4452\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\n\u001b[1;32m   4462\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/lightgbm/basic.py:1116\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     _safe_call(\n\u001b[1;32m   1108\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterValidateFeatureNames(\n\u001b[1;32m   1109\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         )\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[0;32m-> 1116\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1123\u001b[0m predict_type \u001b[38;5;241m=\u001b[39m _C_API_PREDICT_NORMAL\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw_score:\n","File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/lightgbm/basic.py:808\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[0;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cat_cols) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pandas_categorical):\n\u001b[0;32m--> 808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain and valid dataset categorical_feature do not match.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col, category \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cat_cols, pandas_categorical):\n\u001b[1;32m    810\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(data[col]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(category):\n","\u001b[0;31mValueError\u001b[0m: train and valid dataset categorical_feature do not match."]}],"source":["def create_submission(test_df, submission_dir, submission_file):\n","    df_subm = pd.read_csv(submission_dir)\n","    df_subm.set_index(\"case_id\", inplace=True)\n","    \n","    df_subm[\"score\"] = test_df.set_index(\"case_id\")[\"pred_prob\"]\n","    df_subm.to_csv(submission_file)\n","\n","\n","test_df = Predicting(df_test, features)\n","create_submission(test_df, SAMPLE_SUB, \"submission.csv\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7921029,"sourceId":50160,"sourceType":"competition"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
