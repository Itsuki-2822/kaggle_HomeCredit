{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import pickle as pkl\n",
    "import  gc\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    home_directory = os.path.expanduser('~/kaggle_HomeCredit/parquet_files/')\n",
    "    train_data_path = os.path.join(home_directory, 'train/')\n",
    "    test_data_path = os.path.join(home_directory, 'test/')\n",
    "    \n",
    "    train_applprev_path = 'train_applprev_*.parquet'\n",
    "    train_base_path =  'train_base.parquet'\n",
    "    train_credit_path = 'train_credit_bureau_*.parquet'\n",
    "    train_debitcard_path = 'train_debitcard_1.parquet'\n",
    "    train_deposit_path = 'train_deposit_1.parquet'\n",
    "    train_other_path = 'train_other_1.parquet'\n",
    "    train_person_path = 'train_person_*.parquet'\n",
    "    train_static_path = 'train_static_*.parquet'\n",
    "    train_tax__path = 'train_tax_registry_*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def reduce_mem_usage(df, int_cast=True, obj_to_category=False, subset=None):\n",
    "    \"\"\"\n",
    "    1. メモリ使用量のチェック\n",
    "    2. データ型の確認と変更\n",
    "    3. メモリ使用量の再計算と削減効果の表示\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    gc.collect()\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "#     cols_none = subset if subset is  None else df.columns.tolist()\n",
    "#     for col_non in tqdm(cols_none):\n",
    "#         df[col_non] = df[col_non].fillna(-888)\n",
    "    \n",
    "    cols = subset if subset is not None else df.columns.tolist()\n",
    "\n",
    "    for col in tqdm(cols):\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n",
    "#             df[col] = df[col].fillna(-888)\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "#             # test if column can be converted to an integer\n",
    "#             treat_as_int = str(col_type)[:3] == 'int'\n",
    "#             if int_cast and not treat_as_int:\n",
    "#                 treat_as_int = check_if_integer(df[col])\n",
    "                \n",
    "            treat_as_int = True\n",
    "            if treat_as_int:\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n",
    "                    df[col] = df[col].astype(np.uint8, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n",
    "                    df[col] = df[col].astype(np.uint16, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n",
    "                    df[col] = df[col].astype(np.uint32, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64, errors='ignore')\n",
    "                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n",
    "                    df[col] = df[col].astype(np.uint64, errors='ignore')\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16, errors='ignore')\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32, errors='ignore')\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64, errors='ignore')\n",
    "        elif 'datetime' not in col_type.name and obj_to_category:\n",
    "            df[col] = df[col].fillna('Mis')\n",
    "            df[col] = df[col].astype('category')\n",
    "    gc.collect()\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.3f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "def date_column_depth_0(df):\n",
    "    \"\"\"\n",
    "    1. 特定の命名規則に基づいて日付列を特定\n",
    "    2. それらを日付型に変換します。\n",
    "    3. 'date_decision'列と他の日付列との差分（日数）を計算し、新しい列としてデータフレームに追加します。\n",
    "    \"\"\"\n",
    "    date_columns = ['date_decision'] + [x for x in df.columns if x[-1] == 'D'] \n",
    "    df[date_columns] = df[date_columns].apply(pd.to_datetime, errors='coerce')\n",
    "    df_diff = df[date_columns].apply(lambda col: (df['date_decision'] - col).dt.days)\n",
    "    df_diff.columns = [f'Diff_{col}' for col in df_diff.columns]\n",
    "    df = pd.concat([df, df_diff], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def union_parquest(list_parq):\n",
    "    \"\"\"\n",
    "    1. 指定されたParquetファイルのリストからデータフレームの読み込み\n",
    "    2. 各データフレームにreduce_mem_usage関数を適用し、メモリ使用量を削減します。\n",
    "    3. これらのデータフレームを結合し、再度メモリ使用量の削減を試みます。\n",
    "    \"\"\"\n",
    "    df_list = [reduce_mem_usage(pd.read_parquet(i)) for i in list_parq]\n",
    "    union_df = pd.concat(df_list)\n",
    "    union_df = reduce_mem_usage(union_df)\n",
    "    return union_df   \n",
    "\n",
    "def gini(x):\n",
    "    \"\"\"\n",
    "    配列内の各値に対して、他の全ての値との差の絶対値を計算し、これらの差の総和を求めます。\n",
    "    この総和を配列の長さの2乗と配列の平均値の積で正規化し、Gini係数を計算します。\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        total += np.sum(np.abs(xi - x[i:]))\n",
    "    return total / (len(x)**2 * np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_merge(base_data,train_vs_test,data_type):\n",
    "    \"\"\"\n",
    "    1. train_vs_test変数をチェックし、'train'または'test'のいずれかに基づいて、\n",
    "        対応するデータセットのファイルパスをCFG設定から取得します。\n",
    "    2. 'num_group1' カラムが存在する場合に限り、そのカラムの値が 0 である行のみを保持し、\n",
    "        そのカラムをデータフレームから削除する処理を行っています。\n",
    "    3. 読み込んだデータフレームを先に作成した空のデータフレームに結合します\n",
    "    \"\"\"\n",
    "    if train_vs_test ==  'train':\n",
    "        file_path = CFG.train_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    elif train_vs_test ==  'test':\n",
    "        file_path = CFG.test_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    df_i_merged = pd.DataFrame()\n",
    "    \n",
    "    for i in list_parq:\n",
    "        print(i)\n",
    "        df_i = pd.read_parquet(i)\n",
    "        df_i = reduce_mem_usage(df_i)\n",
    "        if 'num_group1' in df_i.columns: \n",
    "            df_i = df_i[df_i['num_group1'] == 0 ]\n",
    "            df_i = df_i.drop(columns = 'num_group1')\n",
    "    #         df_i_merged = df_i_merged.merge(df_i,how = 'left',on = 'case_id')\n",
    "        df_i_merged = pd.concat([df_i_merged,df_i])\n",
    "        del df_i\n",
    "        gc.collect()\n",
    "    return df_i_merged        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_merge_v2(base_data,train_vs_test,data_type):\n",
    "    \"\"\"\n",
    "    1. train_vs_test変数をチェックし、'train'または'test'のいずれかに基づいて、\n",
    "        対応するデータセットのファイルパスをCFG設定から取得します。\n",
    "    2. ファイルパスのlistを読み込みnum_group1のカラムを削除する\n",
    "    3. 読み込んだデータフレームを先に作成した空のデータフレームに結合します\n",
    "    \"\"\"\n",
    "    if train_vs_test ==  'train':\n",
    "        file_path = CFG.train_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    elif train_vs_test ==  'test':\n",
    "        file_path = CFG.test_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    df_i_merged = pd.DataFrame()\n",
    "    \n",
    "    for i in list_parq:\n",
    "        print(i)\n",
    "        df_i = pd.read_parquet(i)\n",
    "        df_i = reduce_mem_usage(df_i)\n",
    "        if 'num_group1' in df_i.columns: \n",
    "#             df_i = df_i[df_i['num_group1'] == 0 ]\n",
    "            df_i = df_i.drop(columns = 'num_group1')\n",
    "    #         df_i_merged = df_i_merged.merge(df_i,how = 'left',on = 'case_id')\n",
    "        df_i_merged = pd.concat([df_i_merged,df_i])\n",
    "        del df_i\n",
    "        gc.collect()\n",
    "    return df_i_merged        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_merge_depth_2(base_data,train_vs_test,data_type):\n",
    "    \"\"\"\n",
    "    1. train_vs_test変数をチェックし、'train'または'test'のいずれかに基づいて、\n",
    "        対応するデータセットのファイルパスをCFG設定から取得します。\n",
    "    2. num_group1列が存在する場合、その値が0である行のみを残します。\n",
    "        オブジェクト型の列を除外し、残った数値型のデータに対してピボットテーブルを作成します。\n",
    "        このピボットテーブルでは、case_idをインデックスにして、各数値型の列に対して最大値（max）と最小値（min）の集計関数を適用します。\n",
    "        集計後の列名を再構成し、num_groupを含む任意の列を削除します。\n",
    "    3.\n",
    "    \"\"\"\n",
    "    if train_vs_test ==  'train':\n",
    "        file_path = CFG.train_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    elif train_vs_test ==  'test':\n",
    "        file_path = CFG.test_data_path\n",
    "        list_parq =  [file_path + '/' + i for i in os.listdir(file_path) if data_type in i ] \n",
    "        \n",
    "    df_i_merged = pd.DataFrame()\n",
    "    \n",
    "    for i in list_parq:\n",
    "        print(i)\n",
    "        df_i = pd.read_parquet(i)\n",
    "        df_i = reduce_mem_usage(df_i)\n",
    "        if 'num_group1' in df_i.columns: \n",
    "            df_i = df_i[df_i['num_group1'] == 0 ]\n",
    "            df_i = df_i.select_dtypes(exclude=['object'])\n",
    "            df_i = pd.pivot_table(df_i,index = 'case_id', aggfunc= {'max','min'})\n",
    "            df_i.columns = [f'{j}_{i}' if j != '' else f'{i}' for i,j in df_i.columns]\n",
    "            df_i = df_i.drop(columns = [ i for i in df_i.columns if 'num_group' in i])\n",
    "        df_i_merged = pd.concat([df_i_merged,df_i])\n",
    "        del df_i\n",
    "        gc.collect()\n",
    "    return df_i_merged        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 58.24 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 461.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 26.207 MB\n",
      "Decreased by 55.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_base_df = pd.read_parquet(CFG.train_data_path + CFG.train_base_path)\n",
    "train_base_df = reduce_mem_usage(train_base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_static_0_0.parquet\n",
      "Memory usage of dataframe is 1279.85 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 263.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1045.325 MB\n",
      "Decreased by 18.3%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_static_0_1.parquet\n",
      "Memory usage of dataframe is 666.73 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [00:00<00:00, 483.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 547.050 MB\n",
      "Decreased by 18.0%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_static_cb_0.parquet\n",
      "Memory usage of dataframe is 606.73 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:00<00:00, 200.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 601.006 MB\n",
      "Decreased by 0.9%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_applprev_1_1.parquet\n",
      "Memory usage of dataframe is 825.27 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 124.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 797.596 MB\n",
      "Decreased by 3.4%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_applprev_1_0.parquet\n",
      "Memory usage of dataframe is 1216.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:00<00:00, 84.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1175.304 MB\n",
      "Decreased by 3.4%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_1_1.parquet\n",
      "Memory usage of dataframe is 3621.87 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:02<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 3564.565 MB\n",
      "Decreased by 1.6%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_1_0.parquet\n",
      "Memory usage of dataframe is 2476.11 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 56.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2436.932 MB\n",
      "Decreased by 1.6%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_1_2.parquet\n",
      "Memory usage of dataframe is 2256.48 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 54.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2220.774 MB\n",
      "Decreased by 1.6%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_1_3.parquet\n",
      "Memory usage of dataframe is 1253.25 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 102.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1233.424 MB\n",
      "Decreased by 1.6%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_b_1.parquet\n",
      "Memory usage of dataframe is 29.45 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 1512.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 28.554 MB\n",
      "Decreased by 3.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_debitcard_1.parquet\n",
      "Memory usage of dataframe is 7.20 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 1109.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 5.551 MB\n",
      "Decreased by 22.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_deposit_1.parquet\n",
      "Memory usage of dataframe is 5.53 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2376.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 3.459 MB\n",
      "Decreased by 37.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_person_1.parquet\n",
      "Memory usage of dataframe is 839.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:00<00:00, 331.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 808.322 MB\n",
      "Decreased by 3.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_tax_registry_a_1.parquet\n",
      "Memory usage of dataframe is 124.96 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 250.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 78.101 MB\n",
      "Decreased by 37.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_tax_registry_b_1.parquet\n",
      "Memory usage of dataframe is 42.26 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 757.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 26.415 MB\n",
      "Decreased by 37.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_tax_registry_c_1.parquet\n",
      "Memory usage of dataframe is 127.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 258.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 79.723 MB\n",
      "Decreased by 37.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = train_base_df[['case_id']]\n",
    "variable_type_list = ['train_static_0',\n",
    "                      'train_static_cb_0',\n",
    "                      'train_applprev_1',\n",
    "                      'train_credit_bureau_a_1',\n",
    "                     'train_credit_bureau_b_1',\n",
    "                     'train_debitcard_1',\n",
    "                     'train_deposit_1',\n",
    "                     'train_person_1',\n",
    "                     'train_tax_registry_a_1',\n",
    "                     'train_tax_registry_b_1',\n",
    "                     'train_tax_registry_c_1']\n",
    "for k in variable_type_list:\n",
    "    df_k = multi_merge(train_base_df,'train',k)\n",
    "    df_merged = df_merged.merge(df_k,how = 'outer',on = 'case_id')\n",
    "    del df_k\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "#Merge with Base\n",
    "df_merged_train_depth_1_0 = train_base_df.merge(df_merged,how = 'left',on = 'case_id')\n",
    "del df_merged\n",
    "#Convert date columns to difference\n",
    "date_columns_train_depth_1_0 = [x for x in df_merged_train_depth_1_0.columns if x[-1] == 'D']\n",
    "df_merged_train_depth_1_0 = date_column_depth_0(df_merged_train_depth_1_0)\n",
    "\n",
    "\n",
    "df_merged_train_depth_1_0 = df_merged_train_depth_1_0.drop(columns = date_columns_train_depth_1_0)\n",
    "gc.collect()\n",
    "\n",
    "# df_merged_train_depth_1_0.to_parquet('/kaggle/working/df_merged_train_depth_1_0.parquet')\n",
    "del df_merged_train_depth_1_0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_2.parquet\n",
      "Memory usage of dataframe is 2593.82 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2303.722 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_8.parquet\n",
      "Memory usage of dataframe is 2018.85 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 15.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1793.055 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_3.parquet\n",
      "Memory usage of dataframe is 3850.66 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 3419.997 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_6.parquet\n",
      "Memory usage of dataframe is 3698.08 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 3284.483 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_7.parquet\n",
      "Memory usage of dataframe is 1167.78 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 32.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1037.176 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_10.parquet\n",
      "Memory usage of dataframe is 635.80 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 60.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 560.505 MB\n",
      "Decreased by 11.8%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_4.parquet\n",
      "Memory usage of dataframe is 3917.61 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 3479.457 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_0.parquet\n",
      "Memory usage of dataframe is 767.70 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 676.792 MB\n",
      "Decreased by 11.8%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_5.parquet\n",
      "Memory usage of dataframe is 4791.42 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 4255.541 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_9.parquet\n",
      "Memory usage of dataframe is 2714.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 2410.541 MB\n",
      "Decreased by 11.2%\n",
      "/home/i.itsuki/kaggle_HomeCredit/parquet_files/train//train_credit_bureau_a_2_1.parquet\n",
      "Memory usage of dataframe is 1139.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 34.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1012.177 MB\n",
      "Decreased by 11.2%\n"
     ]
    }
   ],
   "source": [
    "df_merged = train_base_df[['case_id']]\n",
    "df_k = multi_merge(df_merged,'train','train_credit_bureau_a_2')\n",
    "\n",
    "\n",
    "train_bureau = glob.glob(os.path.join(CFG.train_data_path,CFG.train_credit_path))\n",
    "credit_bureau_b_2 = pd.read_parquet(train_bureau)\n",
    "# credit_bureau_b_2 = credit_bureau_b_2[credit_bureau_b_2['num_group1'] == 0 ]\n",
    "credit_bureau_b_2 = credit_bureau_b_2.rename(columns = {'pmts_date_1107D':'record_date','pmts_dpdvalue_108P':'max_dpd'})\n",
    "credit_bureau_b_2 = credit_bureau_b_2.dropna(subset = 'record_date')\n",
    "credit_bureau_b_2 = credit_bureau_b_2[['case_id','record_date','max_dpd']]\n",
    "credit_bureau_b_2['record_date'] = pd.to_datetime(credit_bureau_b_2['record_date'])\n",
    "\n",
    "\n",
    "\n",
    "# df_k = df_k[df_k['num_group1'] == 0 ]\n",
    "df_k = df_k.select_dtypes(exclude=['object'])\n",
    "#Get max record date\n",
    "df_k['record_date'] = pd.to_datetime(df_k[['pmts_year_1139T', 'pmts_year_507T']].max(axis = 1).astype('Int64').astype(str) +  '-'  +df_k[['pmts_month_158T', 'pmts_month_706T']].max(axis = 1).astype('Int64').astype(str) +  '-' +  '1',errors= 'coerce')\n",
    "#Get max dpd\n",
    "df_k['max_dpd'] = df_k[['pmts_dpd_1073P', 'pmts_dpd_303P']].max(axis = 1)\n",
    "df_k = df_k[['case_id','record_date','max_dpd']]\n",
    "df_k = pd.concat([df_k,credit_bureau_b_2],axis = 0)\n",
    "#Merge with base\n",
    "df_k_merged = train_base_df[['case_id','date_decision']].merge(df_k[['case_id','record_date','max_dpd']], how = 'inner', on ='case_id')\n",
    "#Delete df_k\n",
    "del df_k\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "df_k_merged['date_decision'] = pd.to_datetime(df_k_merged['date_decision'])\n",
    "df_k_merged = df_k_merged.assign(\n",
    "    time_diff=\n",
    "    (df_k_merged.date_decision.dt.year - df_k_merged.record_date.dt.year) * 12 +\n",
    "    (df_k_merged.date_decision.dt.month - df_k_merged.record_date.dt.month)\n",
    ")\n",
    "df_k_merged = df_k_merged[df_k_merged['time_diff'] >= 0]\n",
    "df_k_merged['max_dpd'] = df_k_merged['max_dpd'].fillna(0)\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 0) & (df_k_merged['time_diff'] <= 3),'time_diff_cat'] = '0_3_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 3) & (df_k_merged['time_diff'] <= 6),'time_diff_cat'] = '3_6_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 6) & (df_k_merged['time_diff'] <= 9),'time_diff_cat'] = '6_9_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 9) & (df_k_merged['time_diff'] <= 12),'time_diff_cat'] = '9_12_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 12) & (df_k_merged['time_diff'] <= 18),'time_diff_cat'] = '12_18_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 18) & (df_k_merged['time_diff'] <= 24),'time_diff_cat'] = '18_24_months'\n",
    "\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 0) & (df_k_merged['time_diff'] <= 3),'time_diff_cat'] = '0_3_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 3) & (df_k_merged['time_diff'] <= 6),'time_diff_cat'] = '3_6_months'\n",
    "# df_k_merged.loc[(df_k_merged['time_diff'] > 6) & (df_k_merged['time_diff'] <= 9),'time_diff_cat'] = '6_9_months'\n",
    "df_k_merged.loc[(df_k_merged['time_diff'] > 0) & (df_k_merged['time_diff'] <= 6),'time_diff_cat'] = '0_6_months'\n",
    "df_k_merged.loc[(df_k_merged['time_diff'] > 0) & (df_k_merged['time_diff'] <= 12),'time_diff_cat'] = '0_12_months'\n",
    "df_k_merged.loc[(df_k_merged['time_diff'] > 0) & (df_k_merged['time_diff'] <= 24),'time_diff_cat'] = '0_24_months'\n",
    "df_k_merged.loc[(df_k_merged['time_diff'] > 24),'time_diff_cat'] = '24_months'\n",
    "df_k_pivot = pd.pivot_table(df_k_merged,index = 'case_id', columns = 'time_diff_cat',values = 'max_dpd',aggfunc= {'max','min','mean','median'})\n",
    "del df_k_merged\n",
    "gc.collect()\n",
    "\n",
    "df_k_pivot.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in df_k_pivot.columns]\n",
    "df_k_pivot = df_k_pivot.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_k_pivot.to_parquet('/kaggle/working/df_merged_train_depth_2_v2.parquet')\n",
    "del df_k_pivot\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_1_0.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_merged_train_depth_1_0 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_1_0.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_merged_train_depth_2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_2_v2.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_merged_train_depth_1_0 \u001b[38;5;241m=\u001b[39m reduce_mem_usage(df_merged_train_depth_1_0)\n",
      "File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/kaggle_HomeCredit/.venv/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_1_0.parquet'"
     ]
    }
   ],
   "source": [
    "df_merged_train_depth_1_0 = pd.read_parquet('/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_1_0.parquet')\n",
    "df_merged_train_depth_2 = pd.read_parquet('/kaggle/input/efficient-data-read-only-pandas-lgbm/df_merged_train_depth_2_v2.parquet')\n",
    "df_merged_train_depth_1_0 = reduce_mem_usage(df_merged_train_depth_1_0)\n",
    "df_merged_train_depth_2 = reduce_mem_usage(df_merged_train_depth_2)\n",
    "df_merged_train = df_merged_train_depth_1_0.merge(df_merged_train_depth_2,how = 'left',on=  'case_id')\n",
    "del df_merged_train_depth_1_0\n",
    "del df_merged_train_depth_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Missialue\n",
    "num_cols = df_merged_train.select_dtypes(include=np.number).columns\n",
    "df_merged_train[num_cols] = df_merged_train[num_cols].fillna(0)\n",
    "\n",
    "object_cols = df_merged_train.select_dtypes(include='object').columns\n",
    "df_merged_train[object_cols] = df_merged_train[object_cols].fillna('Mis')\n",
    "df_merged_train = df_merged_train.drop_duplicates(subset= 'case_id')    \n",
    "    \n",
    "#Reindexing\n",
    "identifier_cols = ['date_decision','MONTH']\n",
    "target = 'target'\n",
    "# Reindex\n",
    "df_merged_train = df_merged_train.set_index(['case_id','WEEK_NUM']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define X,y\n",
    "X = df_merged_train.drop(columns = identifier_cols + [target])\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "y = df_merged_train['target']\n",
    "#Delete data\n",
    "del df_merged_train\n",
    "gc.collect()\n",
    "#Pick some weeks from starting and some weeks from end as OOT\n",
    "oot_weeks = [0,  1,  2,  3, \n",
    "                        48, 49, 50, 51, 52,\n",
    "                        87, 88, 89,90, 91]\n",
    "#oot df\n",
    "X_oot = X[X.index.isin(oot_weeks,level = 1)]\n",
    "y_oot = y[y.index.isin(oot_weeks,level = 1)]\n",
    "\n",
    "#training df\n",
    "X = X[~X.index.isin(oot_weeks,level = 1)]\n",
    "y = y[~y.index.isin(oot_weeks,level = 1)]\n",
    "\n",
    "\n",
    "#Train test split(stratified with WEEK_NUM in index 1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, stratify= list(X.index.get_level_values(1)) , random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val,stratify= list(X_val.index.get_level_values(1)) ,test_size=0.50, random_state=42)\n",
    "#delete\n",
    "del X,y\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
