{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"},{"sourceId":8141798,"sourceType":"datasetVersion","datasetId":4813818}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 標準ライブラリ\nimport gc\nimport os\nimport pickle\nimport random\nimport sys\nimport warnings\nfrom itertools import combinations, permutations\nfrom pathlib import Path\n\n# サードパーティのライブラリ\nimport category_encoders as ce\nimport joblib\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport scipy as sp\nimport seaborn as sns\nimport torch\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import f1_score, log_loss, matthews_corrcoef, roc_auc_score\nfrom sklearn.model_selection import (GroupKFold, KFold, StratifiedKFold,\n                                     StratifiedGroupKFold, TimeSeriesSplit,\n                                     train_test_split)\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom tqdm.auto import tqdm\n\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\nimport glob","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:42.969417Z","iopub.execute_input":"2024-04-17T04:26:42.970052Z","iopub.status.idle":"2024-04-17T04:26:52.945733Z","shell.execute_reply.started":"2024-04-17T04:26:42.970008Z","shell.execute_reply":"2024-04-17T04:26:52.944488Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    home_directory = os.path.expanduser('~/kaggle_HomeCredit/')\n    kaggle_directory = os.path.expanduser('/kaggle/input/home-credit-credit-risk-model-stability/')\n    \n    train_data_path = os.path.join(home_directory, 'train/')\n    test_data_path = os.path.join(home_directory, 'test/')\n    \n    OOF_DATA_PATH = Path(home_directory) / 'oof'\n    MODEL_DATA_PATH = Path(home_directory) / 'models'\n    SUB_DATA_PATH = Path(home_directory) / 'submission'\n\n    def __init__(self):\n        self.create_directories()\n    \n    def create_directories(self):\n        for path in [self.OOF_DATA_PATH, self.MODEL_DATA_PATH, self.SUB_DATA_PATH]:\n            path.mkdir(parents=True, exist_ok=True)\n    \n    \n    VER = 4\n    AUTHOR = 'Mira'\n    COMPETITION = 'HomeCredit'\n\n    METHOD_LIST = ['lightgbm']\n    seed = 28\n    n_folds = 5\n    target_col = 'target'\n    metric = 'auc'\n    \n    metric_maximize_flag = True\n    num_boost_round = 500\n    early_stopping_round = 200\n    verbose = 25\n    classification_lgb_params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': 0.05,\n        'seed': seed,\n        #\"device_type\": \"gpu\",\n    }\n    classification_xgb_params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'logloss',\n        'learning_rate': 0.05,\n        'random_state': seed,\n        \"tree_method\": \"gpu_hist\",\n    }\n\n    classification_cat_params = {\n        'learning_rate': 0.05,\n        'iterations': num_boost_round,\n        'random_seed': seed,\n        \"task_type\": \"GPU\",\n    }\n    model_weight_dict = {'lightgbm': 1}\n    \n\nclass is_kaggle:\n    def __init__(self, Kaggle):\n        if Kaggle == \"Yes\":\n            self.path = Path(CFG.kaggle_directory)\n            CFG.MODEL_DATA_PATH = Path('/kaggle/input/git-input-ver4/models')\n        else:\n            self.path = Path(CFG.home_directory)\n            CFG.MODEL_DATA_PATH = Path(CFG.home_directory) / 'models'\n      \ncfg_instance = CFG()      \nselector = is_kaggle(\"Yes\")","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:44:19.784854Z","iopub.execute_input":"2024-04-17T04:44:19.785291Z","iopub.status.idle":"2024-04-17T04:44:19.799654Z","shell.execute_reply.started":"2024-04-17T04:44:19.785257Z","shell.execute_reply":"2024-04-17T04:44:19.798431Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Pipeline:\n    @staticmethod\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int32))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))            \n\n        return df\n    \n    @staticmethod\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n                df = df.with_columns(pl.col(col).dt.total_days())\n                df = df.with_columns(pl.col(col).cast(pl.Float32))\n                \n        df = df.drop(\"date_decision\", \"MONTH\")\n\n        return df\n    \n    @staticmethod\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n\n                if isnull > 0.95:\n                    df = df.drop(col)\n\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:52.965808Z","iopub.execute_input":"2024-04-17T04:26:52.966643Z","iopub.status.idle":"2024-04-17T04:26:52.982624Z","shell.execute_reply.started":"2024-04-17T04:26:52.966611Z","shell.execute_reply":"2024-04-17T04:26:52.981287Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Aggregator:\n    @staticmethod\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        expr_std = [pl.std(col).alias(f\"std_{col}\") for col in cols] \n\n        return expr_max + expr_min + expr_mean + expr_std \n\n    @staticmethod\n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n\n        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_min + expr_max\n\n    @staticmethod\n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        \n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n\n    @staticmethod\n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        \n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n    \n    @staticmethod\n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n\n        return expr_max\n\n    @staticmethod\n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:52.986080Z","iopub.execute_input":"2024-04-17T04:26:52.986588Z","iopub.status.idle":"2024-04-17T04:26:53.013660Z","shell.execute_reply.started":"2024-04-17T04:26:52.986548Z","shell.execute_reply":"2024-04-17T04:26:53.012141Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    \n    if depth in [1, 2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n    \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    for path in glob.glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        \n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        \n        chunks.append(df)\n        \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.015896Z","iopub.execute_input":"2024-04-17T04:26:53.017417Z","iopub.status.idle":"2024-04-17T04:26:53.031927Z","shell.execute_reply.started":"2024-04-17T04:26:53.017378Z","shell.execute_reply":"2024-04-17T04:26:53.030318Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n        \n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n        \n    df_base = df_base.pipe(Pipeline.handle_dates)\n    \n    return df_base","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.033815Z","iopub.execute_input":"2024-04-17T04:26:53.034275Z","iopub.status.idle":"2024-04-17T04:26:53.048653Z","shell.execute_reply.started":"2024-04-17T04:26:53.034237Z","shell.execute_reply":"2024-04-17T04:26:53.047562Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    \n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    \n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    \n    return df_data, cat_cols","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.052116Z","iopub.execute_input":"2024-04-17T04:26:53.052857Z","iopub.status.idle":"2024-04-17T04:26:53.059414Z","shell.execute_reply.started":"2024-04-17T04:26:53.052819Z","shell.execute_reply":"2024-04-17T04:26:53.058148Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.060844Z","iopub.execute_input":"2024-04-17T04:26:53.062099Z","iopub.status.idle":"2024-04-17T04:26:53.081894Z","shell.execute_reply.started":"2024-04-17T04:26:53.062057Z","shell.execute_reply":"2024-04-17T04:26:53.080717Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ROOT = selector.path\nTRAIN_DIR       = ROOT / \"parquet_files/train\"\nTEST_DIR        = ROOT / \"parquet_files/test\"\nSAMPLE_SUB = ROOT / \"sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.083514Z","iopub.execute_input":"2024-04-17T04:26:53.084653Z","iopub.status.idle":"2024-04-17T04:26:53.099697Z","shell.execute_reply.started":"2024-04-17T04:26:53.084614Z","shell.execute_reply":"2024-04-17T04:26:53.098216Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:26:53.105262Z","iopub.execute_input":"2024-04-17T04:26:53.105761Z","iopub.status.idle":"2024-04-17T04:30:13.732782Z","shell.execute_reply.started":"2024-04-17T04:26:53.105720Z","shell.execute_reply":"2024-04-17T04:30:13.730156Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test_data_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:13.736509Z","iopub.execute_input":"2024-04-17T04:30:13.737116Z","iopub.status.idle":"2024-04-17T04:30:14.120056Z","shell.execute_reply.started":"2024-04-17T04:30:13.737042Z","shell.execute_reply":"2024-04-17T04:30:14.118713Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_train = feature_eng(**train_data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndf_test = feature_eng(**test_data_store)\nprint(\"train data shape:\\t\", df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:14.121840Z","iopub.execute_input":"2024-04-17T04:30:14.122234Z","iopub.status.idle":"2024-04-17T04:30:33.151437Z","shell.execute_reply.started":"2024-04-17T04:30:14.122202Z","shell.execute_reply":"2024-04-17T04:30:33.149222Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 738)\ntrain data shape:\t (10, 737)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = df_train.pipe(Pipeline.filter_cols)\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:33.153176Z","iopub.execute_input":"2024-04-17T04:30:33.153569Z","iopub.status.idle":"2024-04-17T04:30:36.839987Z","shell.execute_reply.started":"2024-04-17T04:30:33.153536Z","shell.execute_reply":"2024-04-17T04:30:36.838797Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 519)\ntest data shape:\t (10, 518)\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train, cat_cols = to_pandas(df_train)\ndf_test, cat_cols = to_pandas(df_test, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:30:36.841811Z","iopub.execute_input":"2024-04-17T04:30:36.842715Z","iopub.status.idle":"2024-04-17T04:31:02.696711Z","shell.execute_reply.started":"2024-04-17T04:30:36.842671Z","shell.execute_reply":"2024-04-17T04:31:02.695837Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def lightgbm_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_valid = lgb.Dataset(x_valid, y_valid)\n    model = lgb.train(\n                params = CFG.classification_lgb_params,\n                train_set = lgb_train,\n                num_boost_round = CFG.num_boost_round,\n                valid_sets = [lgb_train, lgb_valid],\n                #feval = CFG.metric,\n                callbacks=[lgb.early_stopping(stopping_rounds=CFG.early_stopping_round,\n                                              verbose=CFG.verbose)]\n            )\n    valid_pred = model.predict(x_valid)\n    \n    importance_df = pd.DataFrame({\n        'feature': features,\n        'importance': model.feature_importance(importance_type='gain')\n    })\n    importance_df['importance'] = importance_df['importance'] / np.sum(importance_df['importance'])\n    importance_df = importance_df.sort_values(by='importance', ascending=False)\n    print(importance_df)\n    return model, valid_pred\ndef xgboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    xgb_train = xgb.DMatrix(data=x_train, label=y_train)\n    xgb_valid = xgb.DMatrix(data=x_valid, label=y_valid)\n    model = xgb.train(\n                CFG.classification_xgb_params,\n                dtrain = xgb_train,\n                num_boost_round = CFG.num_boost_round,\n                evals = [(xgb_train, 'train'), (xgb_valid, 'eval')],\n                early_stopping_rounds = CFG.early_stopping_round,\n                verbose_eval = CFG.verbose,\n                #feval = CFG.metric,\n                maximize = CFG.metric_maximize_flag,\n        )\n    valid_pred = model.predict(xgb.DMatrix(x_valid))\n    return model, valid_pred\ndef catboost_training(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, features: list):\n    cat_train = Pool(data=x_train, label=y_train)\n    cat_valid = Pool(data=x_valid, label=y_valid)\n    model = CatBoostClassifier(**CFG.classification_cat_params)\n    model.fit(cat_train,\n              eval_set = [cat_valid],\n              early_stopping_rounds = CFG.early_stopping_round,\n              verbose = CFG.verbose,\n              use_best_model = True)\n    valid_pred = model.predict_proba(x_valid)[:, 1]\n    return model, valid_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:02.698023Z","iopub.execute_input":"2024-04-17T04:31:02.698680Z","iopub.status.idle":"2024-04-17T04:31:02.716427Z","shell.execute_reply.started":"2024-04-17T04:31:02.698649Z","shell.execute_reply":"2024-04-17T04:31:02.715175Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def gradient_boosting_model_cv_training(method: str, train_df: pd.DataFrame, features: list):\n    weeks = df_train[\"WEEK_NUM\"]\n    X = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n    y = df_train[\"target\"]\n    \n    oof_predictions = np.zeros(len(train_df))\n    oof_fold = np.zeros(len(train_df))\n    cv = StratifiedGroupKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\n    for fold, (train_index, valid_index) in enumerate(cv.split(X, y, groups=weeks)):\n        print('-'*50)\n        print(f'{method} training fold {fold+1}')\n\n        x_train = train_df[features].iloc[train_index]\n        y_train = train_df[CFG.target_col].iloc[train_index]\n        x_valid = train_df[features].iloc[valid_index]\n        y_valid = train_df[CFG.target_col].iloc[valid_index]\n        if method == 'lightgbm':\n            model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'xgboost':\n            model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid, features)\n        if method == 'catboost':\n            model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, features)\n\n        pickle.dump(model, open(CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl', 'wb'))\n\n        oof_predictions[valid_index] = valid_pred\n        oof_fold[valid_index] = fold + 1\n        del x_train, x_valid, y_train, y_valid, model, valid_pred\n        gc.collect()\n\n    score = roc_auc_score(train_df[CFG.target_col], oof_predictions)\n    print(f'{method} our out of folds CV f1score is {score}')\n\n    oof_df = pd.DataFrame({CFG.target_col: train_df[CFG.target_col], f'{method}_prediction': oof_predictions, 'fold': oof_fold})\n    oof_df.to_csv(CFG.OOF_DATA_PATH / f'oof_{method}_seed{CFG.seed}_ver{CFG.VER}.csv', index = False)\n\ndef Learning(input_df: pd.DataFrame, features: list):\n    for method in CFG.METHOD_LIST:\n        gradient_boosting_model_cv_training(method, input_df, features)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:02.718112Z","iopub.execute_input":"2024-04-17T04:31:02.718495Z","iopub.status.idle":"2024-04-17T04:31:02.736734Z","shell.execute_reply.started":"2024-04-17T04:31:02.718463Z","shell.execute_reply":"2024-04-17T04:31:02.735122Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def unified_inference(method: str, x_test: pd.DataFrame):\n    test_pred = np.zeros(len(x_test))\n    for fold in range(CFG.n_folds):\n        model_path = CFG.MODEL_DATA_PATH / f'{method}_fold{fold + 1}_seed{CFG.seed}_ver{CFG.VER}.pkl'\n        model = pickle.load(open(model_path, 'rb'))\n\n        if method == 'lightgbm':\n            pred = model.predict(x_test)\n        elif method == 'xgboost':\n            pred = model.predict(xgb.DMatrix(x_test))\n        elif method == 'catboost':\n            pred = model.predict_proba(x_test)[:, 1]\n        else:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        test_pred += pred\n    \n    return test_pred / CFG.n_folds\n\ndef gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list):\n    x_test = test_df[features]\n    test_pred = unified_inference(method, x_test)\n    return test_pred\n\ndef Predicting(input_df: pd.DataFrame, features: list):\n    output_df = input_df.copy()\n    output_df['pred_prob'] = 0\n    for method in CFG.METHOD_LIST:\n        output_df[f'{method}_pred_prob'] = gradient_boosting_model_inference(method, input_df, features)\n        output_df['pred_prob'] += CFG.model_weight_dict[method] * output_df[f'{method}_pred_prob']\n    return output_df","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:02.738715Z","iopub.execute_input":"2024-04-17T04:31:02.739196Z","iopub.status.idle":"2024-04-17T04:31:02.756358Z","shell.execute_reply.started":"2024-04-17T04:31:02.739160Z","shell.execute_reply":"2024-04-17T04:31:02.755271Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def target_enc(df,test,col):\n    features = [col for col in df.columns if col != CFG.target_col]\n    kf = KFold(n_splits=CFG.n_folds,shuffle=True,random_state = CFG.seed)\n    encoded_features = []\n\n    for train_idx, val_idx in kf.split(df):\n        X_train, X_valid = df[features].iloc[train_idx], df[features].iloc[val_idx]\n        y_train = df[CFG.target_col].iloc[train_idx]\n\n        target_encoder = ce.TargetEncoder()\n        target_encoder.fit(X_train[col], y_train)\n\n        X_valid[f'{col}_target_Encoded'] = target_encoder.transform(X_valid[col])\n        encoded_features.append(X_valid)\n\n\n    encoded_df = pd.concat(encoded_features).sort_index()\n    df[f'{col}_target_Encoded'] = encoded_df[f'{col}_target_Encoded']\n    \n    target_encoder = ce.TargetEncoder()\n    target_encoder.fit(df[[col]], df[CFG.target_col])\n\n    test[f'{col}_target_Encoded'] = target_encoder.transform(test[[col]])\n    \n    return df, test\n\ndef encoder(df, test):\n    object_columns = [col for col in df.columns if df[col].dtype.name == 'object' or df[col].dtype.name == 'category']\n    \n    bool_columns = [col for col in df.columns if df[col].dtype == 'bool']\n\n    for col in object_columns:\n        df, test = target_enc(df, test, col)\n        \n    for col in bool_columns:\n        df[col] = df[col].astype(int)\n        test[col] = test[col].astype(int)\n\n    df.drop(object_columns, axis=1, inplace=True)\n    test.drop(object_columns, axis=1, inplace=True)\n        \n    return df, test\n\n\ndef preprocess(df,test):\n    df,test = encoder(df,test)\n    features = [col for col in df_train.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\n    \n    return df,test,features","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:02.757985Z","iopub.execute_input":"2024-04-17T04:31:02.758437Z","iopub.status.idle":"2024-04-17T04:31:02.775438Z","shell.execute_reply.started":"2024-04-17T04:31:02.758402Z","shell.execute_reply":"2024-04-17T04:31:02.774391Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"del train_data_store,test_data_store\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:02.776575Z","iopub.execute_input":"2024-04-17T04:31:02.776896Z","iopub.status.idle":"2024-04-17T04:31:03.738785Z","shell.execute_reply.started":"2024-04-17T04:31:02.776870Z","shell.execute_reply":"2024-04-17T04:31:03.737613Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"df_train = df_train.sample(frac=0.1)\ndf_test = df_test.sample(frac=0.1)\n\ndf_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)\nweeks = df_train[\"WEEK_NUM\"]\nfeatures = [col for col in df_train.columns if col != CFG.target_col and col not in [\"case_id\", \"WEEK_NUM\"]]\ndf_train,df_test,features = preprocess(df_train,df_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:31:03.740238Z","iopub.execute_input":"2024-04-17T04:31:03.740565Z","iopub.status.idle":"2024-04-17T04:38:04.288309Z","shell.execute_reply.started":"2024-04-17T04:31:03.740540Z","shell.execute_reply":"2024-04-17T04:38:04.287051Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Memory usage of dataframe is 486.04 MB\nMemory usage after optimization is: 195.43 MB\nDecreased by 59.8%\nMemory usage of dataframe is 0.01 MB\nMemory usage after optimization is: 0.01 MB\nDecreased by 4.9%\n","output_type":"stream"}]},{"cell_type":"code","source":"#Learning(df_train, features)","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:38:04.289820Z","iopub.execute_input":"2024-04-17T04:38:04.290308Z","iopub.status.idle":"2024-04-17T04:38:04.294076Z","shell.execute_reply.started":"2024-04-17T04:38:04.290279Z","shell.execute_reply":"2024-04-17T04:38:04.293248Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_submission(test_df, submission_dir, submission_file):\n    df_subm = pd.read_csv(submission_dir)\n    df_subm.set_index(\"case_id\", inplace=True)\n    \n    df_subm[\"score\"] = test_df.set_index(\"case_id\")[\"pred_prob\"]\n    df_subm.to_csv(submission_file)\n\n\ntest_df = Predicting(df_test, features)\ncreate_submission(test_df, SAMPLE_SUB, \"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-17T04:44:23.865578Z","iopub.execute_input":"2024-04-17T04:44:23.866046Z","iopub.status.idle":"2024-04-17T04:44:24.384908Z","shell.execute_reply.started":"2024-04-17T04:44:23.866014Z","shell.execute_reply":"2024-04-17T04:44:24.384000Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}